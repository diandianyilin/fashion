{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1480907/2792033319.py:11: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file)\n",
      "Processing images: 100%|██████████| 10379/10379 [00:52<00:00, 196.03file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching records saved to /data1/dxw_data/llm/redbook_final/script_next/matching_records.csv\n"
     ]
    }
   ],
   "source": [
    "# 得到去掉black area的text文件\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the paths\n",
    "image_folder = '/data1/dxw_data/llm/redbook_final/script_next/combined_seg_img_pure_094'\n",
    "csv_file = '/data1/dxw_data/llm/redbook_final/script_next/rawdata_20%.csv'\n",
    "output_file = '/data1/dxw_data/llm/redbook_final/script_next/matching_records.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Initialize a list to store matching rows\n",
    "matching_rows = []\n",
    "\n",
    "# Iterate over the files in the image folder with a progress bar\n",
    "for filename in tqdm(os.listdir(image_folder), desc=\"Processing images\", unit=\"file\"):\n",
    "    if filename.endswith('.png'):\n",
    "        # Extract the poster_id and post_id from the filename\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 3:\n",
    "            date, poster_id, post_id = parts[0], parts[1], parts[2]\n",
    "\n",
    "            # Find matching rows in the DataFrame\n",
    "            matches = df[(df['poster_id'] == poster_id) & (df['post_id'] == post_id)]\n",
    "\n",
    "            # Append the matching rows to the list\n",
    "            if not matches.empty:\n",
    "                matching_rows.append(matches)\n",
    "\n",
    "# Concatenate all matching rows into a single DataFrame\n",
    "if matching_rows:\n",
    "    matching_df = pd.concat(matching_rows)\n",
    "\n",
    "    # Save the matching records to a new CSV file\n",
    "    matching_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f'Matching records saved to {output_file}')\n",
    "else:\n",
    "    print('No matching records found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning summaries:   0%|          | 0/12807 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.591 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Cleaning summaries: 100%|██████████| 12807/12807 [00:09<00:00, 1286.89it/s]\n",
      "Extracting keywords: 100%|██████████| 12807/12807 [00:09<00:00, 1360.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated records with cleaned summaries and keywords saved to /data1/dxw_data/llm/redbook_final/script_next/matching_records.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import jieba\n",
    "from rake_nltk import Rake\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define the paths\n",
    "matching_records_file = '/data1/dxw_data/llm/redbook_final/script_next/matching_records.csv'\n",
    "stopwords_file_path = 'stopwords_cn.txt'\n",
    "\n",
    "# Load the stopwords\n",
    "with open(stopwords_file_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().splitlines())\n",
    "\n",
    "# Load the matching records CSV\n",
    "df = pd.read_csv(matching_records_file)\n",
    "\n",
    "# Function to clean the summary text\n",
    "def clean_text(text, stopwords):\n",
    "    # Convert emoji to text\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove specific text patterns\n",
    "    text = re.sub(r'- 小红书,,', '', text)\n",
    "    text = re.sub(r',,\\d{2}-\\d{2},,', '', text)\n",
    "    text = re.sub(r'#', ' ', text)\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    # Word segmentation\n",
    "    words = jieba.cut(cleaned_text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to extract keywords using RAKE\n",
    "def extract_keywords(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return r.get_ranked_phrases()\n",
    "\n",
    "# Function to clean the extracted keywords\n",
    "def clean_keywords(keywords):\n",
    "    # Remove any NaN or invalid characters\n",
    "    if isinstance(keywords, list):\n",
    "        cleaned_keywords = [kw for kw in keywords if isinstance(kw, str) and kw.strip()]\n",
    "        return cleaned_keywords if cleaned_keywords else np.nan\n",
    "    return np.nan\n",
    "\n",
    "# Apply the cleaning function with progress bar\n",
    "tqdm.pandas(desc=\"Cleaning summaries\")\n",
    "df['summary_cleaned'] = df['summary'].progress_apply(lambda x: clean_text(str(x), stopwords))\n",
    "\n",
    "# Apply the keyword extraction with progress bar\n",
    "tqdm.pandas(desc=\"Extracting keywords\")\n",
    "df['rake_keywords'] = df['summary_cleaned'].progress_apply(lambda x: extract_keywords(x))\n",
    "\n",
    "# Clean the extracted keywords\n",
    "df['rake_keywords'] = df['rake_keywords'].apply(clean_keywords)  #! 去掉nan，并检测有如果为空就删去。\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(matching_records_file, index=False)\n",
    "\n",
    "print(f'Updated records with cleaned summaries and keywords saved to {matching_records_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimagebind\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imagebind_model\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/__init__.py:1465\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_meta_registrations.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_op_to_registry, global_decomposition_table, meta_table\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _elementwise_meta, ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_decomp/__init__.py:170\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# populate the table\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decomp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecompositions\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_refs\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# This list was copied from torch/_inductor/decomposition.py\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# excluding decompositions that results in prim ops\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Resulting opset of decomposition is core aten ops\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcore_aten_decompositions\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[OpOverload, Callable]:\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_refs/__init__.py:2145\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m prims\u001b[38;5;241m.\u001b[39mconvert_element_type(result, torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m   2141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;129;43m@register_decomposition\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m-> 2145\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43msum\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensorLikeType\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mList\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2149\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTensorLikeType\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_boolean_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_integer_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_decomp/__init__.py:131\u001b[0m, in \u001b[0;36mregister_decomposition.<locals>.decomposition_decorator\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    128\u001b[0m     _add_op_to_registry(registry, op, fn)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# To handle allowing multiple aten_ops at once\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregister\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maten_op\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(fn, pytree)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[38;5;241m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten([fn(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/utils/_pytree.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtree_map\u001b[39m(fn: Any, pytree: PyTree) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTree:\n\u001b[1;32m    195\u001b[0m     flat_args, spec \u001b[38;5;241m=\u001b[39m tree_flatten(pytree)\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten([\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m flat_args], spec)\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_decomp/__init__.py:128\u001b[0m, in \u001b[0;36mregister_decomposition.<locals>.decomposition_decorator.<locals>.register\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister\u001b[39m(op):\n\u001b[0;32m--> 128\u001b[0m     \u001b[43m_add_op_to_registry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_decomp/__init__.py:44\u001b[0m, in \u001b[0;36m_add_op_to_registry\u001b[0;34m(registry, op, fn)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, OpOverloadPacket)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ol \u001b[38;5;129;01min\u001b[39;00m op\u001b[38;5;241m.\u001b[39moverloads():\n\u001b[0;32m---> 44\u001b[0m         overloads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mol\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op_overload \u001b[38;5;129;01min\u001b[39;00m overloads:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op_overload \u001b[38;5;129;01min\u001b[39;00m registry:\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/torch/_ops.py:478\u001b[0m, in \u001b[0;36mOpOverloadPacket.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    476\u001b[0m use_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[1;32m    477\u001b[0m \u001b[38;5;66;03m# TODO: disallow access to overloads registered by JIT\u001b[39;00m\n\u001b[0;32m--> 478\u001b[0m op_, op_dk_, tags \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_operation_overload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_qualified_op_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_key\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m schema \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_schema(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qualified_op_name, use_key)\n\u001b[1;32m    482\u001b[0m overload \u001b[38;5;241m=\u001b[39m OpOverload(\u001b[38;5;28mself\u001b[39m, op_, op_dk_, schema, tags)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType\n",
    "from imagebind import data\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "csv_file = '/data1/dxw_data/llm/redbook_final/script_next/matching_records.csv'\n",
    "image_folder = '/data1/dxw_data/llm/redbook_final/script_next/combined_seg_img_pure_094'\n",
    "output_folder = '/data1/dxw_data/llm/redbook_final/script_next/combined_embeddings'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Device setup\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Image transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to load and transform a single image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(text, image_path):\n",
    "    # Prepare inputs\n",
    "    inputs = {}\n",
    "    if text is not None:\n",
    "        inputs[ModalityType.TEXT] = data.load_and_transform_text([text], device)\n",
    "    if image_path is not None:\n",
    "        inputs[ModalityType.VISION] = load_image(image_path)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(inputs)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Iterate over each row in the CSV and process\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "    poster_id = row['poster_id']\n",
    "    post_id = row['post_id']\n",
    "    rake_keywords = row['rake_keywords']\n",
    "    \n",
    "    # Find corresponding image file\n",
    "    image_filename = f\"*_{poster_id}_{post_id}_*.png\"\n",
    "    image_path = next((os.path.join(image_folder, fname) for fname in os.listdir(image_folder) if fname.endswith('.png') and f\"{poster_id}_{post_id}\" in fname), None)\n",
    "    \n",
    "    if image_path:\n",
    "        # Generate embeddings\n",
    "        embeddings = generate_embeddings(rake_keywords, image_path)\n",
    "        \n",
    "        # Concatenate text and vision embeddings\n",
    "        concatenated_embeddings = torch.cat((embeddings[ModalityType.TEXT], embeddings[ModalityType.VISION]), dim=1)\n",
    "        \n",
    "        # Save the concatenated embeddings\n",
    "        output_file = os.path.join(output_folder, f\"{poster_id}_{post_id}_embedding.pt\")\n",
    "        torch.save(concatenated_embeddings, output_file)\n",
    "\n",
    "print(f\"Embedding process complete. Embeddings saved to {output_folder}.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "embedding_folder = '/data1/dxw_data/llm/redbook_final/script_next/combined_embeddings'\n",
    "output_folder = '/data1/dxw_data/llm/redbook_final/script_next/combined_seg_img_pure_094_cluster_imagebind3'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load concatenated embeddings\n",
    "embedding_files = [os.path.join(embedding_folder, fname) for fname in os.listdir(embedding_folder) if fname.endswith('.pt')]\n",
    "\n",
    "# Load all embeddings into a list\n",
    "all_embeddings = []\n",
    "for embedding_file in tqdm(embedding_files, desc=\"Loading embeddings\"):\n",
    "    embedding = torch.load(embedding_file)\n",
    "    all_embeddings.append(embedding)\n",
    "\n",
    "# Concatenate all embeddings into a single tensor\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Determine optimal number of clusters using Average Silhouette Method\n",
    "silhouette_scores = []\n",
    "k_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 150, 200]  # Discrete values for k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    labels = kmeans.fit_predict(all_embeddings.numpy())\n",
    "    score = silhouette_score(all_embeddings.numpy(), labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(30, 6))  # Increased width to three times the original\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Average Silhouette Score vs. Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Select the optimal number of clusters based on the silhouette scores\n",
    "optimal_k = k_values[silhouette_scores.index(max(silhouette_scores))]\n",
    "\n",
    "# Perform clustering with optimal k\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=0)\n",
    "labels = kmeans.fit_predict(all_embeddings.numpy())\n",
    "\n",
    "# Save clustered images to output folders based on the clustering results\n",
    "for idx, label in tqdm(enumerate(labels), desc=\"Saving clustered images\", total=len(labels)):\n",
    "    label_folder = os.path.join(output_folder, str(label))\n",
    "    os.makedirs(label_folder, exist_ok=True)\n",
    "    \n",
    "    # Extract corresponding image file name from embedding file name\n",
    "    embedding_file = embedding_files[idx]\n",
    "    image_filename = os.path.basename(embedding_file).replace('_embedding.pt', '.png')\n",
    "    \n",
    "    # Define source image path\n",
    "    source_image_path = os.path.join(embedding_folder.replace('combined_embeddings', 'combined_seg_img_pure_094'), image_filename)\n",
    "    \n",
    "    # Copy image to corresponding cluster folder\n",
    "    if os.path.exists(source_image_path):\n",
    "        shutil.copy(source_image_path, os.path.join(label_folder, image_filename))\n",
    "\n",
    "# Save clustering labels to JSON file\n",
    "labels_json = {os.path.basename(embedding_files[idx]): int(label) for idx, label in enumerate(labels)}\n",
    "with open(os.path.join(output_folder, 'labels.json'), 'w') as f:\n",
    "    json.dump(labels_json, f)\n",
    "\n",
    "print(f'Clustering complete. Output saved to {output_folder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
