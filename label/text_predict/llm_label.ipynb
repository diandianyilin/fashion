{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 13:26:45.466340: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-02 13:26:45.588436: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-02 13:26:46.212968: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-09-02 13:26:46.213050: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2024-09-02 13:26:46.213056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970f94c32c694ffd8b484a1752b10eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 14232/14232 [17:04:50<00:00,  4.32s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /data1/dxw_data/llm/RA/cuhk_xinyu/filtered_texts_with_styles1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GLM:\n",
    "    max_token: int = 2048\n",
    "    temperature: float = 0.2\n",
    "    top_p: float = 0.9\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history_len: int = 1024\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"GLM\"\n",
    "\n",
    "    def load_model(self, llm_device=\"gpu\", model_name_or_path=None):\n",
    "        model_config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path, config=model_config, trust_remote_code=True, device='cuda:1').half()\n",
    "\n",
    "    def _call(self, prompt: str, history: List[str] = [], stop: Optional[List[str]] = None):\n",
    "        response, _ = self.model.chat(\n",
    "            self.tokenizer, prompt,\n",
    "            history=history[-self.history_len:] if self.history_len > 0 else [],\n",
    "            max_length=self.max_token, temperature=self.temperature,\n",
    "            top_p=self.top_p)\n",
    "        return response\n",
    "\n",
    "modelpath = \"/data1/dxw_data/llm/chatglm3-6b\"\n",
    "llm = GLM()\n",
    "llm.load_model(model_name_or_path=modelpath)\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = '/data1/dxw_data/llm/RA/cuhk_xinyu/segmented_texts_part1.csv'\n",
    "output_file_path = '/data1/dxw_data/llm/RA/cuhk_xinyu/filtered_texts_with_styles1.csv'\n",
    "\n",
    "# Read the input CSV file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "def format_segmented_text(segmented_text):\n",
    "    # Split the text into words and enclose each word in single quotes\n",
    "    formatted_text = ','.join([f\"'{word.strip()}'\" for word in segmented_text.split()])\n",
    "    return formatted_text\n",
    "\n",
    "def create_style_prompt(formatted_segmented_text):\n",
    "    return (\n",
    "        f\"\"\"\n",
    "        Given the following list of words/phrases: [{formatted_segmented_text}], identify which words are related to some specific fashion styles.\n",
    "        \n",
    "        Please return the related words in a list format, e.g., [word1, word2, ...].\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Processing each row\n",
    "df['style_list'] = ''\n",
    "\n",
    "batch_size = 200\n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing rows\"):\n",
    "    segmented_text = row['segmented_text'].strip()\n",
    "    \n",
    "    # Format the segmented text\n",
    "    formatted_segmented_text = format_segmented_text(segmented_text)\n",
    "    \n",
    "    # Create a prompt for the GLM model\n",
    "    prompt = create_style_prompt(formatted_segmented_text)\n",
    "    \n",
    "    # Get the model output\n",
    "    try:\n",
    "        style_output = llm._call(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "        style_output = \"[]\"\n",
    "\n",
    "    # Save the raw output directly to the dataframe\n",
    "    df.at[index, 'style_list'] = str(style_output)\n",
    "    \n",
    "    # Every 200 rows, save the current dataframe to the CSV file\n",
    "    if (index + 1) % batch_size == 0:\n",
    "        df.iloc[index + 1 - batch_size:index + 1].to_csv(output_file_path, mode='a', header=not os.path.exists(output_file_path), index=False)\n",
    "\n",
    "# Save any remaining rows that were not saved in the last batch\n",
    "if df.shape[0] % batch_size != 0:\n",
    "    df.iloc[df.shape[0] - (df.shape[0] % batch_size):].to_csv(output_file_path, mode='a', header=not os.path.exists(output_file_path), index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
