{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import emoji\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/data1/dxw_data/llm/redbook_final/script_next/rawdata_20%.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Combine post_title, post_content, and post_tag into combind_text\n",
    "df['combind_text'] = df[['post_title', 'post_content', 'post_tag']].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "# Load stop words\n",
    "stopwords_path = '/data1/dxw_data/llm/Multimodal-MKT/label/text-cluster/stopwords_cn.txt'\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(line.strip() for line in f)\n",
    "\n",
    "# Remove stop words, punctuation, numbers, English letters, and convert emojis\n",
    "def preprocess_text(text):\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove punctuation, numbers, and English letters\n",
    "    text = re.sub(r'[^\\u4e00-\\u9fa5\\s]', '', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [word for word in jieba.cut(text) if word.strip() and word not in stopwords]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing with a progress bar\n",
    "df['combind_text'] = [preprocess_text(text) for text in tqdm(df['combind_text'], desc='Preprocessing Text')]\n",
    "\n",
    "# Step 2: Segment the text using jieba\n",
    "df['segmented_text'] = [' '.join(jieba.cut(text)) for text in tqdm(df['combind_text'], desc='Segmenting Text')]\n",
    "\n",
    "# List of Chinese number characters and '小红书'\n",
    "chinese_numbers = ['小红书', '一', '二', '两', '三', '四', '五', '六', '七', '八', '九', '十', '百', '千', '万', '亿']\n",
    "\n",
    "# Step 3: Identify words that match key_words\n",
    "key_words = [\n",
    "    \"裙\", \"裙子\", \"项链\", \"配饰\", \"裤\", \"吊带\", \"风格\", \"饰品\", \"单品\", \"衬衫\", \"身材\", \"耳环\", \"主义\", \"混搭\", \n",
    "    \"手链\", \"元素\", \"绒\", \"肩\", \"鞋子\", \"瘦\", \"套装\", \"款\", \"毛\", \"吊坠\", \"造型\", \"型\", \"饰\", \"袜\", \n",
    "    \"马甲\", \"系\", \"夹克\", \"裳\", \"推荐\", \"服\", \"衣服\", \"靴\", \"款\", \"白t\", \"搭配\", \"恤\", \"大衣\", \"头\", \"风\", \n",
    "    \"毛衣\", \"服\", \"内搭\", \"靴子\", \"链\", \"套装\", \"头发\", \"背心\", \"毛衣\", \"外套\", \"帽\", \"发型\", \"包\", \"衣\", \n",
    "    \"戒指\", \"鞋\", \"衫\", \"袍\", \"手镯\", \"单品\", \"装\", \"镜\", \"帽子\", \"袖\", \"风\", \"感\", \"系\", \"型\", \"搭\", \"装\", \n",
    "    \"式\", \"派\", \"调\", \"潮\", \"范\", \"领\", \"色\", \"款\", \"裤\", \"裙\", \"穿\", \"搭\", \"夏\", \"春\", \"秋\", \"冬\", \"鞋\", \n",
    "    \"白\", \"季\", \"白\", \"红\", \"黑\", \"蓝\", \"绿\", \"黄\", \"紫\", \"灰\", \"衣\", \"服\", \"套\", \"包\", \"潮流\", \"时尚\", \n",
    "    \"复古\", \"简约\", \"休闲\", \"通勤\", \"街头\", \"个性\", \"优雅\", \"气质\", \"名媛\", \"甜美\", \"清新\", \"叠穿\", \n",
    "    \"搭配\", \"混搭\", \"色彩\", \"质感\", \"配饰\", \"外套\", \"毛衣\", \"村衫\", \"牛仔\", \"婚礼\", \"度假\", \"派对\", \n",
    "    \"职场\", \"约会\", \"旅行\"\n",
    "]\n",
    "\n",
    "# Function to check if any word matches a key word and does not start with chinese_numbers\n",
    "def find_matching_words(segmented_text, key_words, chinese_numbers):\n",
    "    words = segmented_text.split()\n",
    "    matching_terms = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Skip words that start with any chinese_number\n",
    "        if any(word.startswith(cn) for cn in chinese_numbers):\n",
    "            continue\n",
    "        # Check if word ends with a keyword\n",
    "        if any(word.endswith(kw) for kw in key_words):\n",
    "            matching_terms.append(word)\n",
    "    \n",
    "    return matching_terms\n",
    "\n",
    "# Apply the function to segmented_text and save results to a text file with a progress bar\n",
    "output_file = '/data1/dxw_data/llm/RA/cuhk_xinyu/matching_words_combined_unique3.txt'\n",
    "unique_terms = set()\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc='Finding Matching Words'):\n",
    "    matching_terms = find_matching_words(row['segmented_text'], key_words, chinese_numbers)\n",
    "    unique_terms.update(matching_terms)\n",
    "\n",
    "# Remove duplicates and save to file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for term in sorted(unique_terms):  # Sort the terms for easier reading\n",
    "        f.write(f\"{term}\\n\")\n",
    "\n",
    "print(\"Processing complete. Unique matching words saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，文件已保存。\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# 输入文件路径\n",
    "input_files = [\n",
    "    '/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_notstyle_dataset-output.csv',\n",
    "    '/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_style_dataset-output.csv'  # 第二个文件路径\n",
    "]\n",
    "\n",
    "# 输出文件路径\n",
    "output_label_1_file = 'combined_output_label_1.txt'\n",
    "output_label_0_file = 'combined_output_label_0.txt'\n",
    "\n",
    "# 打开两个输出文件\n",
    "with open(output_label_1_file, mode='w', encoding='utf-8') as label_1_file, \\\n",
    "     open(output_label_0_file, mode='w', encoding='utf-8') as label_0_file:\n",
    "    \n",
    "    # 逐个处理输入文件\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, mode='r', encoding='utf-8') as infile:\n",
    "            # 创建csv阅读器\n",
    "            reader = csv.DictReader(infile)\n",
    "            \n",
    "            # 遍历每一行，根据output_label决定写入哪个文件\n",
    "            for row in reader:\n",
    "                word = row['word']\n",
    "                output_label = row['output_label']\n",
    "                \n",
    "                if output_label == '1':\n",
    "                    label_1_file.write(f\"{word}\\n\")\n",
    "                elif output_label == '0':\n",
    "                    label_0_file.write(f\"{word}\\n\")\n",
    "\n",
    "print(\"处理完成，文件已保存。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
