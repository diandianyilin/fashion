{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion-related samples (positive): 52242\n",
      "Number of non-fashion-related samples (negative): 304611\n",
      "The datasets are not balanced. Difference: 252369\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load fashion-related data (positive samples)\n",
    "fashion_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword' column which contains fashion-related keywords\n",
    "fashion_samples = fashion_df['keyword group']\n",
    "\n",
    "# Directory containing non-fashion-related samples\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "\n",
    "# Initialize an empty list to store non-fashion samples\n",
    "non_fashion_samples = []\n",
    "\n",
    "# Recursively iterate through all subdirectories and files in non_fashion_texts directory\n",
    "for root, dirs, files in os.walk(neg_dir):\n",
    "    for file in files:\n",
    "        # Read only text files\n",
    "        if file.startswith('wiki'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Add each line as a sample\n",
    "                lines = f.readlines()\n",
    "                non_fashion_samples.extend([line.strip() for line in lines if line.strip()])\n",
    "\n",
    "# Convert non-fashion samples to a pandas Series for consistency\n",
    "non_fashion_samples = pd.Series(non_fashion_samples)\n",
    "\n",
    "# Step 2: Count the number of samples in each dataset\n",
    "num_fashion_samples = fashion_samples.shape[0]\n",
    "num_non_fashion_samples = non_fashion_samples.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of fashion-related samples (positive): {num_fashion_samples}\")\n",
    "print(f\"Number of non-fashion-related samples (negative): {num_non_fashion_samples}\")\n",
    "\n",
    "# Step 3: Check if they are balanced\n",
    "if num_fashion_samples == num_non_fashion_samples:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(f\"The datasets are not balanced. Difference: {abs(num_fashion_samples - num_non_fashion_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling:\n",
    "- randomly reducing the number of samples from the majority class (non-fashion-related samples) to match the number of samples in the minority class (fashion-related samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downsampled non-fashion-related samples: 52242\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# If the non-fashion dataset is larger, downsample it\n",
    "if num_non_fashion_samples > num_fashion_samples:\n",
    "    # Randomly select `num_fashion_samples` non-fashion samples\n",
    "    downsampled_non_fashion_samples = non_fashion_samples.sample(n=num_fashion_samples, random_state=42)\n",
    "else:\n",
    "    downsampled_non_fashion_samples = non_fashion_samples\n",
    "\n",
    "# Print the number of samples after downsampling\n",
    "print(f\"Number of downsampled non-fashion-related samples: {downsampled_non_fashion_samples.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read Data Files\n",
    "- read positive samples\n",
    "- read negative samples\n",
    "- combine the datasets\n",
    "#### Step 2: Load pre-trained sentence Transformer Model\n",
    "- load text2vec-large-chinese model\n",
    "- generate embeddings for each word\n",
    "#### Step 3: Get embeddings for the dataset\n",
    "- apply the embeddings function\n",
    "- convert labels to tensor\n",
    "#### Step 4: Train-test split\n",
    "- split the data\n",
    "- create PyTorh TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (356852, 2)\n",
      "Data Format Example:\n",
      "      word  label\n",
      "0     市政府.      0\n",
      "1   地理与气候.      0\n",
      "2  綠 (消歧義)      0\n",
      "3      邯山区      0\n",
      "4     江戸幕府      0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Progress: 100%|██████████| 11152/11152 [15:32<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([356852, 1024])\n",
      "Embedding Example:\n",
      "tensor([ 0.9775, -0.6769, -0.7114,  ..., -0.5798,  1.1212, -2.3075],\n",
      "       device='cuda:0')\n",
      "Labels Shape: torch.Size([356852])\n",
      "Labels Example:\n",
      "tensor([0, 0, 0, 0, 0])\n",
      "Train Embeddings Shape: torch.Size([285481, 1024]), Train Labels Shape: torch.Size([285481])\n",
      "Val Embeddings Shape: torch.Size([71371, 1024]), Val Labels Shape: torch.Size([71371])\n",
      "Train Loader Batch Size: 8922 batches\n",
      "Val Loader Batch Size: 2231 batches\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Read Positive Samples from 'topics_filtered.csv'\n",
    "# Load the fashion lexicon (positive samples)\n",
    "pos_file = \"/home/disk1/red_disk1/fashion/topics_filtered.csv\"\n",
    "positive_samples_df = pd.read_csv(pos_file)\n",
    "positive_samples = positive_samples_df['keyword group'].dropna().tolist()  # Extract keywords column\n",
    "\n",
    "# Assign label 1 to positive samples\n",
    "positive_samples = pd.DataFrame(positive_samples, columns=['word'])\n",
    "positive_samples['label'] = 1\n",
    "\n",
    "# Step 2: Read Negative Samples from non_fashion_texts\n",
    "def read_negative_samples(directory):\n",
    "    all_lines = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    all_lines.extend([line.strip() for line in file if line.strip() != \"\"])\n",
    "    return all_lines\n",
    "\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "negative_samples_list = read_negative_samples(neg_dir)\n",
    "\n",
    "# Assign label 0 to negative samples\n",
    "negative_samples = pd.DataFrame(negative_samples_list, columns=['word'])\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "# Step 3: Combine Positive and Negative Samples\n",
    "data = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shape and format of the data\n",
    "print(f\"Combined Data Shape: {data.shape}\")\n",
    "print(f\"Data Format Example:\\n{data.head()}\")\n",
    "\n",
    "# Step 4: Load text2vec-large-chinese model\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Embedding Progress\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 5: Get embeddings for the dataset\n",
    "embeddings = get_text2vec_embeddings(data['word'], text2vec_model)\n",
    "\n",
    "# Print the shape and format of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding Example:\\n{embeddings[0]}\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Print the labels format\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels Example:\\n{labels[:5]}\")\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the split data shapes\n",
    "print(f\"Train Embeddings Shape: {train_embeddings.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Embeddings Shape: {val_embeddings.shape}, Val Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print the data loader batch size\n",
    "print(f\"Train Loader Batch Size: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader Batch Size: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a RedNet-based binary classification model\n",
    "1. Model Definition: ResNet-Based Binary Classifier\n",
    "2. Training Loop\n",
    "3. Validation Loop\n",
    "4. Training and Evaluation\n",
    "\n",
    "#### Summary\n",
    "- Model Setup: It initializes a ResNet18 model tailored for binary classification using text embeddings as inputs.\n",
    "- Training and Validation: The code implements a standard training loop with backpropagation and uses validation to track performance.\n",
    "- Performance Metrics: Accuracy, precision, recall, and F1 score are calculated to evaluate the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.66it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 345.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0922, Val Loss: 0.0618, Accuracy: 0.9754\n",
      "Precision: 0.9172, Recall: 0.9143, F1 Score: 0.9158\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.64it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 355.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0546, Val Loss: 0.0586, Accuracy: 0.9775\n",
      "Precision: 0.9595, Recall: 0.8829, F1 Score: 0.9196\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:49<00:00, 38.82it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 408.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411, Val Loss: 0.0471, Accuracy: 0.9813\n",
      "Precision: 0.9461, Recall: 0.9245, F1 Score: 0.9352\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:49<00:00, 38.86it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 354.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0321, Val Loss: 0.0481, Accuracy: 0.9826\n",
      "Precision: 0.9419, Recall: 0.9390, F1 Score: 0.9404\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.74it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 400.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0262, Val Loss: 0.0526, Accuracy: 0.9829\n",
      "Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 434.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: Accuracy: 0.9829, Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 7: Define ResNet-Based Binary Classification Model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):  # Adjust embedding_dim based on text2vec output (1024)\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        # Use pre-trained ResNet18, but replace the first conv layer to accept text2vec embeddings\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Binary classification\n",
    "        \n",
    "        # Define a simple MLP layer to map embeddings to ResNet input size\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),  # Match the expected ResNet input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),  # Match the expected ResNet input size after projection\n",
    "        )\n",
    "        \n",
    "        # Adding a Conv layer to convert 1D to a 2D feature map\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "\n",
    "        # Adaptive pooling layer to transform the spatial dimensions to (7, 7)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)  # Project text2vec embeddings to match ResNet input size\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # Reshape to fit Conv2D (batch_size, channels, height, width)\n",
    "        x = self.embedding_conv(x)  # Use conv layer to convert 1D feature to 2D feature map\n",
    "        x = self.adaptive_pool(x)  # Apply adaptive pooling to resize to (7, 7)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct embedding dimension (1024 as per the output shape)\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "\n",
    "# Step 8: Define Training Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Progress\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Progress\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "# Step 9: Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 10: Final Evaluation\n",
    "val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "print(f\"Final Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /home/disk1/red_disk1/fashion/tfashion\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Save the trained model\n",
    "\n",
    "# Save model and tokenizer to the desired directory\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion\"\n",
    "\n",
    "# Make the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, \"tfashion.pth\"))\n",
    "\n",
    "# Optionally, save additional model metadata if needed (e.g., optimizer state, epoch)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(model_save_path, \"tfashion_checkpoint.pth\"))\n",
    "\n",
    "print(f\"Model saved at: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare post data\n",
    "- Combine 'post_title' and 'post_content' into 'post_text'\n",
    "- Clean 'post_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean.csv')\n",
    "# df = pd.read_csv('/home/disk1/red_disk1/poster_9305.csv')\n",
    "\n",
    "# Combine 'post_title' and 'post_content' into 'post_text'\n",
    "df['post_text'] = df['post_title'].fillna('') + ' ' + df['post_content'].fillna('')\n",
    "\n",
    "# Drop 'post_title' and 'post_content' columns\n",
    "df = df.drop(columns=['post_title', 'post_content', 'post_tag'])\n",
    "\n",
    "# Save the updated DataFrame\n",
    "# df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_9305_combined.csv', index=False)\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "# Load stopwords from the provided file\n",
    "with open('/home/disk1/red_disk1/fashion/stopwords_cn.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Function for text cleaning\n",
    "def clean_text(text, stopwords):\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove specific patterns\n",
    "    text = re.sub(r'- 小红书,,', '', text)  # Removing \"- 小红书,,\"\n",
    "    text = re.sub(r'小红书', '', text)  # Explicitly remove \"小红书\"\n",
    "    text = re.sub(r',,\\d{2}-\\d{2},,', '', text)  # Removing patterns like \",,XX-XX,,\"\n",
    "    text = re.sub(r'#', ' ', text)  # Replace '#' with a space\n",
    "    text = re.sub(r'\\s+', '', text)  # This will remove all whitespace characters\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    # Remove stopwords (word-based removal)\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopwords])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function to 'post_text'\n",
    "df['cleaned_post_text'] = df['post_text'].apply(lambda x: clean_text(str(x), stopwords))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/post_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model tfashion\n",
    "- Step 1: Load the test dataset.\n",
    "- Step 2: Generate embeddings for each word in the test dataset using text2vec-large-chinese.\n",
    "- Step 3: Use a pre-trained ResNet-based classifier model to predict whether each word is related to fashion or not.\n",
    "- Step 4: Save the predictions alongside the original data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 489/489 [02:17<00:00,  3.57it/s]\n",
      "/tmp/ipykernel_267914/2888443501.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
      "Testing Progress: 100%|██████████| 489/489 [00:01<00:00, 476.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/disk1/red_disk1/fashion/test-output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"  # Update this path if necessary\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['cleaned_post_text']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained tfashion model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained tfashion model from your specified path\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test-output.csv\"  \n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "1. Tokenization: tokenize the post_text content\n",
    "2. Word Embedding: pass each word (instead of the entire post) through the text2vec-large-chinese model to generate embeddings for each word\n",
    "3. Classification: pass embeddings through the tfashion model to classify whether each word is fashion-related or not\n",
    "4. Filter out any words that are present in the stopwords set before applying the model classification\n",
    "5. Filter out single-character words\n",
    "5. Results: The output will be a classification for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267914/336921134.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "Processing Rows:  30%|███       | 4764/15622 [02:21<03:48, 47.48it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Step 1: Load the stopwords file\n",
    "stopwords_file = \"/home/disk1/red_disk1/fashion/stopwords_cn.txt\"\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "# Step 2: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 3: Define ResNetBinaryClassifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Load the CSV file containing post_text\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 5: Tokenize the post_text content into words and remove stopwords\n",
    "def tokenize_text(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    # Filter out stopwords and single-character words\n",
    "    return [word for word in words if word not in stopwords and len(word) > 1]\n",
    "\n",
    "df['tokenized_text'] = df['cleaned_post_text'].apply(tokenize_text)\n",
    "\n",
    "# Step 6: Get embeddings for each word in the tokenized_text column and classify them\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    words = row['tokenized_text']\n",
    "    \n",
    "    # Get embeddings for the words\n",
    "    embeddings = get_text2vec_embeddings(words, text2vec_model).to(device)\n",
    "    \n",
    "    # Make predictions using the pre-trained ResNet binary classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Filter words where the model predicts label 1 (fashion-related)\n",
    "    filtered_words = [word for word, label in zip(words, preds) if label == 1]\n",
    "    filtered_keywords.append(filtered_words)\n",
    "\n",
    "# Step 7: Add the new column filtered_keywords and save the DataFrame\n",
    "df['filtered_keywords'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 12997:\n",
      "summary_cleaned: ['尊嘟', '爱', 'redexclamationmark', '说', '富贵', '小姐姐', '穿', '搭', '尝试', '新', '温柔', '气质', '姐姐', '富家', '千金', '穿', '搭', '秋冬', '温柔', '慵懒', '风', '毛衣', '慵懒', '风穿', '搭', '高级', '感穿', '搭']\n",
      "filter_label: ['爱', '说', '富贵', '小姐姐', '穿', '搭', '尝试', '新', '温柔', '气质', '姐姐', '富家', '千金', '秋冬', '慵懒', '风', '毛衣', '风穿', '高级', '感穿']\n",
      "\n",
      "Example 11585:\n",
      "summary_cleaned: ['爱', '温柔', '白月光', '感觉', '试试', '仙气', '裙子', '新', '中式', '穿', '搭', '民国', '风', '温柔', '连衣裙', '国风', '针织', '新', '中式', '套装']\n",
      "filter_label: ['爱', '温柔', '白月光', '感觉', '试试', '仙气', '裙子', '新', '中式', '穿', '搭', '民国', '风', '连衣裙', '国风', '针织', '套装']\n",
      "\n",
      "Example 11226:\n",
      "summary_cleaned: ['我先', '微胖', '女孩', '微胖', '穿', '搭']\n",
      "filter_label: ['微胖', '女孩', '穿', '搭']\n",
      "\n",
      "Example 13552:\n",
      "summary_cleaned: ['redapple', '型', '跟着', '模特', '穿', 'keycapkeycap', '复古', '百褶裙', '太好', '穿', '身高', 'cm', '体重', '斤斤', '小腿', '围', 'cm', '大腿', 'cm', '腰围', 'cm', '肚围', 'cmredapple', '型', '身材', '我太爱', '这件', '短裙', '前', '几年', '尝试', '百褶裙', '没想到', '穿', '感觉', '不错', '哈哈哈', '材质', '厚', '材质', '质感', '苹果', '型', '身材', '苹果', '型', '小个子', '穿', '搭', '短裙', '穿', '搭', '短裙', '格子裙', '百褶裙', '百褶裙', '搭', '百褶裙', '穿', '搭', '格子', '百褶裙', '复古', '百褶裙', '复古', '格子', '半裙', '秋季', '穿', '搭', '秋季', '新款', '跟着', '模特', '学穿', '搭', '美', '拉德', '穿', '搭', '苹果', '型', '身材', '穿', '搭', '小个子', '苹果', '型', '身材', '苹果', '型', '身材', '穿', '搭', '指南', '苹果', '型', '身材', '显瘦', '穿', '搭', '苹果', '型', '身材', '微胖', '穿', '搭微', '苹果', '型', '身材', '针织', '上衣', '舒服', '针织衫', '搭百搭', '针织', '上衣', '早秋', '必备', '针织衫']\n",
      "filter_label: ['redapple', '型', '跟着', '模特', '穿', '复古', '百褶裙', '太好', '身高', '体重', '斤斤', '小腿', '围', '大腿', '腰围', '肚围', '身材', '我太爱', '这件', '短裙', '前', '几年', '尝试', '没想到', '感觉', '不错', '哈哈哈', '材质', '厚', '质感', '苹果', '小个子', '搭', '格子裙', '格子', '半裙', '秋季', '新款', '学穿', '美', '拉德', '指南', '显瘦', '微胖', '搭微', '针织', '上衣', '舒服', '针织衫', '搭百搭', '早秋', '必备']\n",
      "\n",
      "Example 100:\n",
      "summary_cleaned: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '真的', '太', '好看', '质感', '超棒', '软软', '糯', '糯', '外面', '搭个', '大衣', '妥妥', '韩剧', '女主', '疯狂', '爱上', '大学生', '穿', '搭', '大衣', '穿', '搭', '毛衣', '日常', '穿', '搭', '气质', '穿', '搭', '秋冬', '穿', '搭', '小个子', '穿', '搭', '秋冬', '毛衣']\n",
      "filter_label: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '太', '好看', '质感', '超棒', '软软', '糯', '外面', '搭个', '大衣', '韩剧', '女主', '疯狂', '爱上', '大学生', '穿', '搭', '日常', '气质', '秋冬', '小个子']\n",
      "\n",
      "Example 9914:\n",
      "summary_cleaned: ['万圣节', '穿', '穿', '店里', '新品', '拍', '万圣节', '每日', '穿', '搭', '哥特']\n",
      "filter_label: ['万圣节', '穿', '店里', '新品', '拍', '每日', '搭', '哥特']\n",
      "\n",
      "Example 15089:\n",
      "summary_cleaned: ['身材', '巨巨', '巨好', '穿', 'doubleexclamationmark', '微胖', '女生', '大胆', '穿', '冬天', '真的', '穿', '胯', '宽', '腿', '粗', '梨形', '身材', '微胖', '女孩', '穿', '搭', '微胖', '穿', '搭', '日常', '穿', '搭', '梨形', '身材', '穿', '搭', '梨形', '微胖', '梨形', '身材', '粗', '腿', '梨形', '穿', '搭', '梨形', '胯', '宽', '腿', '粗', '穿', '搭', '微胖', '秋冬', '冬季', '穿', '搭', '微胖', '女生', '冬季', '穿', '搭']\n",
      "filter_label: ['身材', '巨巨', '巨好', '穿', '微胖', '女生', '大胆', '冬天', '真的', '胯', '宽', '腿', '粗', '梨形', '女孩', '搭', '日常', '秋冬', '冬季']\n",
      "\n",
      "Example 2309:\n",
      "summary_cleaned: ['基础', 'Tee', '一衣', '穿', '法则', '夏天', '少不了', '基础', 'T恤', '穿', '更', '花样', '性', '选择', '三条', '类型', '裤子', '搭配', '裤型', '搭配', '基础', 'tee', '发挥', '可玩性', '穿', '香', 'ootd', '每日', '穿', '搭', '男生', '穿', '搭', '开春', '穿', '搭', '基础', '短袖', 'osmosismocuishleESOTERICInexistence', '存世', 'GaforReal']\n",
      "filter_label: ['基础', 'Tee', '一衣', '穿', '法则', '夏天', '少不了', 'T恤', '更', '花样', '性', '选择', '类型', '裤子', '搭配', '裤型', 'tee', '发挥', '可玩性', '香', 'ootd', '每日', '搭', '男生', '开春', '短袖', 'GaforReal']\n",
      "\n",
      "Example 4101:\n",
      "summary_cleaned: ['只美', '拉德', '包包', 'stuffedflatbread', '𖧧', '复古', '棕色', '百搭', 'mate', '大容量', '棕色', '系', 'mediumdarkskintone', '滴', '包包', '撞色', '巧克力', '包包', 'chocolatebar', '羊羔', '毛', '包包', 'ewe', '牛仔', '刺绣', '拼接', '棕色', '包包', 'sewingneedle', '哭喊', '中心', '复古', '格纹', '帆布包', 'burrito', '波士顿', '包型', '复古', '腋下', '包', 'meatonbone', '包包', '偏爱', '小众', '包小众', '包包', '大学生', '上课', '包包', '腋下', '包', '帆布包', '美', '拉德', '棕色', '包包', '复古', '包包', '秋冬', '百搭', '包包', '通勤', '包包', '包包', '分享', '宝宝', '辅食', '包包', '重样', '大容量', '包包']\n",
      "filter_label: ['只美', '拉德', '包包', '复古', '棕色', '百搭', 'mate', '大容量', '系', '滴', '撞色', '巧克力', 'chocolatebar', '羊羔', '毛', '牛仔', '刺绣', '拼接', 'sewingneedle', '中心', '格纹', '帆布包', '包型', '腋下', '包', 'meatonbone', '偏爱', '小众', '包小众', '大学生', '上课', '美', '秋冬', '通勤', '分享', '宝宝', '辅食', '重样']\n",
      "\n",
      "Example 4394:\n",
      "summary_cleaned: ['攒', '夏天', '粉色', 'growingheart', '爱', '穿', '粉色', '系', '中年妇女', 'ootd', '每日', '穿', '搭', '日常', '穿', '搭', '每日', '穿', '搭', '粉色', '系', '粉色', '少女', '心']\n",
      "filter_label: ['夏天', '粉色', 'growingheart', '爱', '穿', '系', '中年妇女', 'ootd', '每日', '搭', '日常', '少女', '心']\n",
      "\n",
      "Example 12496:\n",
      "summary_cleaned: ['回村', '冬季', '穿', '搭']\n",
      "filter_label: ['冬季', '穿', '搭']\n",
      "\n",
      "Example 2827:\n",
      "summary_cleaned: ['条纹', '简约', '款', '长袖', 'doubleexclamationmarkMalaysiains', '条纹', '长袖', '短版', '件套', '平价', '好物', '穿', '香', '挑战', '星期', '穿', '搭', '重样', '笔记', '灵感', '马来西亚', '服装店', '生']\n",
      "filter_label: ['条纹', '简约', '款', '长袖', 'doubleexclamationmarkMalaysiains', '短版', '件套', '平价', '好物', '穿', '香', '挑战', '搭', '重样', '笔记', '灵感', '服装店', '生']\n",
      "\n",
      "Example 15129:\n",
      "summary_cleaned: ['一夜', '爆火', '穿', '搭', '套路', '六一', '猫', '穿', '搭', '普通人', '穿', '搭', '博主穿', '搭', 'ootd']\n",
      "filter_label: ['一夜', '爆火', '穿', '搭', '六一', '猫', '普通人', '博主穿', 'ootd']\n",
      "\n",
      "Example 9491:\n",
      "summary_cleaned: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '白', '鸭绒', '这件', '羽绒服', '棒球', '服', '款式', '时尚', '保暖', 'smilingfacewithhearts', '厚度', '不算', '厚', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '羽绒服', '短款', '羽绒服', '羽绒服', '推荐', '穿着', '还显', '瘦', '羽绒服']\n",
      "filter_label: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '白', '鸭绒', '这件', '服', '款式', '时尚', '保暖', '厚度', '不算', '厚', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '短款', '推荐', '穿着', '还显', '瘦']\n",
      "\n",
      "Example 4194:\n",
      "summary_cleaned: ['平价', '显瘦', '针织衫', '三十多', 'moneybag', '版型', '巨', '好好', '温柔', '氛围', '感', '懂', '软乎乎', '不扎', '弹力', '没', '起球', '针织衫', '搭', '显瘦', '穿', '搭', '平价', '好物', '梨形', '身材', '穿', '香', '秋冬', '穿', '搭', '平价', '好物', '学生', '党']\n",
      "filter_label: ['平价', '显瘦', '针织衫', '三十多', '版型', '巨', '好好', '温柔', '氛围', '感', '懂', '软乎乎', '不扎', '弹力', '没', '起球', '搭', '穿', '好物', '梨形', '身材', '香', '秋冬', '学生', '党']\n",
      "\n",
      "Example 2920:\n",
      "summary_cleaned: ['宜家', '镜子', '好好看', '记录', '生活', '甜妹', '每日', '穿', '搭', '软妹', '穿', '搭', '宜家', '宜家', '拍照', '宜家', '好物']\n",
      "filter_label: ['镜子', '好好看', '记录', '生活', '甜妹', '每日', '穿', '搭', '软妹', '拍照', '好物']\n",
      "\n",
      "Example 3775:\n",
      "summary_cleaned: ['eplus', '姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', 'doubleexclamationmark', '浪漫生活', '记录', '笔记', '灵感', '服装', '测评', '套装', '平价', '衣服', '测评', '初秋', '穿', '搭']\n",
      "filter_label: ['姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', '浪漫生活', '记录', '笔记', '灵感', '服装', '平价', '衣服', '穿', '搭']\n",
      "\n",
      "Example 4534:\n",
      "summary_cleaned: ['几十块', 'lulu', '同款', '外套', '白色', '直', '筒裤', '太显', '瘦', '极', '简穿', '搭', '超会', '穿', '企划', 'lulu', '平替', 'ootd', '每日', '穿', '搭', '显瘦', '穿', '搭', 'ootd', '白色', '牛仔裤', '拍照', '姿势', '重样', 'lulu', '黑色']\n",
      "filter_label: ['几十块', 'lulu', '同款', '外套', '白色', '直', '筒裤', '太显', '瘦', '极', '简穿', '搭', '超会', '穿', '企划', '平替', 'ootd', '每日', '显瘦', '牛仔裤', '拍照', '姿势', '重样', '黑色']\n",
      "\n",
      "Example 10565:\n",
      "summary_cleaned: ['Meetingyouistrulyamiracleofmylif', '六芒星', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '生命', '中', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '手镯', '手工', '件', '不可思议', '事儿', '纯', '银手镯', '手工', '银饰', '首饰', 'blingbling', '晒晒', '手上', '戴', '手工', '手镯']\n",
      "filter_label: ['Meetingyouistrulyamiracleofmylif', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '生命', '中', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '件', '不可思议', '事儿', '纯', '银手镯', '银饰', '首饰', 'blingbling', '晒晒', '手上', '戴']\n",
      "\n",
      "Example 7473:\n",
      "summary_cleaned: ['微胖', '胯', '宽', '妹妹', '我选', '简简单单', '耐看', 'H', '型', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '露', '肩', '不露', '肩', '美杏色', '连衣裙', '气质', '代表', '色', '腰部', '褶皱', '遮', '肚子', '肉', '肉', '简单', '显', '气质', '爸妈', '夸', '裙子', '安利', '月', '第一张', '照片', '笔记', '灵感', '夏日', '宝藏', '裙子', '蓬蓬裙', '微胖', '穿', '搭', '显瘦', '穿', '搭', '显瘦', '连衣裙', '连衣裙', '气质', '连衣裙']\n",
      "filter_label: ['微胖', '胯', '宽', '妹妹', '我选', '简简单单', '耐看', '型', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '露', '肩', '不露', '美杏色', '气质', '色', '腰部', '褶皱', '遮', '肚子', '肉', '简单', '显', '爸妈', '夸', '裙子', '安利', '月', '第一张', '照片', '笔记', '灵感', '宝藏', '蓬蓬裙', '穿', '搭', '显瘦']\n",
      "\n",
      "Example 6619:\n",
      "summary_cleaned: ['cm', '斤', '美式', '复古', '运动', '女孩', '宽松', '紧身', '混', '搭', '缺一不可', '小个子', '穿', '搭', '美式', '穿', '搭', '辣妹', '穿', '搭', '日常', '穿', '搭', '冬季', '穿', '搭']\n",
      "filter_label: ['美式', '复古', '运动', '女孩', '宽松', '紧身', '混', '搭', '缺一不可', '小个子', '穿', '辣妹', '日常', '冬季']\n",
      "\n",
      "Example 5295:\n",
      "summary_cleaned: ['教师', '穿', '搭', '梨形', '身材', '请', '这条', '神裤', '焊', '身上', '蓝', '衬衫', '白', '裤子', '真的', '太', '清爽', '耐看', '氧气', '感', '十足', '梨形', '身材', '找到', '本命', '神裤', '腰围', '臀围', '腰围', '刚刚', '臀围', '放量', '很大', '真的', '超显', '腿', '直', '穿', '香', '教师', '穿', '搭', '穿', '搭', '梨形', '身材', '裤子', '白色', '裤子', 'yyds', '法式', '复古', '穿', '搭', '搭', '裤子', '气质', '穿', '搭薯', '队长', '薯', '管家', '潮流', '薯', '时尚', '薯']\n",
      "filter_label: ['教师', '穿', '搭', '梨形', '身材', '请', '这条', '神裤', '焊', '身上', '蓝', '衬衫', '白', '裤子', '真的', '太', '清爽', '耐看', '氧气', '感', '十足', '找到', '本命', '腰围', '臀围', '刚刚', '放量', '很大', '超显', '腿', '直', '香', '白色', 'yyds', '法式', '复古', '气质', '搭薯', '队长', '薯', '管家', '潮流', '时尚']\n",
      "\n",
      "Example 8590:\n",
      "summary_cleaned: ['学生', '党', '秋冬', '百搭', '长裤', 'jeans', '显高显', '瘦', '显腿直', '神裤', '低', 'keycapkeycapkeycapkeycapkeycapkeycap', '裤子', '长裤', '秋冬', '长裤', '百搭显', '瘦', '神裤', '不露', '腿', '穿', '搭', '高个子', '裤子', '加绒', '裤子']\n",
      "filter_label: ['学生', '党', '秋冬', '百搭', '长裤', '显高显', '瘦', '显腿直', '神裤', '低', '裤子', '百搭显', '不露', '腿', '穿', '搭', '高个子', '加绒']\n",
      "\n",
      "Example 14843:\n",
      "summary_cleaned: ['黑色', '系', '每日', '穿', '搭', '穿', '搭']\n",
      "filter_label: ['黑色', '系', '每日', '穿', '搭']\n",
      "\n",
      "Example 14141:\n",
      "summary_cleaned: ['小个子', '女生', '穿', '这套', 'COOLbutton', '今日', '快乐', '今日', '发起', '风', '穿卫衣', '好开心', '穿卫衣', '搭配', '牛仔裤', '喜欢', '深色', '牛仔裤', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '穿', '搭', '日常', '穿', '搭', '穿', '搭', '小个子', '女生', '穿', '搭', 'ootd', '每日', '穿', '搭', '显瘦', '穿', '搭', '气质', '穿', '搭', '搭', '裤子', '女生']\n",
      "filter_label: ['小个子', '女生', '穿', '这套', 'COOLbutton', '今日', '快乐', '发起', '风', '穿卫衣', '好开心', '搭配', '牛仔裤', '喜欢', '深色', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '搭', '日常', 'ootd', '显瘦', '气质', '裤子']\n",
      "\n",
      "Example 11979:\n",
      "summary_cleaned: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '空空', '一身', '粉粉', '爱', '吊带', '低腰裤', '甜心', '辣妹', 'yk', '辣妹', '穿', '搭', '日常']\n",
      "filter_label: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '一身', '粉粉', '爱', '吊带', '低腰裤', 'yk', '穿', '搭', '日常']\n",
      "\n",
      "Example 11515:\n",
      "summary_cleaned: ['carrot', '裙子', '喜欢', '色', '安排', '做好', '样衣', '拍', '正式', '图', '准确', '实物', '一张', '滴', 'p', '图', '效果', '影响', '观看', '配色', 'Lolita']\n",
      "filter_label: ['carrot', '裙子', '喜欢', '色', '安排', '做好', '样衣', '拍', '正式', '图', '准确', '实物', '一张', '滴', 'p', '效果', '影响', '观看', '配色', 'Lolita']\n",
      "\n",
      "Example 5370:\n",
      "summary_cleaned: ['贵气感', '源头', '血气方刚', '贵气感', '说', '吃', '穿', '多贵', '身上', '那种', '战斗力', '身上', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '更', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '喜欢', '熬夜', '喝酒', '那阵子', '真的', '元气大伤', '补', '一阵子', '气血', '真的', '感觉', '挺', '有用', '脸上', '皮肤', '透亮', '气血', '涌上来', '那种', '红润', '感', '吃', '阿胶', '入', '液体', '阿胶', '口服液', '听说', '液体', '更好', '吸收', '福站', '家', '口服液', '一瓶', 'ml', '阿胶', '成分', '含', 'ml', '剩下', '当归', '党参', '气血', '双补', '阳完', '真的', '感冒', '频繁', '福站', '阿胶', '口服液', '喝', '整年', '去年', '十月', '喝', '平时', '出去玩', '小小的', '一只', '放在', '包里', '喝', '没想到', '效果', 'ym', '不调', '姐妹', '想到', '天天', '喝酒', 'ym', '每次', '特别', '规律', '想', '显得', '贵气', '气质', '气', '场上', '输', '方法', '简单', '缺什么', '补', '精神状态', '喝酒', '调', '微醺', '阿胶', '口服液', '拍照', '日常', '穿', '搭', '享受', '生活', '气血', '补', '喝酒', '微醺', '氛围', '感', '辣妹']\n",
      "filter_label: ['贵气感', '说', '吃', '穿', '多贵', '身上', '那种', '战斗力', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '更', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '熬夜', '喝酒', '那阵子', '真的', '补', '一阵子', '气血', '感觉', '挺', '有用', '皮肤', '透亮', '涌上来', '感', '入', '液体', '口服液', '听说', '更好', '吸收', '家', '一瓶', '成分', '含', '剩下', '双补', '感冒', '频繁', '喝', '整年', '去年', '平时', '出去玩', '小小的', '一只', '放在', '包里', '没想到', '效果', '不调', '姐妹', '想到', '天天', '每次', '特别', '想', '显得', '贵气', '气', '场上', '输', '方法', '简单', '调', '微醺', '拍照', '日常', '搭', '享受', '生活', '氛围', '辣妹']\n",
      "\n",
      "Example 2141:\n",
      "summary_cleaned: ['天', '我会', '穿', '香云纱', '改良', '汉服', '香云纱', '改良', '日常', '款', '汉服', '懒', '拒绝', '繁琐', '一套', '出门', '香云纱', '香云纱', '新国潮', '穿', '搭', '新', '中式', '改良', '汉服', '汉服', '汉服', '穿', '搭', '汉服', '日常', '新', '中式', '穿', '搭']\n",
      "filter_label: ['天', '我会', '穿', '香云纱', '改良', '汉服', '日常', '款', '懒', '拒绝', '繁琐', '一套', '出门', '新国潮', '搭', '新', '中式']\n",
      "\n",
      "Example 1164:\n",
      "summary_cleaned: ['继', '山本', '裤后', '春夏', '神裤', '这肤', '感', '舒服', '绝', '啊啊啊', '真的', '舒服', '没', '穿', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '起球', '不易', '皱', '着实', '无可挑剔', '神裤', '啊啊啊', '感觉', '必大', '爆神裤', '裤子', '裤子', '种草', '搭', '裤子', '显瘦', '神裤', '粉色', '裤子', '小个子', '裤子', '梨型', '身材', '裤子', '百搭神裤', '夏季', '裤子', 'OotD', '每日', '穿', '搭', 'ootd', '每日', '穿', '搭', '春夏', '穿', '搭', '职场', '通勤', '穿', '搭', '日常', '穿', '搭', '显瘦', '穿', '搭', '裤子', '穿', '搭', '微胖', '女孩', '穿', '搭', '夏日', '穿', '搭', '休闲', '穿', '搭', '慵懒', '裤子', '裤子', '推荐', '工人', '神裤', '垂感', '裤子', '梨形', '身材', '条', '裤子', '真的', '绝']\n",
      "filter_label: ['山本', '裤后', '春夏', '神裤', '这肤', '感', '舒服', '绝', '啊啊啊', '真的', '没', '穿', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '起球', '不易', '皱', '着实', '无可挑剔', '感觉', '爆神裤', '裤子', '种草', '搭', '显瘦', '粉色', '小个子', '梨型', '身材', '百搭神裤', '夏季', 'OotD', '每日', 'ootd', '职场', '通勤', '日常', '微胖', '女孩', '夏日', '休闲', '慵懒', '推荐', '工人', '垂感', '梨形', '条']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: 随机采样30行\n",
    "sampled_df = df.sample(n=30, random_state=42)  # 随机采样30行\n",
    "\n",
    "# Step 3: 去重函数，保持顺序不变\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: 打印输出其中的 summary_cleaned 列和 filter_label 列，去掉 filter_label 中的重复单词\n",
    "for i, row in sampled_df.iterrows():\n",
    "    summary_cleaned = eval(row['cleaned_post_text'])  # 转换为列表\n",
    "    filter_label = eval(row['filtered_keywords'])  # 转换为列表\n",
    "    filter_label_unique = remove_duplicates(filter_label)  # 去重\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"summary_cleaned: {summary_cleaned}\")\n",
    "    print(f\"filter_label: {filter_label_unique}\")\n",
    "    print()  # 空行用于美观输出"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
