{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion-related samples (positive): 52242\n",
      "Number of non-fashion-related samples (negative): 304611\n",
      "The datasets are not balanced. Difference: 252369\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load fashion-related data (positive samples)\n",
    "fashion_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword' column which contains fashion-related keywords\n",
    "fashion_samples = fashion_df['keyword group']\n",
    "\n",
    "# Directory containing non-fashion-related samples\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "\n",
    "# Initialize an empty list to store non-fashion samples\n",
    "non_fashion_samples = []\n",
    "\n",
    "# Recursively iterate through all subdirectories and files in non_fashion_texts directory\n",
    "for root, dirs, files in os.walk(neg_dir):\n",
    "    for file in files:\n",
    "        # Read only text files\n",
    "        if file.startswith('wiki'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Add each line as a sample\n",
    "                lines = f.readlines()\n",
    "                non_fashion_samples.extend([line.strip() for line in lines if line.strip()])\n",
    "\n",
    "# Convert non-fashion samples to a pandas Series for consistency\n",
    "non_fashion_samples = pd.Series(non_fashion_samples)\n",
    "\n",
    "# Step 2: Count the number of samples in each dataset\n",
    "num_fashion_samples = fashion_samples.shape[0]\n",
    "num_non_fashion_samples = non_fashion_samples.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of fashion-related samples (positive): {num_fashion_samples}\")\n",
    "print(f\"Number of non-fashion-related samples (negative): {num_non_fashion_samples}\")\n",
    "\n",
    "# Step 3: Check if they are balanced\n",
    "if num_fashion_samples == num_non_fashion_samples:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(f\"The datasets are not balanced. Difference: {abs(num_fashion_samples - num_non_fashion_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling:\n",
    "- randomly reducing the number of samples from the majority class (non-fashion-related samples) to match the number of samples in the minority class (fashion-related samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downsampled non-fashion-related samples: 52242\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# If the non-fashion dataset is larger, downsample it\n",
    "if num_non_fashion_samples > num_fashion_samples:\n",
    "    # Randomly select `num_fashion_samples` non-fashion samples\n",
    "    downsampled_non_fashion_samples = non_fashion_samples.sample(n=num_fashion_samples, random_state=42)\n",
    "else:\n",
    "    downsampled_non_fashion_samples = non_fashion_samples\n",
    "\n",
    "# Print the number of samples after downsampling\n",
    "print(f\"Number of downsampled non-fashion-related samples: {downsampled_non_fashion_samples.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read Data Files\n",
    "- read positive samples\n",
    "- read negative samples\n",
    "- combine the datasets\n",
    "#### Step 2: Load pre-trained sentence Transformer Model\n",
    "- load text2vec-large-chinese model\n",
    "- generate embeddings for each word\n",
    "#### Step 3: Get embeddings for the dataset\n",
    "- apply the embeddings function\n",
    "- convert labels to tensor\n",
    "#### Step 4: Train-test split\n",
    "- split the data\n",
    "- create PyTorh TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (356855, 2)\n",
      "Data Format Example:\n",
      "                                                word  label\n",
      "0  由百獸戰隊牙吠連者起，在全員唱完名後會喊出一句標語，再接戰隊的隊名，而要準備開戰前，紅戰士都...      0\n",
      "1                                                歷史.      0\n",
      "2                                            傳統建築語彙.      0\n",
      "3        石洞口发电厂目前共有6台发电机组，总装机容量240万千瓦。在组织结构上，分为三个实体。      0\n",
      "4                                               LNTU      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Progress: 100%|██████████| 11152/11152 [17:51<00:00, 10.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([356855, 1024])\n",
      "Embedding Example:\n",
      "tensor([-0.8630,  0.6928, -0.4863,  ...,  0.3087, -0.2302,  0.3473],\n",
      "       device='cuda:0')\n",
      "Labels Shape: torch.Size([356855])\n",
      "Labels Example:\n",
      "tensor([0, 0, 0, 0, 0])\n",
      "Train Embeddings Shape: torch.Size([285484, 1024]), Train Labels Shape: torch.Size([285484])\n",
      "Val Embeddings Shape: torch.Size([71371, 1024]), Val Labels Shape: torch.Size([71371])\n",
      "Train Loader Batch Size: 8922 batches\n",
      "Val Loader Batch Size: 2231 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Read Positive Samples from 'topics_filtered.csv'\n",
    "# Load the fashion lexicon (positive samples)\n",
    "pos_file = \"/home/disk1/red_disk1/fashion/topics_filtered.csv\"\n",
    "positive_samples_df = pd.read_csv(pos_file)\n",
    "positive_samples = positive_samples_df['keyword group'].dropna().tolist()  # Extract keywords column\n",
    "\n",
    "# Assign label 1 to positive samples\n",
    "positive_samples = pd.DataFrame(positive_samples, columns=['word'])\n",
    "positive_samples['label'] = 1\n",
    "\n",
    "# Step 2: Read Negative Samples from non_fashion_texts\n",
    "def read_negative_samples(directory):\n",
    "    all_lines = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    all_lines.extend([line.strip() for line in file if line.strip() != \"\"])\n",
    "    return all_lines\n",
    "\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "negative_samples_list = read_negative_samples(neg_dir)\n",
    "\n",
    "# Assign label 0 to negative samples\n",
    "negative_samples = pd.DataFrame(negative_samples_list, columns=['word'])\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "# Step 3: Combine Positive and Negative Samples\n",
    "data = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shape and format of the data\n",
    "print(f\"Combined Data Shape: {data.shape}\")\n",
    "print(f\"Data Format Example:\\n{data.head()}\")\n",
    "\n",
    "# Step 4: Load text2vec-large-chinese model\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Embedding Progress\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 5: Get embeddings for the dataset\n",
    "embeddings = get_text2vec_embeddings(data['word'], text2vec_model)\n",
    "\n",
    "# Print the shape and format of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding Example:\\n{embeddings[0]}\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Print the labels format\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels Example:\\n{labels[:5]}\")\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the split data shapes\n",
    "print(f\"Train Embeddings Shape: {train_embeddings.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Embeddings Shape: {val_embeddings.shape}, Val Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print the data loader batch size\n",
    "print(f\"Train Loader Batch Size: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader Batch Size: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a ResNet-based binary classification model\n",
    "1. Model Definition: ResNet-Based Binary Classifier\n",
    "2. Training Loop\n",
    "3. Validation Loop\n",
    "4. Training and Evaluation\n",
    "\n",
    "#### Summary\n",
    "- Model Setup: It initializes a ResNet18 model tailored for binary classification using text embeddings as inputs.\n",
    "- Training and Validation: The code implements a standard training loop with backpropagation and uses validation to track performance.\n",
    "- Performance Metrics: Accuracy, precision, recall, and F1 score are calculated to evaluate the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [05:56<00:00, 25.04it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:08<00:00, 270.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0862, Val Loss: 0.0649, Accuracy: 0.9743\n",
      "Precision: 0.9378, Recall: 0.8820, F1 Score: 0.9090\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [05:52<00:00, 25.33it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:08<00:00, 270.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0546, Val Loss: 0.0606, Accuracy: 0.9764\n",
      "Precision: 0.8931, Recall: 0.9518, F1 Score: 0.9215\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [05:53<00:00, 25.24it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:08<00:00, 271.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411, Val Loss: 0.0510, Accuracy: 0.9801\n",
      "Precision: 0.9150, Recall: 0.9518, F1 Score: 0.9330\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [05:57<00:00, 24.94it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:08<00:00, 270.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0322, Val Loss: 0.0469, Accuracy: 0.9820\n",
      "Precision: 0.9435, Recall: 0.9321, F1 Score: 0.9378\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [05:56<00:00, 25.05it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:08<00:00, 271.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0261, Val Loss: 0.0480, Accuracy: 0.9815\n",
      "Precision: 0.9386, Recall: 0.9336, F1 Score: 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 324.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: Accuracy: 0.9815, Precision: 0.9386, Recall: 0.9336, F1 Score: 0.9361\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 7: Define ResNet-Based Binary Classification Model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):  # Adjust embedding_dim based on text2vec output (1024)\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        # Use pre-trained ResNet18, but replace the first conv layer to accept text2vec embeddings\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Binary classification\n",
    "        \n",
    "        # Define a simple MLP layer to map embeddings to ResNet input size\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),  # Match the expected ResNet input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),  # Match the expected ResNet input size after projection\n",
    "        )\n",
    "        \n",
    "        # Adding a Conv layer to convert 1D to a 2D feature map\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "\n",
    "        # Adaptive pooling layer to transform the spatial dimensions to (7, 7)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)  # Project text2vec embeddings to match ResNet input size\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # Reshape to fit Conv2D (batch_size, channels, height, width)\n",
    "        x = self.embedding_conv(x)  # Use conv layer to convert 1D feature to 2D feature map\n",
    "        x = self.adaptive_pool(x)  # Apply adaptive pooling to resize to (7, 7)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct embedding dimension (1024 as per the output shape)\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "\n",
    "# Step 8: Define Training Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Progress\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Progress\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "# Step 9: Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 10: Final Evaluation\n",
    "val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "print(f\"Final Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /home/disk1/red_disk1/fashion/tfashion\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Save the trained model\n",
    "\n",
    "# Save model and tokenizer to the desired directory\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion\"\n",
    "\n",
    "# Make the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, \"tfashion.pth\"))\n",
    "\n",
    "# Optionally, save additional model metadata if needed (e.g., optimizer state, epoch)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(model_save_path, \"tfashion_checkpoint.pth\"))\n",
    "\n",
    "print(f\"Model saved at: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare post data\n",
    "- Combine 'post_title' and 'post_content' into 'post_text'\n",
    "- Clean 'post_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean.csv')\n",
    "# df = pd.read_csv('/home/disk1/red_disk1/poster_9305.csv')\n",
    "\n",
    "# Combine 'post_title' and 'post_content' into 'post_text'\n",
    "df['post_text'] = df['post_title'].fillna('') + ' ' + df['post_content'].fillna('')\n",
    "\n",
    "# Drop 'post_title' and 'post_content' columns\n",
    "df = df.drop(columns=['post_title', 'post_content', 'post_tag'])\n",
    "\n",
    "# Save the updated DataFrame\n",
    "# df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_9305_combined.csv', index=False)\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "# Load stopwords from the provided file\n",
    "with open('/home/disk1/red_disk1/fashion/stopwords_cn.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Function for text cleaning\n",
    "def clean_text(text, stopwords):\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove specific patterns\n",
    "    text = re.sub(r'- 小红书,,', '', text)  # Removing \"- 小红书,,\"\n",
    "    text = re.sub(r'小红书', '', text)  # Explicitly remove \"小红书\"\n",
    "    text = re.sub(r',,\\d{2}-\\d{2},,', '', text)  # Removing patterns like \",,XX-XX,,\"\n",
    "    text = re.sub(r'#', ' ', text)  # Replace '#' with a space\n",
    "    text = re.sub(r'\\s+', '', text)  # This will remove all whitespace characters\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    # Remove stopwords (word-based removal)\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopwords])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function to 'post_text'\n",
    "df['cleaned_post_text'] = df['post_text'].apply(lambda x: clean_text(str(x), stopwords))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/post_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Test the model tfashion\n",
    "- Step 1: Load the test dataset.\n",
    "- Step 2: Generate embeddings for each word in the test dataset using text2vec-large-chinese.\n",
    "- Step 3: Use a pre-trained ResNet-based classifier model to predict whether each word is related to fashion or not.\n",
    "- Step 4: Save the predictions alongside the original data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/disk1/red_disk1/fashion/text2vec-large-chinese.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/disk1/red_disk1/fashion/text2vec-large-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m text2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Function to get embeddings from text2vec-large-chinese\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text2vec_embeddings\u001b[39m(texts, model):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:306\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    294\u001b[0m         modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    295\u001b[0m             model_name_or_path,\n\u001b[1;32m    296\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    319\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:1454\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[1;32m   1452\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[0;32m-> 1454\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1461\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:56\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     55\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     59\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:87\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:3504\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3500\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3501\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3502\u001b[0m         )\n\u001b[1;32m   3503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3505\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3506\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3507\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3508\u001b[0m         )\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3510\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/disk1/red_disk1/fashion/text2vec-large-chinese."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"  # Update this path if necessary\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['cleaned_post_text']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained tfashion model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained tfashion model from your specified path\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test-output.csv\"  \n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the model\n",
    "1. Tokenization: tokenize the post_text content\n",
    "2. Word Embedding: pass each word (instead of the entire post) through the text2vec-large-chinese model to generate embeddings for each word\n",
    "3. Classification: pass embeddings through the tfashion model to classify whether each word is fashion-related or not\n",
    "4. Filter out any words that are present in the stopwords set before applying the model classification\n",
    "5. Filter out single-character words\n",
    "6. Results: The output will be a classification for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.498 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Processing Rows: 100%|██████████| 15622/15622 [08:28<00:00, 30.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results saved to /home/disk1/red_disk1/fashion/test_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Step 1: Load the stopwords file\n",
    "stopwords_file = \"/home/disk1/red_disk1/fashion/stopwords_cn.txt\"\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "# Step 2: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 3: Define ResNetBinaryClassifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Load the CSV file containing post_text\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 5: Tokenize the post_text content into words and remove stopwords\n",
    "def tokenize_text(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    # Retain English words using regex (keeping words with ASCII characters)\n",
    "    english_words = re.findall(r'[a-zA-Z]+', text)\n",
    "    # Filter out stopwords and single-character Chinese words\n",
    "    filtered_words = [word for word in words if word not in stopwords and len(word) > 1]\n",
    "    return filtered_words + english_words  # Keep both filtered Chinese and English words\n",
    "\n",
    "df['tokenized_text'] = df['cleaned_post_text'].apply(tokenize_text)\n",
    "\n",
    "# Step 6: Get embeddings for each word in the tokenized_text column and classify them\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    words = row['tokenized_text']\n",
    "    \n",
    "    # Get embeddings for the words\n",
    "    embeddings = get_text2vec_embeddings(words, text2vec_model).to(device)\n",
    "    \n",
    "    # Make predictions using the pre-trained ResNet binary classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Filter words where the model predicts label 1 (fashion-related), and keep English words\n",
    "    filtered_words = [word for word, label in zip(words, preds) if label == 1 or re.match(r'[a-zA-Z]+', word)]\n",
    "    filtered_keywords.append(filtered_words)\n",
    "\n",
    "# Step 7: Add the new column filtered_keywords and save the DataFrame\n",
    "df['filtered_keywords2'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 12997:\n",
      "summary_cleaned: ['尊嘟', 'redexclamationmark', '富贵', '小姐姐', '尝试', '温柔', '气质', '姐姐', '富家', '千金', '秋冬', '温柔', '慵懒', '毛衣', '慵懒', '风穿', '高级', '感穿', 'redexclamationmark', 'look']\n",
      "filter_label: ['尊嘟', 'redexclamationmark', '富贵', '小姐姐', '尝试', '温柔', '气质', '姐姐', '富家', '千金', '秋冬', '慵懒', '毛衣', '风穿', '高级', '感穿', 'look']\n",
      "\n",
      "Example 11585:\n",
      "summary_cleaned: ['温柔', '白月光', '感觉', '试试', '仙气', '裙子', '中式', '民国', '温柔', '连衣裙', '国风', '针织', '中式', '套装']\n",
      "filter_label: ['温柔', '白月光', '感觉', '试试', '仙气', '裙子', '中式', '民国', '连衣裙', '国风', '针织', '套装']\n",
      "\n",
      "Example 11226:\n",
      "summary_cleaned: ['我先', '微胖', '女孩', '微胖']\n",
      "filter_label: ['我先', '微胖', '女孩']\n",
      "\n",
      "Example 13552:\n",
      "summary_cleaned: ['redapple', '跟着', '模特', 'keycapkeycap', '复古', '百褶裙', '太好', '身高', 'cm', '体重', '斤斤', '小腿', 'cm', '大腿', 'cm', '腰围', 'cm', '肚围', 'cmredapple', '身材', '我太爱', '这件', '短裙', '几年', '尝试', '百褶裙', '没想到', '感觉', '不错', '哈哈哈', '材质', '材质', '质感', '苹果', '身材', '苹果', '小个子', '短裙', '短裙', '格子裙', '百褶裙', '百褶裙', '百褶裙', '格子', '百褶裙', '复古', '百褶裙', '复古', '格子', '半裙', '秋季', '秋季', '新款', '跟着', '模特', '学穿', '拉德', '苹果', '身材', '小个子', '苹果', '身材', '苹果', '身材', '指南', '苹果', '身材', '显瘦', '苹果', '身材', '微胖', '搭微', '苹果', '身材', '针织', '上衣', '舒服', '针织衫', '搭百搭', '针织', '上衣', '早秋', '必备', '针织衫', 'redapple', 'keycapkeycap', 'cm', 'cm', 'cm', 'cm', 'cmredapple']\n",
      "filter_label: ['redapple', '跟着', '模特', 'keycapkeycap', '复古', '百褶裙', '太好', '身高', 'cm', '体重', '斤斤', '小腿', '大腿', '腰围', '肚围', 'cmredapple', '身材', '我太爱', '这件', '短裙', '几年', '尝试', '没想到', '感觉', '不错', '哈哈哈', '材质', '质感', '苹果', '小个子', '格子裙', '格子', '半裙', '秋季', '新款', '学穿', '拉德', '指南', '显瘦', '微胖', '搭微', '针织', '上衣', '舒服', '针织衫', '搭百搭', '早秋', '必备']\n",
      "\n",
      "Example 100:\n",
      "summary_cleaned: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '真的', '好看', '质感', '超棒', '软软', '外面', '搭个', '大衣', '妥妥', '韩剧', '女主', '疯狂', '爱上', '大学生', '大衣', '毛衣', '日常', '气质', '秋冬', '小个子', '秋冬', '毛衣']\n",
      "filter_label: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '好看', '质感', '超棒', '软软', '外面', '搭个', '大衣', '妥妥', '韩剧', '女主', '疯狂', '爱上', '大学生', '日常', '气质', '秋冬', '小个子']\n",
      "\n",
      "Example 9914:\n",
      "summary_cleaned: ['万圣节', '店里', '新品', '万圣节', '每日', '哥特']\n",
      "filter_label: ['万圣节', '店里', '新品', '每日', '哥特']\n",
      "\n",
      "Example 15089:\n",
      "summary_cleaned: ['身材', '巨巨', '巨好', 'doubleexclamationmark', '微胖', '女生', '大胆', '冬天', '真的', '梨形', '身材', '微胖', '女孩', '微胖', '日常', '梨形', '身材', '梨形', '微胖', '梨形', '身材', '梨形', '梨形', '微胖', '秋冬', '冬季', '微胖', '女生', '冬季', 'doubleexclamationmark']\n",
      "filter_label: ['身材', '巨巨', 'doubleexclamationmark', '微胖', '女生', '大胆', '冬天', '真的', '梨形', '女孩', '日常', '秋冬', '冬季']\n",
      "\n",
      "Example 2309:\n",
      "summary_cleaned: ['基础', 'Tee', '一衣', '法则', '夏天', '少不了', '基础', 'T恤', '花样', '选择', '三条', '类型', '裤子', '搭配', '裤型', '搭配', '基础', 'tee', '发挥', '可玩性', 'ootd', '每日', '男生', '开春', '基础', '短袖', 'osmosismocuishleESOTERICInexistence', '存世', 'GaforReal', 'Tee', 'T', 'tee', 'ootd', 'osmosismocuishleESOTERICInexistence', 'GaforReal']\n",
      "filter_label: ['基础', 'Tee', '一衣', '法则', '夏天', '少不了', 'T恤', '花样', '选择', '类型', '裤子', '搭配', '裤型', 'tee', '发挥', '可玩性', 'ootd', '每日', '男生', '开春', '短袖', 'osmosismocuishleESOTERICInexistence', 'GaforReal', 'T']\n",
      "\n",
      "Example 4101:\n",
      "summary_cleaned: ['只美', '拉德', '包包', 'stuffedflatbread', '复古', '棕色', '百搭', 'mate', '大容量', '棕色', 'mediumdarkskintone', '包包', '撞色', '巧克力', '包包', 'chocolatebar', '羊羔', '包包', 'ewe', '牛仔', '刺绣', '拼接', '棕色', '包包', 'sewingneedle', '哭喊', '中心', '复古', '格纹', '帆布包', 'burrito', '波士顿', '包型', '复古', '腋下', 'meatonbone', '包包', '偏爱', '小众', '包小众', '包包', '大学生', '上课', '包包', '腋下', '帆布包', '拉德', '棕色', '包包', '复古', '包包', '秋冬', '百搭', '包包', '通勤', '包包', '包包', '分享', '宝宝', '辅食', '包包', '重样', '大容量', '包包', 'stuffedflatbread', 'mate', 'mediumdarkskintone', 'chocolatebar', 'ewe', 'sewingneedle', 'burrito', 'meatonbone']\n",
      "filter_label: ['只美', '拉德', '包包', 'stuffedflatbread', '复古', '棕色', '百搭', 'mate', '大容量', 'mediumdarkskintone', '撞色', '巧克力', 'chocolatebar', '羊羔', 'ewe', '牛仔', '刺绣', '拼接', 'sewingneedle', '中心', '格纹', '帆布包', 'burrito', '包型', '腋下', 'meatonbone', '偏爱', '小众', '包小众', '大学生', '上课', '秋冬', '通勤', '分享', '宝宝', '辅食', '重样']\n",
      "\n",
      "Example 4394:\n",
      "summary_cleaned: ['夏天', '粉色', 'growingheart', '粉色', '中年妇女', 'ootd', '每日', '日常', '每日', '粉色', '粉色', '少女', 'growingheart', 'ootd']\n",
      "filter_label: ['夏天', '粉色', 'growingheart', '中年妇女', 'ootd', '每日', '日常', '少女']\n",
      "\n",
      "Example 12496:\n",
      "summary_cleaned: ['回村', '冬季']\n",
      "filter_label: ['回村', '冬季']\n",
      "\n",
      "Example 2827:\n",
      "summary_cleaned: ['条纹', '简约', '长袖', 'doubleexclamationmarkMalaysiains', '条纹', '长袖', '短版', '件套', '平价', '好物', '挑战', '星期', '重样', '笔记', '灵感', '马来西亚', '服装店', 'doubleexclamationmarkMalaysiains']\n",
      "filter_label: ['条纹', '简约', '长袖', 'doubleexclamationmarkMalaysiains', '短版', '件套', '平价', '好物', '挑战', '重样', '笔记', '灵感', '服装店']\n",
      "\n",
      "Example 15129:\n",
      "summary_cleaned: ['一夜', '爆火', '套路', '六一', '普通人', '博主穿', 'ootd', 'ootd']\n",
      "filter_label: ['一夜', '爆火', '套路', '六一', '普通人', '博主穿', 'ootd']\n",
      "\n",
      "Example 9491:\n",
      "summary_cleaned: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '鸭绒', '这件', '羽绒服', '棒球', '款式', '时尚', '保暖', 'smilingfacewithhearts', '厚度', '不算', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '羽绒服', '短款', '羽绒服', '羽绒服', '推荐', '穿着', '还显', '羽绒服', 'rmoneybag', 'doubleexclamationmarkg', 'smilingfacewithhearts', 'cleanfit']\n",
      "filter_label: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '鸭绒', '这件', '棒球', '款式', '时尚', '保暖', 'smilingfacewithhearts', '厚度', '不算', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '短款', '推荐', '穿着', '还显']\n",
      "\n",
      "Example 4194:\n",
      "summary_cleaned: ['平价', '显瘦', '针织衫', '三十多', 'moneybag', '版型', '好好', '温柔', '氛围', '软乎乎', '不扎', '弹力', '起球', '针织衫', '显瘦', '平价', '好物', '梨形', '身材', '秋冬', '平价', '好物', '学生', 'moneybag']\n",
      "filter_label: ['平价', '显瘦', '针织衫', 'moneybag', '版型', '好好', '温柔', '氛围', '软乎乎', '不扎', '弹力', '好物', '梨形', '身材', '秋冬', '学生']\n",
      "\n",
      "Example 2920:\n",
      "summary_cleaned: ['宜家', '镜子', '好好看', '记录', '生活', '甜妹', '每日', '软妹', '宜家', '宜家', '拍照', '宜家', '好物']\n",
      "filter_label: ['镜子', '好好看', '记录', '生活', '甜妹', '每日', '软妹', '拍照', '好物']\n",
      "\n",
      "Example 3775:\n",
      "summary_cleaned: ['eplus', '姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', 'doubleexclamationmark', '浪漫生活', '记录', '笔记', '灵感', '服装', '测评', '套装', '平价', '衣服', '测评', '初秋', 'eplus', 'doubleexclamationmark']\n",
      "filter_label: ['eplus', '姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', 'doubleexclamationmark', '浪漫生活', '记录', '笔记', '灵感', '服装', '平价', '衣服']\n",
      "\n",
      "Example 4534:\n",
      "summary_cleaned: ['几十块', 'lulu', '同款', '外套', '白色', '筒裤', '太显', '简穿', '超会', '企划', 'lulu', '平替', 'ootd', '每日', '显瘦', 'ootd', '白色', '牛仔裤', '拍照', '姿势', '重样', 'lulu', '黑色', 'lulu', 'plus', 'lulu', 'ootd', 'ootd', 'lulu']\n",
      "filter_label: ['几十块', 'lulu', '同款', '外套', '白色', '筒裤', '太显', '简穿', '超会', '企划', '平替', 'ootd', '每日', '显瘦', '牛仔裤', '拍照', '姿势', '重样', '黑色', 'plus']\n",
      "\n",
      "Example 10565:\n",
      "summary_cleaned: ['Meetingyouistrulyamiracleofmylif', '六芒星', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '生命', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '手镯', '手工', '不可思议', '事儿', '银手镯', '手工', '银饰', '首饰', 'blingbling', '晒晒', '手上', '手工', '手镯', 'Meetingyouistrulyamiracleofmylif', 'Meetingyouistrulyamiracleofmylife', 'blingbling']\n",
      "filter_label: ['Meetingyouistrulyamiracleofmylif', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '不可思议', '事儿', '银手镯', '银饰', '首饰', 'blingbling', '晒晒', '手上']\n",
      "\n",
      "Example 7473:\n",
      "summary_cleaned: ['微胖', '妹妹', '我选', '简简单单', '耐看', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '不露', '美杏色', '连衣裙', '气质', '代表', '腰部', '褶皱', '肚子', '简单', '气质', '爸妈', '裙子', '安利', '第一张', '照片', '笔记', '灵感', '夏日', '宝藏', '裙子', '蓬蓬裙', '微胖', '显瘦', '显瘦', '连衣裙', '连衣裙', '气质', '连衣裙', 'H']\n",
      "filter_label: ['微胖', '妹妹', '我选', '简简单单', '耐看', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '不露', '美杏色', '气质', '腰部', '褶皱', '简单', '爸妈', '裙子', '安利', '第一张', '照片', '笔记', '灵感', '宝藏', '蓬蓬裙', '显瘦', 'H']\n",
      "\n",
      "Example 6619:\n",
      "summary_cleaned: ['cm', '美式', '复古', '运动', '女孩', '宽松', '紧身', '缺一不可', '小个子', '美式', '辣妹', '日常', '冬季', 'cm']\n",
      "filter_label: ['cm', '美式', '复古', '运动', '女孩', '宽松', '紧身', '缺一不可', '小个子', '辣妹', '日常', '冬季']\n",
      "\n",
      "Example 5295:\n",
      "summary_cleaned: ['教师', '梨形', '身材', '这条', '神裤', '身上', '衬衫', '裤子', '真的', '清爽', '耐看', '氧气', '十足', '梨形', '身材', '找到', '本命', '神裤', '腰围', '臀围', '腰围', '刚刚', '臀围', '放量', '很大', '真的', '超显', '教师', '梨形', '身材', '裤子', '白色', '裤子', 'yyds', '法式', '复古', '裤子', '气质', '搭薯', '队长', '管家', '潮流', '时尚', 'yyds']\n",
      "filter_label: ['教师', '梨形', '身材', '这条', '神裤', '身上', '衬衫', '裤子', '真的', '清爽', '耐看', '氧气', '十足', '找到', '本命', '腰围', '臀围', '刚刚', '放量', '超显', '白色', 'yyds', '法式', '复古', '气质', '搭薯', '队长', '潮流', '时尚']\n",
      "\n",
      "Example 8590:\n",
      "summary_cleaned: ['学生', '秋冬', '百搭', '长裤', 'jeans', '显高显', '显腿直', '神裤', 'keycapkeycapkeycapkeycapkeycapkeycap', '裤子', '长裤', '秋冬', '长裤', '百搭显', '神裤', '不露', '高个子', '裤子', '加绒', '裤子', 'jeans', 'keycapkeycapkeycapkeycapkeycapkeycap']\n",
      "filter_label: ['学生', '秋冬', '百搭', '长裤', 'jeans', '显高显', '显腿直', '神裤', 'keycapkeycapkeycapkeycapkeycapkeycap', '裤子', '百搭显', '不露', '高个子', '加绒']\n",
      "\n",
      "Example 14843:\n",
      "summary_cleaned: ['黑色', '每日']\n",
      "filter_label: ['黑色', '每日']\n",
      "\n",
      "Example 14141:\n",
      "summary_cleaned: ['小个子', '女生', '这套', 'COOLbutton', '今日', '快乐', '今日', '发起', '穿卫衣', '好开心', '穿卫衣', '搭配', '牛仔裤', '喜欢', '深色', '牛仔裤', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '日常', '小个子', '女生', 'ootd', '每日', '显瘦', '气质', '裤子', '女生', 'COOLbutton', 'ootd']\n",
      "filter_label: ['小个子', '女生', '这套', 'COOLbutton', '今日', '快乐', '发起', '穿卫衣', '好开心', '搭配', '牛仔裤', '喜欢', '深色', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '日常', 'ootd', '显瘦', '气质', '裤子']\n",
      "\n",
      "Example 11979:\n",
      "summary_cleaned: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '空空', '一身', '粉粉', '吊带', '低腰裤', '甜心', '辣妹', 'yk', '辣妹', '日常', 'yk']\n",
      "filter_label: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '空空', '一身', '粉粉', '吊带', '低腰裤', 'yk', '日常']\n",
      "\n",
      "Example 11515:\n",
      "summary_cleaned: ['carrot', '裙子', '喜欢', '安排', '做好', '样衣', '正式', '准确', '实物', '一张', '效果', '影响', '观看', '配色', 'Lolita', 'carrot', 'p', 'Lolita']\n",
      "filter_label: ['carrot', '裙子', '喜欢', '安排', '做好', '样衣', '正式', '准确', '实物', '一张', '效果', '影响', '观看', '配色', 'Lolita', 'p']\n",
      "\n",
      "Example 5370:\n",
      "summary_cleaned: ['贵气感', '源头', '血气方刚', '贵气感', '多贵', '身上', '那种', '战斗力', '身上', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '喜欢', '熬夜', '喝酒', '那阵子', '真的', '元气大伤', '一阵子', '气血', '真的', '感觉', '有用', '脸上', '皮肤', '透亮', '气血', '涌上来', '那种', '红润', '阿胶', '液体', '阿胶', '口服液', '听说', '液体', '更好', '吸收', '福站', '口服液', '一瓶', 'ml', '阿胶', '成分', 'ml', '剩下', '当归', '党参', '气血', '双补', '阳完', '真的', '感冒', '频繁', '福站', '阿胶', '口服液', '整年', '去年', '十月', '平时', '出去玩', '小小的', '一只', '放在', '包里', '没想到', '效果', 'ym', '不调', '姐妹', '想到', '天天', '喝酒', 'ym', '每次', '特别', '规律', '显得', '贵气', '气质', '场上', '方法', '简单', '缺什么', '精神状态', '喝酒', '微醺', '阿胶', '口服液', '拍照', '日常', '享受', '生活', '气血', '喝酒', '微醺', '氛围', '辣妹', 'ml', 'ml', 'ym', 'ym']\n",
      "filter_label: ['贵气感', '多贵', '身上', '那种', '战斗力', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '熬夜', '喝酒', '那阵子', '真的', '元气大伤', '一阵子', '气血', '感觉', '有用', '皮肤', '透亮', '涌上来', '听说', '更好', '吸收', '一瓶', 'ml', '成分', '党参', '双补', '感冒', '频繁', '整年', '去年', '平时', '出去玩', '小小的', '一只', '放在', '包里', '没想到', '效果', 'ym', '不调', '姐妹', '想到', '天天', '每次', '特别', '显得', '贵气', '方法', '简单', '微醺', '拍照', '日常', '享受', '生活', '氛围', '辣妹']\n",
      "\n",
      "Example 2141:\n",
      "summary_cleaned: ['我会', '香云纱', '改良', '汉服', '香云纱', '改良', '日常', '汉服', '拒绝', '繁琐', '一套', '出门', '香云纱', '香云纱', '新国潮', '中式', '改良', '汉服', '汉服', '汉服', '汉服', '日常', '中式']\n",
      "filter_label: ['我会', '香云纱', '改良', '汉服', '日常', '拒绝', '繁琐', '一套', '出门', '新国潮', '中式']\n",
      "\n",
      "Example 1164:\n",
      "summary_cleaned: ['山本', '裤后', '春夏', '神裤', '这肤', '舒服', '啊啊啊', '真的', '舒服', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '起球', '不易', '着实', '无可挑剔', '神裤', '啊啊啊', '感觉', '必大', '爆神裤', '裤子', '裤子', '种草', '裤子', '显瘦', '神裤', '粉色', '裤子', '小个子', '裤子', '梨型', '身材', '裤子', '百搭神裤', '夏季', '裤子', 'OotD', '每日', 'ootd', '每日', '春夏', '职场', '通勤', '日常', '显瘦', '裤子', '微胖', '女孩', '夏日', '休闲', '慵懒', '裤子', '裤子', '推荐', '工人', '神裤', '垂感', '裤子', '梨形', '身材', '裤子', '真的', 'OotD', 'ootd']\n",
      "filter_label: ['裤后', '春夏', '神裤', '这肤', '舒服', '啊啊啊', '真的', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '不易', '着实', '无可挑剔', '感觉', '爆神裤', '裤子', '种草', '显瘦', '粉色', '小个子', '梨型', '身材', '百搭神裤', '夏季', 'OotD', '每日', 'ootd', '职场', '通勤', '日常', '微胖', '女孩', '夏日', '休闲', '慵懒', '推荐', '工人', '垂感', '梨形']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: 随机采样30行\n",
    "sampled_df = df.sample(n=30, random_state=42)  # 随机采样30行\n",
    "\n",
    "# Step 3: 去重函数，保持顺序不变\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: 打印输出其中的 summary_cleaned 列和 filter_label 列，去掉 filter_label 中的重复单词\n",
    "for i, row in sampled_df.iterrows():\n",
    "    summary_cleaned = eval(row['tokenized_text'])  # 转换为列表\n",
    "    filter_label = eval(row['filtered_keywords2'])  # 转换为列表\n",
    "    filter_label_unique = remove_duplicates(filter_label)  # 去重\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"summary_cleaned: {summary_cleaned}\")\n",
    "    print(f\"filter_label: {filter_label_unique}\")\n",
    "    print()  # 空行用于美观输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4542:\n",
      "Original post_text: rU领修身短袖plus宽松牛仔裤太显瘦啦link上短下长遮胯显瘦裤子我的ootdOOTD穿搭分享时尚百搭超会穿企划ootd每日穿搭显瘦穿搭正肩t恤修身短袖\n",
      "Filtered keywords: ['rU', '修身', '短袖', '宽松', '牛仔裤', '太显', 'link', '下长', '显瘦', '裤子', 'ootdOOTD', '分享', '时尚', '百搭', '超会', '企划', 'ootd', '每日', 'plus', 't']\n",
      "\n",
      "Example 9801:\n",
      "Original post_text: 一眼心动有多少次都爱的鱼尾连衣裙dress这条人鱼尾连衣裙也太显身材了这个颜色也很绝一点也不艳俗下摆我真的超喜欢层层褶皱走起路来很美日常穿搭连衣裙显瘦连衣裙Ootd今天穿什么香连衣裙显瘦穿搭夏天穿裙子针织裙温柔穿搭我的日常吊带裙鱼尾裙每日穿搭笔记灵感\n",
      "Filtered keywords: ['一眼', '心动', '鱼尾', '连衣裙', 'dress', '这条', '太显', '身材', '颜色', '一点', '不艳俗', '下摆', '真的', '喜欢', '层层', '褶皱', '走起路来', '日常', '显瘦', 'Ootd', '夏天', '裙子', '针织', '温柔', '吊带裙', '每日', '笔记', '灵感']\n",
      "\n",
      "Example 11898:\n",
      "Original post_text: 好辣我说裙子好显身材这条感觉很适合过节穿哈哈哈日常穿搭每日穿搭我的日常穿搭\n",
      "Filtered keywords: ['好辣', '裙子', '好显', '身材', '感觉', '适合', '哈哈哈', '日常', '每日']\n",
      "\n",
      "Example 513:\n",
      "Original post_text: 纽约拥堵费省moneybag方案这算一劳永逸嘛omggrimacingface纽约成为全美首个推行拥堵费的城市而且一转眼就要开始正式实行了meltingfacecalendar开始时间月日起motorway收费路段进入曼岛中城街及以南就必须支付moneybag私家车价格kitchenknife每辆车每天仅收一次moneybag网约车kitchenknife次按次收费alarmclock时段yenbanknote周一至周五ampm高峰时段支付全价yenbanknote周六周日ampm高峰时段支付全价除高峰时段外拥堵费为全价的左右redexclamationmark优惠申请现已开放redexclamationmarkcheckmarkbutton去年个人收入低于w可以申请低收入优惠checkmarkbutton享受半价off优惠仅限私家车checkmarkbutton但是每个月的前次还是要付原价第次开始才有优惠而且只针对高峰时段pagewithcurl需提供材料checkmarkbutton个人税表checkmarkbutton如果属于某项福利需提供证明checkmarkbutton相关车辆信息申请时一定要有纽约EZPass账户sleepyface月日之前要多开车去几次曼哈顿去到就是赚到小伙伴们怎么看这个莫名其妙的fee万般滋味皆是生活笔记灵感曼哈顿纽约拥堵费纽约生活省钱攻略美国留学生活美国华人生活周末去哪每日资讯纽约吃喝玩乐\n",
      "Filtered keywords: ['拥堵', '费省', 'moneybag', '方案', '一劳永逸', 'omggrimacingface', '全美', '首个', '城市', '正式', '实行', 'meltingfacecalendar', '时间', 'motorway', '收费', '街及', '价格', 'kitchenknife', '仅收', 'alarmclock', 'yenbanknote', '周一', 'ampm', '高峰', '全价', '周日', '费为', 'redexclamationmark', '优惠', '开放', 'redexclamationmarkcheckmarkbutton', '去年', '个人收入', '低于', '低收入', 'checkmarkbutton', '享受', '半价', '仅限', '前次', '原价', 'pagewithcurl', '提供', '材料', '福利', '相关', '信息', 'EZPass', '账户', 'sleepyface', '小伙伴', '莫名其妙', 'fee', '万般', '滋味', '生活', '笔记', '灵感', '省钱', '攻略', '美国', '留学', '美国华人', '周末', '每日', '吃喝玩乐', 'w', 'off']\n",
      "\n",
      "Example 12162:\n",
      "Original post_text: 慵懒风穿搭and优雅自信彩虹一字肩毛衣希野逸儿气质穿搭针织衫毛衣一字肩针织毛衣\n",
      "Filtered keywords: ['慵懒', '风穿', '优雅', '自信', '彩虹', '一字', '毛衣', '希野逸儿', '气质', '针织衫', '针织', 'and']\n",
      "\n",
      "Example 5221:\n",
      "Original post_text: 教师穿搭紫色针织衫好温柔呐今天穿什么香教师穿搭气质穿搭日常穿搭ootd每日穿搭每日穿搭日常穿搭显瘦穿搭针织开衫跟着博主学穿搭薯队长薯管家潮流薯时尚薯\n",
      "Filtered keywords: ['教师', '紫色', '针织衫', '温柔', '气质', '日常', 'ootd', '每日', '显瘦', '针织', '开衫', '跟着', '博主学', '搭薯', '队长', '潮流', '时尚']\n",
      "\n",
      "Example 7685:\n",
      "Original post_text: 格子衬衫fallenleaf一套简单好看的初秋look今天穿什么香小个子穿搭Ootd格子控没衬衫怎么型每日穿搭显瘦穿搭\n",
      "Filtered keywords: ['格子', '衬衫', 'fallenleaf', '一套', '简单', '好看', '初秋', '小个子', 'Ootd', '控没', '每日', '显瘦', 'look']\n",
      "\n",
      "Example 8495:\n",
      "Original post_text: 如果穿搭不看脸你会不会多看我两眼穿搭Ootd每日穿搭日常穿搭ootd每日穿搭宝宝辅食\n",
      "Filtered keywords: ['两眼', 'Ootd', '每日', '日常', 'ootd', '宝宝', '辅食']\n",
      "\n",
      "Example 10337:\n",
      "Original post_text: r黑色短袖plus红色短裙红色好特别redcircle辣妹短裙短裙推荐我的ootd时尚穿搭分享ootd每日穿搭平价短袖黑色短袖穿搭红色短裙穿搭韩系穿搭\n",
      "Filtered keywords: ['黑色', '短袖', '红色', '短裙', '特别', 'redcircle', '辣妹', '推荐', 'ootd', '时尚', '分享', '每日', '平价', '韩系', 'r', 'plus']\n",
      "\n",
      "Example 10926:\n",
      "Original post_text: pearls裙子是thepeachofficial\n",
      "Filtered keywords: ['pearls', '裙子', 'thepeachofficial']\n",
      "\n",
      "Example 3418:\n",
      "Original post_text: 深圳国乙泰酷辣没想到在深圳国乙也贴到了好多帅老师呜呜呜呜呜呜好开心我贴贴贴贴贴贴贴贴贴贴贴贴贴贴就是全场一整天没有蹲到一个李泽言唉brokenheart深圳国乙同好会国乙only集邮\n",
      "Filtered keywords: ['深圳', '没想到', '好多', '老师', '呜呜', '好开心', '全场', '一整天', 'brokenheart', '集邮', 'only']\n",
      "\n",
      "Example 8833:\n",
      "Original post_text: Japancocoshnik日本小众饰品合集限时折扣因为踢球赢了商场限时折扣日本的折扣日期calendar叫人摸不透每次都是奇妙的理由去日本买什么COCOSHNIK日系cocoshnik饰品分享首饰就要bulingbuling宝藏饰品大公开\n",
      "Filtered keywords: ['Japancocoshnik', '日本', '饰品', '合集', '限时', '折扣', '日期', 'calendar', '摸不透', '每次', '奇妙', 'COCOSHNIK', '日系', 'cocoshnik', '分享', '首饰', 'bulingbuling', '宝藏', '公开']\n",
      "\n",
      "Example 5089:\n",
      "Original post_text: 今天有点冷我的日常穿搭每日分享每日穿搭秋冬西装外套秋冬毛衣\n",
      "Filtered keywords: ['日常', '每日', '分享', '秋冬', '西装', '外套', '毛衣']\n",
      "\n",
      "Example 8810:\n",
      "Original post_text: Japan日本小众品牌Christmastree限定款彩色宝石戒指每一颗都是独一无二的美去日本买什么彩色宝石彩宝戒指天然石小众戒指首饰就要blingbling宝藏首饰大公开\n",
      "Filtered keywords: ['Japan', '日本', '品牌', 'Christmastree', '限定', '彩色', '宝石', '戒指', '一颗', '独一无二', '美去', '彩宝', '天然', '首饰', 'blingbling', '宝藏', '公开']\n",
      "\n",
      "Example 6191:\n",
      "Original post_text: 今天好酷GENTLEMONSTER去上海玩了一天打卡了这个新展还挺好拍的哈哈空间很大很特别每日穿搭日常穿搭我的日常日常穿搭\n",
      "Filtered keywords: ['好酷', 'GENTLEMONSTER', '上海', '打卡', '新展', '空间', '特别', '每日', '日常']\n",
      "\n",
      "Example 42:\n",
      "Original post_text: 我的哭喊小马甲和裙子还蛮配呢哈哈哈真的很爱叠穿就是夏天也喜欢大学生穿搭夏季短袖裙子马甲马甲搭配叠穿复古穿搭小个子穿搭春夏穿搭甜妹穿搭今天穿搭上学穿什么\n",
      "Filtered keywords: ['马甲', '裙子', '蛮配', '哈哈哈', '真的', '很爱叠', '夏天', '喜欢', '大学生', '夏季', '短袖', '搭配', '复古', '小个子', '春夏', '甜妹', '上学']\n",
      "\n",
      "Example 15568:\n",
      "Original post_text: 有些人表面购物分享实际给大家wrappedgift开场白mushroom手机壳mushroom抱枕套米奇的鞋子奶噗噗花瓶秋冬配饰们海绵宝宝盲盒半身裙wrappedgiftsofticecream毛绒玩具wrappedgift小酒杯们growingheart地毯购物分享奶噗噗花瓶RPDCLaVie我的盲盒分享我的开箱日常\n",
      "Filtered keywords: ['表面', '购物', '分享', 'wrappedgift', 'mushroom', '手机', '枕套', '米奇', '鞋子', '花瓶', '秋冬', '配饰', '宝宝', '盲盒', '半身裙', 'wrappedgiftsofticecream', '毛绒玩具', '酒杯', 'growingheart', '地毯', 'RPDCLaVie', '开箱', '日常']\n",
      "\n",
      "Example 2872:\n",
      "Original post_text: 优衣库新品衬衫尊嘟巨好看美美拿下坐等天凉秋天第张照片没衬衫怎么型优衣库衬衫优衣库爆款日记初秋衬衫优衣库试衣间优衣库新款格子衬衫\n",
      "Filtered keywords: ['优衣库', '新品', '衬衫', '尊嘟', '好看', '美美', '拿下', '坐等', '天凉', '秋天', '照片', '爆款', '日记', '初秋', '试衣间', '新款', '格子']\n",
      "\n",
      "Example 8322:\n",
      "Original post_text: 小个子女生今天穿这套COOLbutton年度幸福感好物今天穿针织衫搭配牛仔裤这条牛仔裤垂感挺好的穿久了也没有鼓包搭配针织衫刚好一套日常穿搭每日穿搭ootd每日穿搭周末去哪儿\n",
      "Filtered keywords: ['小个子', '女生', '这套', 'COOLbutton', '年度', '幸福感', '好物', '针织衫', '搭配', '牛仔裤', '垂感', '鼓包', '一套', '日常', '每日', 'ootd', '周末']\n",
      "\n",
      "Example 10042:\n",
      "Original post_text: 这双高跟玛丽珍绝了doubleexclamationmark巨增高显腿细真的好稳好好穿真的好想焊在脚上啊啊啊鞋控の日常玛丽珍增高厚底鞋小皮鞋神仙鞋子尝试一个新look最百搭的鞋我的平价好物\n",
      "Filtered keywords: ['双高', '玛丽', '珍绝', 'doubleexclamationmark', '增高', '显腿细', '真的', '好稳', '好好', '啊啊啊', '鞋控', '日常', '厚底', '皮鞋', '神仙', '鞋子', '尝试', '最百搭', '平价', '好物', 'look']\n",
      "\n",
      "Example 8648:\n",
      "Original post_text: 可爱补给浪漫生活的记录者笔记灵感看见其他姐妹都是用白色假发搭配的不过我基本上从不买整定的假发我带着头疼lolita日常lolita穿搭沉睡梦境lolita洋装\n",
      "Filtered keywords: ['可爱', '补给', '浪漫生活', '记录', '笔记', '灵感', '姐妹', '白色', '假发', '搭配', '整定', '头疼', 'lolita', '日常', '沉睡', '梦境', '洋装']\n",
      "\n",
      "Example 9288:\n",
      "Original post_text: 真实的测评希望被大家看到啊doubleexclamationmarkdoubleexclamationmark面料很舒服的一件吊带颜色特别适合夏天更适合大bear姐妹有运险今天穿什么香度假穿什么慵懒感穿搭吊带背心平价穿搭自带胸垫吊带平价吊带\n",
      "Filtered keywords: ['真实', '测评', '希望', 'doubleexclamationmarkdoubleexclamationmark', '面料', '舒服', '一件', '吊带', '颜色', '特别', '适合', '夏天', 'bear', '姐妹', '度假', '慵懒', '感穿', '背心', '平价', '自带', '胸垫']\n",
      "\n",
      "Example 12723:\n",
      "Original post_text: 今天的幸运色是lemoncozy的一天我说水逆你赶紧退散色彩搭配我的日常给你日常的穿搭参考\n",
      "Filtered keywords: ['幸运色', 'lemoncozy', '水逆', '赶紧', '退散', '色彩', '搭配', '日常', '参考']\n",
      "\n",
      "Example 13198:\n",
      "Original post_text: 微胖胯宽妹妹可以相信我选的简简单单耐看H型身材大骨架秋天的衣服太漂亮啦棕色短款皮衣泰酷了吧搭配牛仔裤很显比例早秋日常实用性穿搭显瘦显比例我的面试穿搭笔记灵感显瘦穿搭微胖穿搭日常穿搭皮衣短款皮衣遮胯牛仔裤秋天穿什么\n",
      "Filtered keywords: ['微胖', '妹妹', '我选', '简简单单', '耐看', '身材', '骨架', '秋天', '衣服', '太漂亮', '棕色', '短款', '皮衣', '泰酷', '搭配', '牛仔裤', '很显', '比例', '早秋', '日常', '实用性', '显瘦显', '面试', '笔记', '灵感', '显瘦', 'H']\n",
      "\n",
      "Example 8744:\n",
      "Original post_text: 琥珀心就是优雅大小姐吧日常碎片PLOG笔记灵感玻璃花信琥珀心今日穿搭吃我一波Lolita安利\n",
      "Filtered keywords: ['优雅', '小姐', '日常', '碎片', 'PLOG', '笔记', '灵感', '玻璃花', '今日', '一波', 'Lolita', '安利']\n",
      "\n",
      "Example 12479:\n",
      "Original post_text: cm斤腿粗姐妹也能拿捏一定要小细腿吗蜜大腿行不行小个子穿搭美式穿搭穿搭辣妹穿搭\n",
      "Filtered keywords: ['cm', '姐妹', '拿捏', '要小细', '大腿', '行不行', '小个子', '美式', '辣妹']\n",
      "\n",
      "Example 12581:\n",
      "Original post_text: 姐妹们这个plus的上衣真的美疯这么搭配好辣好御今天穿什么香高腰牛仔裤显瘦牛仔裤平价好穿牛仔裤短款上衣高个子女生穿搭御姐穿搭高个子牛仔裤加长牛仔裤秋冬穿搭高个子加长裤子\n",
      "Filtered keywords: ['姐妹', '上衣', '真的', '美疯', '搭配', '好辣', '香高腰', '牛仔裤', '显瘦', '平价', '短款', '高个子', '女生', '御姐', '加长', '秋冬', '裤子', 'plus']\n",
      "\n",
      "Example 13551:\n",
      "Original post_text: redapple型身材救命喇叭裤就应该这样搭瘦高绝身高cm体重斤斤小腿围cm大腿cm腰围cm肚围cmredapple型身材每一套我都很喜欢喇叭裤就这样穿哈哈哈哈哈哈苹果型身材苹果型小个子穿搭微喇叭裤喇叭裤牛仔喇叭裤喇叭裤穿搭一件衣服多种穿搭一衣多穿一衣多穿秋牛仔微喇叭裤显瘦小个子喇叭裤苹果型身材穿搭小个子苹果型身材苹果型身材穿搭指南苹果型身材显瘦穿搭苹果型身材微胖穿搭苹果型身材穿搭技巧\n",
      "Filtered keywords: ['redapple', '身材', '救命', '喇叭裤', '瘦高', '身高', 'cm', '体重', '斤斤', '小腿', '大腿', '腰围', '肚围', 'cmredapple', '一套', '喜欢', '哈哈哈', '苹果', '小个子', '搭微', '牛仔', '一件', '衣服', '多种', '一衣', '显瘦', '指南', '微胖', '技巧']\n",
      "\n",
      "Example 6840:\n",
      "Original post_text: 以上的姐妹在哪里好辣好辣裙子有长短两个长度可以选开衩不会走光今天穿什么香国风旗袍改良旗袍高个子女生穿搭高个子女生好看的裙子神仙裙子蝴蝶吊带裙韩系穿搭御姐穿搭吊带裙蝴蝶裙子高个子吊带裙\n",
      "Filtered keywords: ['姐妹', '好辣', '裙子', '长短', '两个', '开衩', '国风', '旗袍', '改良', '高个子', '女生', '好看', '神仙', '蝴蝶', '吊带裙', '韩系', '御姐']\n",
      "\n",
      "Example 5558:\n",
      "Original post_text: outfitpawprints好高级的一套看展穿搭butterfly今天是优雅小fish正经不过一秒最后一张才是我本人小众穿搭设计师品牌暗黑系elywood小众设计师品牌初秋穿搭高级感穿搭长裙看展穿搭侘寂美学神仙裙子小众设计感裙子推荐\n",
      "Filtered keywords: ['outfitpawprints', '高级', '一套', '看展', 'butterfly', '优雅', 'fish', '正经', '一秒', '一张', '小众', '设计师', '品牌', '暗黑', 'elywood', '初秋', '感穿', '长裙', '美学', '神仙', '裙子', '设计', '推荐']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: Sample 30 rows randomly\n",
    "# sampled_df = df.sample(n=30, random_state=42)\n",
    "sampled_df = df.sample(n=30)\n",
    "\n",
    "# Step 3: Function to remove duplicates while maintaining order\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: Iterate over sampled rows and process the content\n",
    "for i, row in sampled_df.iterrows():\n",
    "    try:\n",
    "        # Keep the original cleaned_post_text as is for display\n",
    "        summary_cleaned_original = row['cleaned_post_text']\n",
    "        \n",
    "        # Attempt to safely convert filtered_keywords to list, if possible\n",
    "        if isinstance(row['filtered_keywords2'], str):\n",
    "            try:\n",
    "                filter_label = ast.literal_eval(row['filtered_keywords2'])  # Safely convert to list\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Treat it as a simple string if not a valid list\n",
    "                filter_label = row['filtered_keywords2'].split()\n",
    "        else:\n",
    "            filter_label = row['filtered_keywords2']\n",
    "    \n",
    "    except (ValueError, SyntaxError):\n",
    "        # Log problematic rows\n",
    "        print(f\"Error parsing row {i}, skipping this row.\")\n",
    "        continue\n",
    "    \n",
    "    # Remove duplicates from filtered_keywords\n",
    "    filter_label_unique = remove_duplicates(filter_label)\n",
    "    \n",
    "    # Output the original text and processed keywords\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Original post_text: {summary_cleaned_original}\")\n",
    "    print(f\"Filtered keywords: {filter_label_unique}\")\n",
    "    print()  # Empty line for better output formatting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
