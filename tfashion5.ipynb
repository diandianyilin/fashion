{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion-related samples (positive): 52242\n",
      "Number of non-fashion-related samples (negative): 304611\n",
      "The datasets are not balanced. Difference: 252369\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load fashion-related data (positive samples)\n",
    "fashion_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword' column which contains fashion-related keywords\n",
    "fashion_samples = fashion_df['keyword group']\n",
    "\n",
    "# Directory containing non-fashion-related samples\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "\n",
    "# Initialize an empty list to store non-fashion samples\n",
    "non_fashion_samples = []\n",
    "\n",
    "# Recursively iterate through all subdirectories and files in non_fashion_texts directory\n",
    "for root, dirs, files in os.walk(neg_dir):\n",
    "    for file in files:\n",
    "        # Read only text files\n",
    "        if file.startswith('wiki'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Add each line as a sample\n",
    "                lines = f.readlines()\n",
    "                non_fashion_samples.extend([line.strip() for line in lines if line.strip()])\n",
    "\n",
    "# Convert non-fashion samples to a pandas Series for consistency\n",
    "non_fashion_samples = pd.Series(non_fashion_samples)\n",
    "\n",
    "# Step 2: Count the number of samples in each dataset\n",
    "num_fashion_samples = fashion_samples.shape[0]\n",
    "num_non_fashion_samples = non_fashion_samples.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of fashion-related samples (positive): {num_fashion_samples}\")\n",
    "print(f\"Number of non-fashion-related samples (negative): {num_non_fashion_samples}\")\n",
    "\n",
    "# Step 3: Check if they are balanced\n",
    "if num_fashion_samples == num_non_fashion_samples:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(f\"The datasets are not balanced. Difference: {abs(num_fashion_samples - num_non_fashion_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling:\n",
    "- randomly reducing the number of samples from the majority class (non-fashion-related samples) to match the number of samples in the minority class (fashion-related samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downsampled non-fashion-related samples: 52242\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# If the non-fashion dataset is larger, downsample it\n",
    "if num_non_fashion_samples > num_fashion_samples:\n",
    "    # Randomly select `num_fashion_samples` non-fashion samples\n",
    "    downsampled_non_fashion_samples = non_fashion_samples.sample(n=num_fashion_samples, random_state=42)\n",
    "else:\n",
    "    downsampled_non_fashion_samples = non_fashion_samples\n",
    "\n",
    "# Print the number of samples after downsampling\n",
    "print(f\"Number of downsampled non-fashion-related samples: {downsampled_non_fashion_samples.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read Data Files\n",
    "- read positive samples\n",
    "- read negative samples\n",
    "- combine the datasets\n",
    "#### Step 2: Load pre-trained sentence Transformer Model\n",
    "- load text2vec-large-chinese model\n",
    "- generate embeddings for each word\n",
    "#### Step 3: Get embeddings for the dataset\n",
    "- apply the embeddings function\n",
    "- convert labels to tensor\n",
    "#### Step 4: Train-test split\n",
    "- split the data\n",
    "- create PyTorh TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (356852, 2)\n",
      "Data Format Example:\n",
      "      word  label\n",
      "0     å¸‚æ”¿åºœ.      0\n",
      "1   åœ°ç†ä¸æ°”å€™.      0\n",
      "2  ç¶  (æ¶ˆæ­§ç¾©)      0\n",
      "3      é‚¯å±±åŒº      0\n",
      "4     æ±Ÿæˆ¸å¹•åºœ      0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11152/11152 [15:32<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([356852, 1024])\n",
      "Embedding Example:\n",
      "tensor([ 0.9775, -0.6769, -0.7114,  ..., -0.5798,  1.1212, -2.3075],\n",
      "       device='cuda:0')\n",
      "Labels Shape: torch.Size([356852])\n",
      "Labels Example:\n",
      "tensor([0, 0, 0, 0, 0])\n",
      "Train Embeddings Shape: torch.Size([285481, 1024]), Train Labels Shape: torch.Size([285481])\n",
      "Val Embeddings Shape: torch.Size([71371, 1024]), Val Labels Shape: torch.Size([71371])\n",
      "Train Loader Batch Size: 8922 batches\n",
      "Val Loader Batch Size: 2231 batches\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Read Positive Samples from 'topics_filtered.csv'\n",
    "# Load the fashion lexicon (positive samples)\n",
    "pos_file = \"/home/disk1/red_disk1/fashion/topics_filtered.csv\"\n",
    "positive_samples_df = pd.read_csv(pos_file)\n",
    "positive_samples = positive_samples_df['keyword group'].dropna().tolist()  # Extract keywords column\n",
    "\n",
    "# Assign label 1 to positive samples\n",
    "positive_samples = pd.DataFrame(positive_samples, columns=['word'])\n",
    "positive_samples['label'] = 1\n",
    "\n",
    "# Step 2: Read Negative Samples from non_fashion_texts\n",
    "def read_negative_samples(directory):\n",
    "    all_lines = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    all_lines.extend([line.strip() for line in file if line.strip() != \"\"])\n",
    "    return all_lines\n",
    "\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "negative_samples_list = read_negative_samples(neg_dir)\n",
    "\n",
    "# Assign label 0 to negative samples\n",
    "negative_samples = pd.DataFrame(negative_samples_list, columns=['word'])\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "# Step 3: Combine Positive and Negative Samples\n",
    "data = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shape and format of the data\n",
    "print(f\"Combined Data Shape: {data.shape}\")\n",
    "print(f\"Data Format Example:\\n{data.head()}\")\n",
    "\n",
    "# Step 4: Load text2vec-large-chinese model\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Embedding Progress\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 5: Get embeddings for the dataset\n",
    "embeddings = get_text2vec_embeddings(data['word'], text2vec_model)\n",
    "\n",
    "# Print the shape and format of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding Example:\\n{embeddings[0]}\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Print the labels format\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels Example:\\n{labels[:5]}\")\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the split data shapes\n",
    "print(f\"Train Embeddings Shape: {train_embeddings.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Embeddings Shape: {val_embeddings.shape}, Val Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print the data loader batch size\n",
    "print(f\"Train Loader Batch Size: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader Batch Size: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a RedNet-based binary classification model\n",
    "1. Model Definition: ResNet-Based Binary Classifier\n",
    "2. Training Loop\n",
    "3. Validation Loop\n",
    "4. Training and Evaluation\n",
    "\n",
    "#### Summary\n",
    "- Model Setup: It initializes a ResNet18 model tailored for binary classification using text embeddings as inputs.\n",
    "- Training and Validation: The code implements a standard training loop with backpropagation and uses validation to track performance.\n",
    "- Performance Metrics: Accuracy, precision, recall, and F1 score are calculated to evaluate the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8922/8922 [03:50<00:00, 38.66it/s]\n",
      "Validation Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2231/2231 [00:06<00:00, 345.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0922, Val Loss: 0.0618, Accuracy: 0.9754\n",
      "Precision: 0.9172, Recall: 0.9143, F1 Score: 0.9158\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8922/8922 [03:50<00:00, 38.64it/s]\n",
      "Validation Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2231/2231 [00:06<00:00, 355.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0546, Val Loss: 0.0586, Accuracy: 0.9775\n",
      "Precision: 0.9595, Recall: 0.8829, F1 Score: 0.9196\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8922/8922 [03:49<00:00, 38.82it/s]\n",
      "Validation Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2231/2231 [00:05<00:00, 408.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411, Val Loss: 0.0471, Accuracy: 0.9813\n",
      "Precision: 0.9461, Recall: 0.9245, F1 Score: 0.9352\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8922/8922 [03:49<00:00, 38.86it/s]\n",
      "Validation Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2231/2231 [00:06<00:00, 354.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0321, Val Loss: 0.0481, Accuracy: 0.9826\n",
      "Precision: 0.9419, Recall: 0.9390, F1 Score: 0.9404\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8922/8922 [03:50<00:00, 38.74it/s]\n",
      "Validation Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2231/2231 [00:05<00:00, 400.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0262, Val Loss: 0.0526, Accuracy: 0.9829\n",
      "Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2231/2231 [00:05<00:00, 434.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: Accuracy: 0.9829, Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 7: Define ResNet-Based Binary Classification Model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):  # Adjust embedding_dim based on text2vec output (1024)\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        # Use pre-trained ResNet18, but replace the first conv layer to accept text2vec embeddings\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Binary classification\n",
    "        \n",
    "        # Define a simple MLP layer to map embeddings to ResNet input size\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),  # Match the expected ResNet input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),  # Match the expected ResNet input size after projection\n",
    "        )\n",
    "        \n",
    "        # Adding a Conv layer to convert 1D to a 2D feature map\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "\n",
    "        # Adaptive pooling layer to transform the spatial dimensions to (7, 7)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)  # Project text2vec embeddings to match ResNet input size\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # Reshape to fit Conv2D (batch_size, channels, height, width)\n",
    "        x = self.embedding_conv(x)  # Use conv layer to convert 1D feature to 2D feature map\n",
    "        x = self.adaptive_pool(x)  # Apply adaptive pooling to resize to (7, 7)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct embedding dimension (1024 as per the output shape)\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "\n",
    "# Step 8: Define Training Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Progress\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Progress\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "# Step 9: Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 10: Final Evaluation\n",
    "val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "print(f\"Final Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /home/disk1/red_disk1/fashion/tfashion\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Save the trained model\n",
    "\n",
    "# Save model and tokenizer to the desired directory\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion\"\n",
    "\n",
    "# Make the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, \"tfashion.pth\"))\n",
    "\n",
    "# Optionally, save additional model metadata if needed (e.g., optimizer state, epoch)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(model_save_path, \"tfashion_checkpoint.pth\"))\n",
    "\n",
    "print(f\"Model saved at: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare post data\n",
    "- Combine 'post_title' and 'post_content' into 'post_text'\n",
    "- Clean 'post_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean.csv')\n",
    "# df = pd.read_csv('/home/disk1/red_disk1/poster_9305.csv')\n",
    "\n",
    "# Combine 'post_title' and 'post_content' into 'post_text'\n",
    "df['post_text'] = df['post_title'].fillna('') + ' ' + df['post_content'].fillna('')\n",
    "\n",
    "# Drop 'post_title' and 'post_content' columns\n",
    "df = df.drop(columns=['post_title', 'post_content', 'post_tag'])\n",
    "\n",
    "# Save the updated DataFrame\n",
    "# df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_9305_combined.csv', index=False)\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "# Load stopwords from the provided file\n",
    "with open('/home/disk1/red_disk1/fashion/stopwords_cn.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Function for text cleaning\n",
    "def clean_text(text, stopwords):\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove specific patterns\n",
    "    text = re.sub(r'- å°çº¢ä¹¦,,', '', text)  # Removing \"- å°çº¢ä¹¦,,\"\n",
    "    text = re.sub(r'å°çº¢ä¹¦', '', text)  # Explicitly remove \"å°çº¢ä¹¦\"\n",
    "    text = re.sub(r',,\\d{2}-\\d{2},,', '', text)  # Removing patterns like \",,XX-XX,,\"\n",
    "    text = re.sub(r'#', ' ', text)  # Replace '#' with a space\n",
    "    text = re.sub(r'\\s+', '', text)  # This will remove all whitespace characters\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    # Remove stopwords (word-based removal)\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopwords])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function to 'post_text'\n",
    "df['cleaned_post_text'] = df['post_text'].apply(lambda x: clean_text(str(x), stopwords))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/post_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model tfashion\n",
    "- Step 1: Load the test dataset.\n",
    "- Step 2: Generate embeddings for each word in the test dataset using text2vec-large-chinese.\n",
    "- Step 3: Use a pre-trained ResNet-based classifier model to predict whether each word is related to fashion or not.\n",
    "- Step 4: Save the predictions alongside the original data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 489/489 [02:17<00:00,  3.57it/s]\n",
      "/tmp/ipykernel_267914/2888443501.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
      "Testing Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 489/489 [00:01<00:00, 476.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/disk1/red_disk1/fashion/test-output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"  # Update this path if necessary\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['cleaned_post_text']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained tfashion model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained tfashion model from your specified path\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test-output.csv\"  \n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "1. Tokenization: tokenize the post_text content\n",
    "2. Word Embedding: pass each word (instead of the entire post) through the text2vec-large-chinese model to generate embeddings for each word\n",
    "3. Classification: pass embeddings through the tfashion model to classify whether each word is fashion-related or not\n",
    "4. Filter out any words that are present in the stopwords set before applying the model classification\n",
    "5. Filter out single-character words\n",
    "5. Results: The output will be a classification for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_267914/336921134.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "Processing Rows:  30%|â–ˆâ–ˆâ–ˆ       | 4764/15622 [02:21<03:48, 47.48it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Step 1: Load the stopwords file\n",
    "stopwords_file = \"/home/disk1/red_disk1/fashion/stopwords_cn.txt\"\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "# Step 2: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 3: Define ResNetBinaryClassifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Load the CSV file containing post_text\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 5: Tokenize the post_text content into words and remove stopwords\n",
    "def tokenize_text(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    # Filter out stopwords and single-character words\n",
    "    return [word for word in words if word not in stopwords and len(word) > 1]\n",
    "\n",
    "df['tokenized_text'] = df['cleaned_post_text'].apply(tokenize_text)\n",
    "\n",
    "# Step 6: Get embeddings for each word in the tokenized_text column and classify them\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    words = row['tokenized_text']\n",
    "    \n",
    "    # Get embeddings for the words\n",
    "    embeddings = get_text2vec_embeddings(words, text2vec_model).to(device)\n",
    "    \n",
    "    # Make predictions using the pre-trained ResNet binary classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Filter words where the model predicts label 1 (fashion-related)\n",
    "    filtered_words = [word for word, label in zip(words, preds) if label == 1]\n",
    "    filtered_keywords.append(filtered_words)\n",
    "\n",
    "# Step 7: Add the new column filtered_keywords and save the DataFrame\n",
    "df['filtered_keywords'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 12997:\n",
      "summary_cleaned: ['å°Šå˜Ÿ', 'çˆ±', 'redexclamationmark', 'è¯´', 'å¯Œè´µ', 'å°å§å§', 'ç©¿', 'æ­', 'å°è¯•', 'æ–°', 'æ¸©æŸ”', 'æ°”è´¨', 'å§å§', 'å¯Œå®¶', 'åƒé‡‘', 'ç©¿', 'æ­', 'ç§‹å†¬', 'æ¸©æŸ”', 'æ…µæ‡’', 'é£', 'æ¯›è¡£', 'æ…µæ‡’', 'é£ç©¿', 'æ­', 'é«˜çº§', 'æ„Ÿç©¿', 'æ­']\n",
      "filter_label: ['çˆ±', 'è¯´', 'å¯Œè´µ', 'å°å§å§', 'ç©¿', 'æ­', 'å°è¯•', 'æ–°', 'æ¸©æŸ”', 'æ°”è´¨', 'å§å§', 'å¯Œå®¶', 'åƒé‡‘', 'ç§‹å†¬', 'æ…µæ‡’', 'é£', 'æ¯›è¡£', 'é£ç©¿', 'é«˜çº§', 'æ„Ÿç©¿']\n",
      "\n",
      "Example 11585:\n",
      "summary_cleaned: ['çˆ±', 'æ¸©æŸ”', 'ç™½æœˆå…‰', 'æ„Ÿè§‰', 'è¯•è¯•', 'ä»™æ°”', 'è£™å­', 'æ–°', 'ä¸­å¼', 'ç©¿', 'æ­', 'æ°‘å›½', 'é£', 'æ¸©æŸ”', 'è¿è¡£è£™', 'å›½é£', 'é’ˆç»‡', 'æ–°', 'ä¸­å¼', 'å¥—è£…']\n",
      "filter_label: ['çˆ±', 'æ¸©æŸ”', 'ç™½æœˆå…‰', 'æ„Ÿè§‰', 'è¯•è¯•', 'ä»™æ°”', 'è£™å­', 'æ–°', 'ä¸­å¼', 'ç©¿', 'æ­', 'æ°‘å›½', 'é£', 'è¿è¡£è£™', 'å›½é£', 'é’ˆç»‡', 'å¥—è£…']\n",
      "\n",
      "Example 11226:\n",
      "summary_cleaned: ['æˆ‘å…ˆ', 'å¾®èƒ–', 'å¥³å­©', 'å¾®èƒ–', 'ç©¿', 'æ­']\n",
      "filter_label: ['å¾®èƒ–', 'å¥³å­©', 'ç©¿', 'æ­']\n",
      "\n",
      "Example 13552:\n",
      "summary_cleaned: ['redapple', 'å‹', 'è·Ÿç€', 'æ¨¡ç‰¹', 'ç©¿', 'keycapkeycap', 'å¤å¤', 'ç™¾è¤¶è£™', 'å¤ªå¥½', 'ç©¿', 'èº«é«˜', 'cm', 'ä½“é‡', 'æ–¤æ–¤', 'å°è…¿', 'å›´', 'cm', 'å¤§è…¿', 'cm', 'è…°å›´', 'cm', 'è‚šå›´', 'cmredapple', 'å‹', 'èº«æ', 'æˆ‘å¤ªçˆ±', 'è¿™ä»¶', 'çŸ­è£™', 'å‰', 'å‡ å¹´', 'å°è¯•', 'ç™¾è¤¶è£™', 'æ²¡æƒ³åˆ°', 'ç©¿', 'æ„Ÿè§‰', 'ä¸é”™', 'å“ˆå“ˆå“ˆ', 'æè´¨', 'åš', 'æè´¨', 'è´¨æ„Ÿ', 'è‹¹æœ', 'å‹', 'èº«æ', 'è‹¹æœ', 'å‹', 'å°ä¸ªå­', 'ç©¿', 'æ­', 'çŸ­è£™', 'ç©¿', 'æ­', 'çŸ­è£™', 'æ ¼å­è£™', 'ç™¾è¤¶è£™', 'ç™¾è¤¶è£™', 'æ­', 'ç™¾è¤¶è£™', 'ç©¿', 'æ­', 'æ ¼å­', 'ç™¾è¤¶è£™', 'å¤å¤', 'ç™¾è¤¶è£™', 'å¤å¤', 'æ ¼å­', 'åŠè£™', 'ç§‹å­£', 'ç©¿', 'æ­', 'ç§‹å­£', 'æ–°æ¬¾', 'è·Ÿç€', 'æ¨¡ç‰¹', 'å­¦ç©¿', 'æ­', 'ç¾', 'æ‹‰å¾·', 'ç©¿', 'æ­', 'è‹¹æœ', 'å‹', 'èº«æ', 'ç©¿', 'æ­', 'å°ä¸ªå­', 'è‹¹æœ', 'å‹', 'èº«æ', 'è‹¹æœ', 'å‹', 'èº«æ', 'ç©¿', 'æ­', 'æŒ‡å—', 'è‹¹æœ', 'å‹', 'èº«æ', 'æ˜¾ç˜¦', 'ç©¿', 'æ­', 'è‹¹æœ', 'å‹', 'èº«æ', 'å¾®èƒ–', 'ç©¿', 'æ­å¾®', 'è‹¹æœ', 'å‹', 'èº«æ', 'é’ˆç»‡', 'ä¸Šè¡£', 'èˆ’æœ', 'é’ˆç»‡è¡«', 'æ­ç™¾æ­', 'é’ˆç»‡', 'ä¸Šè¡£', 'æ—©ç§‹', 'å¿…å¤‡', 'é’ˆç»‡è¡«']\n",
      "filter_label: ['redapple', 'å‹', 'è·Ÿç€', 'æ¨¡ç‰¹', 'ç©¿', 'å¤å¤', 'ç™¾è¤¶è£™', 'å¤ªå¥½', 'èº«é«˜', 'ä½“é‡', 'æ–¤æ–¤', 'å°è…¿', 'å›´', 'å¤§è…¿', 'è…°å›´', 'è‚šå›´', 'èº«æ', 'æˆ‘å¤ªçˆ±', 'è¿™ä»¶', 'çŸ­è£™', 'å‰', 'å‡ å¹´', 'å°è¯•', 'æ²¡æƒ³åˆ°', 'æ„Ÿè§‰', 'ä¸é”™', 'å“ˆå“ˆå“ˆ', 'æè´¨', 'åš', 'è´¨æ„Ÿ', 'è‹¹æœ', 'å°ä¸ªå­', 'æ­', 'æ ¼å­è£™', 'æ ¼å­', 'åŠè£™', 'ç§‹å­£', 'æ–°æ¬¾', 'å­¦ç©¿', 'ç¾', 'æ‹‰å¾·', 'æŒ‡å—', 'æ˜¾ç˜¦', 'å¾®èƒ–', 'æ­å¾®', 'é’ˆç»‡', 'ä¸Šè¡£', 'èˆ’æœ', 'é’ˆç»‡è¡«', 'æ­ç™¾æ­', 'æ—©ç§‹', 'å¿…å¤‡']\n",
      "\n",
      "Example 100:\n",
      "summary_cleaned: ['è¿™ä»¶', 'æ¯›è¡£', 'æ­çº¢', 'å›´å·¾', 'çœŸçš„', 'æ°›å›´', 'æ„Ÿç»', 'çœŸçš„', 'å¤ª', 'å¥½çœ‹', 'è´¨æ„Ÿ', 'è¶…æ£’', 'è½¯è½¯', 'ç³¯', 'ç³¯', 'å¤–é¢', 'æ­ä¸ª', 'å¤§è¡£', 'å¦¥å¦¥', 'éŸ©å‰§', 'å¥³ä¸»', 'ç–¯ç‹‚', 'çˆ±ä¸Š', 'å¤§å­¦ç”Ÿ', 'ç©¿', 'æ­', 'å¤§è¡£', 'ç©¿', 'æ­', 'æ¯›è¡£', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'æ°”è´¨', 'ç©¿', 'æ­', 'ç§‹å†¬', 'ç©¿', 'æ­', 'å°ä¸ªå­', 'ç©¿', 'æ­', 'ç§‹å†¬', 'æ¯›è¡£']\n",
      "filter_label: ['è¿™ä»¶', 'æ¯›è¡£', 'æ­çº¢', 'å›´å·¾', 'çœŸçš„', 'æ°›å›´', 'æ„Ÿç»', 'å¤ª', 'å¥½çœ‹', 'è´¨æ„Ÿ', 'è¶…æ£’', 'è½¯è½¯', 'ç³¯', 'å¤–é¢', 'æ­ä¸ª', 'å¤§è¡£', 'éŸ©å‰§', 'å¥³ä¸»', 'ç–¯ç‹‚', 'çˆ±ä¸Š', 'å¤§å­¦ç”Ÿ', 'ç©¿', 'æ­', 'æ—¥å¸¸', 'æ°”è´¨', 'ç§‹å†¬', 'å°ä¸ªå­']\n",
      "\n",
      "Example 9914:\n",
      "summary_cleaned: ['ä¸‡åœ£èŠ‚', 'ç©¿', 'ç©¿', 'åº—é‡Œ', 'æ–°å“', 'æ‹', 'ä¸‡åœ£èŠ‚', 'æ¯æ—¥', 'ç©¿', 'æ­', 'å“¥ç‰¹']\n",
      "filter_label: ['ä¸‡åœ£èŠ‚', 'ç©¿', 'åº—é‡Œ', 'æ–°å“', 'æ‹', 'æ¯æ—¥', 'æ­', 'å“¥ç‰¹']\n",
      "\n",
      "Example 15089:\n",
      "summary_cleaned: ['èº«æ', 'å·¨å·¨', 'å·¨å¥½', 'ç©¿', 'doubleexclamationmark', 'å¾®èƒ–', 'å¥³ç”Ÿ', 'å¤§èƒ†', 'ç©¿', 'å†¬å¤©', 'çœŸçš„', 'ç©¿', 'èƒ¯', 'å®½', 'è…¿', 'ç²—', 'æ¢¨å½¢', 'èº«æ', 'å¾®èƒ–', 'å¥³å­©', 'ç©¿', 'æ­', 'å¾®èƒ–', 'ç©¿', 'æ­', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'æ¢¨å½¢', 'èº«æ', 'ç©¿', 'æ­', 'æ¢¨å½¢', 'å¾®èƒ–', 'æ¢¨å½¢', 'èº«æ', 'ç²—', 'è…¿', 'æ¢¨å½¢', 'ç©¿', 'æ­', 'æ¢¨å½¢', 'èƒ¯', 'å®½', 'è…¿', 'ç²—', 'ç©¿', 'æ­', 'å¾®èƒ–', 'ç§‹å†¬', 'å†¬å­£', 'ç©¿', 'æ­', 'å¾®èƒ–', 'å¥³ç”Ÿ', 'å†¬å­£', 'ç©¿', 'æ­']\n",
      "filter_label: ['èº«æ', 'å·¨å·¨', 'å·¨å¥½', 'ç©¿', 'å¾®èƒ–', 'å¥³ç”Ÿ', 'å¤§èƒ†', 'å†¬å¤©', 'çœŸçš„', 'èƒ¯', 'å®½', 'è…¿', 'ç²—', 'æ¢¨å½¢', 'å¥³å­©', 'æ­', 'æ—¥å¸¸', 'ç§‹å†¬', 'å†¬å­£']\n",
      "\n",
      "Example 2309:\n",
      "summary_cleaned: ['åŸºç¡€', 'Tee', 'ä¸€è¡£', 'ç©¿', 'æ³•åˆ™', 'å¤å¤©', 'å°‘ä¸äº†', 'åŸºç¡€', 'Tæ¤', 'ç©¿', 'æ›´', 'èŠ±æ ·', 'æ€§', 'é€‰æ‹©', 'ä¸‰æ¡', 'ç±»å‹', 'è£¤å­', 'æ­é…', 'è£¤å‹', 'æ­é…', 'åŸºç¡€', 'tee', 'å‘æŒ¥', 'å¯ç©æ€§', 'ç©¿', 'é¦™', 'ootd', 'æ¯æ—¥', 'ç©¿', 'æ­', 'ç”·ç”Ÿ', 'ç©¿', 'æ­', 'å¼€æ˜¥', 'ç©¿', 'æ­', 'åŸºç¡€', 'çŸ­è¢–', 'osmosismocuishleESOTERICInexistence', 'å­˜ä¸–', 'GaforReal']\n",
      "filter_label: ['åŸºç¡€', 'Tee', 'ä¸€è¡£', 'ç©¿', 'æ³•åˆ™', 'å¤å¤©', 'å°‘ä¸äº†', 'Tæ¤', 'æ›´', 'èŠ±æ ·', 'æ€§', 'é€‰æ‹©', 'ç±»å‹', 'è£¤å­', 'æ­é…', 'è£¤å‹', 'tee', 'å‘æŒ¥', 'å¯ç©æ€§', 'é¦™', 'ootd', 'æ¯æ—¥', 'æ­', 'ç”·ç”Ÿ', 'å¼€æ˜¥', 'çŸ­è¢–', 'GaforReal']\n",
      "\n",
      "Example 4101:\n",
      "summary_cleaned: ['åªç¾', 'æ‹‰å¾·', 'åŒ…åŒ…', 'stuffedflatbread', 'ğ–§§', 'å¤å¤', 'æ£•è‰²', 'ç™¾æ­', 'mate', 'å¤§å®¹é‡', 'æ£•è‰²', 'ç³»', 'mediumdarkskintone', 'æ»´', 'åŒ…åŒ…', 'æ’è‰²', 'å·§å…‹åŠ›', 'åŒ…åŒ…', 'chocolatebar', 'ç¾Šç¾”', 'æ¯›', 'åŒ…åŒ…', 'ewe', 'ç‰›ä»”', 'åˆºç»£', 'æ‹¼æ¥', 'æ£•è‰²', 'åŒ…åŒ…', 'sewingneedle', 'å“­å–Š', 'ä¸­å¿ƒ', 'å¤å¤', 'æ ¼çº¹', 'å¸†å¸ƒåŒ…', 'burrito', 'æ³¢å£«é¡¿', 'åŒ…å‹', 'å¤å¤', 'è…‹ä¸‹', 'åŒ…', 'meatonbone', 'åŒ…åŒ…', 'åçˆ±', 'å°ä¼—', 'åŒ…å°ä¼—', 'åŒ…åŒ…', 'å¤§å­¦ç”Ÿ', 'ä¸Šè¯¾', 'åŒ…åŒ…', 'è…‹ä¸‹', 'åŒ…', 'å¸†å¸ƒåŒ…', 'ç¾', 'æ‹‰å¾·', 'æ£•è‰²', 'åŒ…åŒ…', 'å¤å¤', 'åŒ…åŒ…', 'ç§‹å†¬', 'ç™¾æ­', 'åŒ…åŒ…', 'é€šå‹¤', 'åŒ…åŒ…', 'åŒ…åŒ…', 'åˆ†äº«', 'å®å®', 'è¾…é£Ÿ', 'åŒ…åŒ…', 'é‡æ ·', 'å¤§å®¹é‡', 'åŒ…åŒ…']\n",
      "filter_label: ['åªç¾', 'æ‹‰å¾·', 'åŒ…åŒ…', 'å¤å¤', 'æ£•è‰²', 'ç™¾æ­', 'mate', 'å¤§å®¹é‡', 'ç³»', 'æ»´', 'æ’è‰²', 'å·§å…‹åŠ›', 'chocolatebar', 'ç¾Šç¾”', 'æ¯›', 'ç‰›ä»”', 'åˆºç»£', 'æ‹¼æ¥', 'sewingneedle', 'ä¸­å¿ƒ', 'æ ¼çº¹', 'å¸†å¸ƒåŒ…', 'åŒ…å‹', 'è…‹ä¸‹', 'åŒ…', 'meatonbone', 'åçˆ±', 'å°ä¼—', 'åŒ…å°ä¼—', 'å¤§å­¦ç”Ÿ', 'ä¸Šè¯¾', 'ç¾', 'ç§‹å†¬', 'é€šå‹¤', 'åˆ†äº«', 'å®å®', 'è¾…é£Ÿ', 'é‡æ ·']\n",
      "\n",
      "Example 4394:\n",
      "summary_cleaned: ['æ”’', 'å¤å¤©', 'ç²‰è‰²', 'growingheart', 'çˆ±', 'ç©¿', 'ç²‰è‰²', 'ç³»', 'ä¸­å¹´å¦‡å¥³', 'ootd', 'æ¯æ—¥', 'ç©¿', 'æ­', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'æ¯æ—¥', 'ç©¿', 'æ­', 'ç²‰è‰²', 'ç³»', 'ç²‰è‰²', 'å°‘å¥³', 'å¿ƒ']\n",
      "filter_label: ['å¤å¤©', 'ç²‰è‰²', 'growingheart', 'çˆ±', 'ç©¿', 'ç³»', 'ä¸­å¹´å¦‡å¥³', 'ootd', 'æ¯æ—¥', 'æ­', 'æ—¥å¸¸', 'å°‘å¥³', 'å¿ƒ']\n",
      "\n",
      "Example 12496:\n",
      "summary_cleaned: ['å›æ‘', 'å†¬å­£', 'ç©¿', 'æ­']\n",
      "filter_label: ['å†¬å­£', 'ç©¿', 'æ­']\n",
      "\n",
      "Example 2827:\n",
      "summary_cleaned: ['æ¡çº¹', 'ç®€çº¦', 'æ¬¾', 'é•¿è¢–', 'doubleexclamationmarkMalaysiains', 'æ¡çº¹', 'é•¿è¢–', 'çŸ­ç‰ˆ', 'ä»¶å¥—', 'å¹³ä»·', 'å¥½ç‰©', 'ç©¿', 'é¦™', 'æŒ‘æˆ˜', 'æ˜ŸæœŸ', 'ç©¿', 'æ­', 'é‡æ ·', 'ç¬”è®°', 'çµæ„Ÿ', 'é©¬æ¥è¥¿äºš', 'æœè£…åº—', 'ç”Ÿ']\n",
      "filter_label: ['æ¡çº¹', 'ç®€çº¦', 'æ¬¾', 'é•¿è¢–', 'doubleexclamationmarkMalaysiains', 'çŸ­ç‰ˆ', 'ä»¶å¥—', 'å¹³ä»·', 'å¥½ç‰©', 'ç©¿', 'é¦™', 'æŒ‘æˆ˜', 'æ­', 'é‡æ ·', 'ç¬”è®°', 'çµæ„Ÿ', 'æœè£…åº—', 'ç”Ÿ']\n",
      "\n",
      "Example 15129:\n",
      "summary_cleaned: ['ä¸€å¤œ', 'çˆ†ç«', 'ç©¿', 'æ­', 'å¥—è·¯', 'å…­ä¸€', 'çŒ«', 'ç©¿', 'æ­', 'æ™®é€šäºº', 'ç©¿', 'æ­', 'åšä¸»ç©¿', 'æ­', 'ootd']\n",
      "filter_label: ['ä¸€å¤œ', 'çˆ†ç«', 'ç©¿', 'æ­', 'å…­ä¸€', 'çŒ«', 'æ™®é€šäºº', 'åšä¸»ç©¿', 'ootd']\n",
      "\n",
      "Example 9491:\n",
      "summary_cleaned: ['rmoneybag', 'æ‹¿ä¸‹', 'å¾ˆç«', 'ç¾½ç»’æœ', 'doubleexclamationmarkg', 'ç™½', 'é¸­ç»’', 'è¿™ä»¶', 'ç¾½ç»’æœ', 'æ£’çƒ', 'æœ', 'æ¬¾å¼', 'æ—¶å°š', 'ä¿æš–', 'smilingfacewithhearts', 'åšåº¦', 'ä¸ç®—', 'åš', 'ç‰ˆå‹', 'å¥½çœ‹', 'éŸ©ç³»', 'è¡£é•¿', 'å¾ˆé•¿', 'ä¸ªäººæ„Ÿè§‰', 'é€‚åˆ', 'å°ä¸ªå­', 'å§å¦¹', 'cleanfit', 'ç®€å•', 'å‡ºé—¨', 'ç¾½ç»’æœ', 'çŸ­æ¬¾', 'ç¾½ç»’æœ', 'ç¾½ç»’æœ', 'æ¨è', 'ç©¿ç€', 'è¿˜æ˜¾', 'ç˜¦', 'ç¾½ç»’æœ']\n",
      "filter_label: ['rmoneybag', 'æ‹¿ä¸‹', 'å¾ˆç«', 'ç¾½ç»’æœ', 'doubleexclamationmarkg', 'ç™½', 'é¸­ç»’', 'è¿™ä»¶', 'æœ', 'æ¬¾å¼', 'æ—¶å°š', 'ä¿æš–', 'åšåº¦', 'ä¸ç®—', 'åš', 'ç‰ˆå‹', 'å¥½çœ‹', 'éŸ©ç³»', 'è¡£é•¿', 'å¾ˆé•¿', 'ä¸ªäººæ„Ÿè§‰', 'é€‚åˆ', 'å°ä¸ªå­', 'å§å¦¹', 'cleanfit', 'ç®€å•', 'å‡ºé—¨', 'çŸ­æ¬¾', 'æ¨è', 'ç©¿ç€', 'è¿˜æ˜¾', 'ç˜¦']\n",
      "\n",
      "Example 4194:\n",
      "summary_cleaned: ['å¹³ä»·', 'æ˜¾ç˜¦', 'é’ˆç»‡è¡«', 'ä¸‰åå¤š', 'moneybag', 'ç‰ˆå‹', 'å·¨', 'å¥½å¥½', 'æ¸©æŸ”', 'æ°›å›´', 'æ„Ÿ', 'æ‡‚', 'è½¯ä¹ä¹', 'ä¸æ‰', 'å¼¹åŠ›', 'æ²¡', 'èµ·çƒ', 'é’ˆç»‡è¡«', 'æ­', 'æ˜¾ç˜¦', 'ç©¿', 'æ­', 'å¹³ä»·', 'å¥½ç‰©', 'æ¢¨å½¢', 'èº«æ', 'ç©¿', 'é¦™', 'ç§‹å†¬', 'ç©¿', 'æ­', 'å¹³ä»·', 'å¥½ç‰©', 'å­¦ç”Ÿ', 'å…š']\n",
      "filter_label: ['å¹³ä»·', 'æ˜¾ç˜¦', 'é’ˆç»‡è¡«', 'ä¸‰åå¤š', 'ç‰ˆå‹', 'å·¨', 'å¥½å¥½', 'æ¸©æŸ”', 'æ°›å›´', 'æ„Ÿ', 'æ‡‚', 'è½¯ä¹ä¹', 'ä¸æ‰', 'å¼¹åŠ›', 'æ²¡', 'èµ·çƒ', 'æ­', 'ç©¿', 'å¥½ç‰©', 'æ¢¨å½¢', 'èº«æ', 'é¦™', 'ç§‹å†¬', 'å­¦ç”Ÿ', 'å…š']\n",
      "\n",
      "Example 2920:\n",
      "summary_cleaned: ['å®œå®¶', 'é•œå­', 'å¥½å¥½çœ‹', 'è®°å½•', 'ç”Ÿæ´»', 'ç”œå¦¹', 'æ¯æ—¥', 'ç©¿', 'æ­', 'è½¯å¦¹', 'ç©¿', 'æ­', 'å®œå®¶', 'å®œå®¶', 'æ‹ç…§', 'å®œå®¶', 'å¥½ç‰©']\n",
      "filter_label: ['é•œå­', 'å¥½å¥½çœ‹', 'è®°å½•', 'ç”Ÿæ´»', 'ç”œå¦¹', 'æ¯æ—¥', 'ç©¿', 'æ­', 'è½¯å¦¹', 'æ‹ç…§', 'å¥½ç‰©']\n",
      "\n",
      "Example 3775:\n",
      "summary_cleaned: ['eplus', 'å§å¦¹', 'çœ‹è¿‡', 'åˆç§‹', 'æ ¼è£™', 'å¥—è£…', 'åˆ°è´§', 'æµ‹è¯„', 'doubleexclamationmark', 'æµªæ¼«ç”Ÿæ´»', 'è®°å½•', 'ç¬”è®°', 'çµæ„Ÿ', 'æœè£…', 'æµ‹è¯„', 'å¥—è£…', 'å¹³ä»·', 'è¡£æœ', 'æµ‹è¯„', 'åˆç§‹', 'ç©¿', 'æ­']\n",
      "filter_label: ['å§å¦¹', 'çœ‹è¿‡', 'åˆç§‹', 'æ ¼è£™', 'å¥—è£…', 'åˆ°è´§', 'æµ‹è¯„', 'æµªæ¼«ç”Ÿæ´»', 'è®°å½•', 'ç¬”è®°', 'çµæ„Ÿ', 'æœè£…', 'å¹³ä»·', 'è¡£æœ', 'ç©¿', 'æ­']\n",
      "\n",
      "Example 4534:\n",
      "summary_cleaned: ['å‡ åå—', 'lulu', 'åŒæ¬¾', 'å¤–å¥—', 'ç™½è‰²', 'ç›´', 'ç­’è£¤', 'å¤ªæ˜¾', 'ç˜¦', 'æ', 'ç®€ç©¿', 'æ­', 'è¶…ä¼š', 'ç©¿', 'ä¼åˆ’', 'lulu', 'å¹³æ›¿', 'ootd', 'æ¯æ—¥', 'ç©¿', 'æ­', 'æ˜¾ç˜¦', 'ç©¿', 'æ­', 'ootd', 'ç™½è‰²', 'ç‰›ä»”è£¤', 'æ‹ç…§', 'å§¿åŠ¿', 'é‡æ ·', 'lulu', 'é»‘è‰²']\n",
      "filter_label: ['å‡ åå—', 'lulu', 'åŒæ¬¾', 'å¤–å¥—', 'ç™½è‰²', 'ç›´', 'ç­’è£¤', 'å¤ªæ˜¾', 'ç˜¦', 'æ', 'ç®€ç©¿', 'æ­', 'è¶…ä¼š', 'ç©¿', 'ä¼åˆ’', 'å¹³æ›¿', 'ootd', 'æ¯æ—¥', 'æ˜¾ç˜¦', 'ç‰›ä»”è£¤', 'æ‹ç…§', 'å§¿åŠ¿', 'é‡æ ·', 'é»‘è‰²']\n",
      "\n",
      "Example 10565:\n",
      "summary_cleaned: ['Meetingyouistrulyamiracleofmylif', 'å…­èŠ’æ˜Ÿ', 'é•¶åµŒ', 'æ‰‹é•¯', 'ä¸Šåˆ»', 'Meetingyouistrulyamiracleofmylife', 'é‡è§', 'çœŸçš„', 'ç”Ÿå‘½', 'ä¸­', 'å¥‡è¿¹', 'æ‰‹å·¥', 'æ—¥å¸¸', 'å®è—', 'é¥°å“', 'å…¬å¼€', 'é…é¥°', 'åˆ†äº«', 'æ‰‹é•¯', 'æ‰‹å·¥', 'ä»¶', 'ä¸å¯æ€è®®', 'äº‹å„¿', 'çº¯', 'é“¶æ‰‹é•¯', 'æ‰‹å·¥', 'é“¶é¥°', 'é¦–é¥°', 'blingbling', 'æ™’æ™’', 'æ‰‹ä¸Š', 'æˆ´', 'æ‰‹å·¥', 'æ‰‹é•¯']\n",
      "filter_label: ['Meetingyouistrulyamiracleofmylif', 'é•¶åµŒ', 'æ‰‹é•¯', 'ä¸Šåˆ»', 'Meetingyouistrulyamiracleofmylife', 'é‡è§', 'çœŸçš„', 'ç”Ÿå‘½', 'ä¸­', 'å¥‡è¿¹', 'æ‰‹å·¥', 'æ—¥å¸¸', 'å®è—', 'é¥°å“', 'å…¬å¼€', 'é…é¥°', 'åˆ†äº«', 'ä»¶', 'ä¸å¯æ€è®®', 'äº‹å„¿', 'çº¯', 'é“¶æ‰‹é•¯', 'é“¶é¥°', 'é¦–é¥°', 'blingbling', 'æ™’æ™’', 'æ‰‹ä¸Š', 'æˆ´']\n",
      "\n",
      "Example 7473:\n",
      "summary_cleaned: ['å¾®èƒ–', 'èƒ¯', 'å®½', 'å¦¹å¦¹', 'æˆ‘é€‰', 'ç®€ç®€å•å•', 'è€çœ‹', 'H', 'å‹', 'èº«æ', 'éª¨æ¶', 'å¤æ—¥', 'ä¸¤ç©¿', 'è¿è¡£è£™', 'æ¸©æŸ”', 'éœ²', 'è‚©', 'ä¸éœ²', 'è‚©', 'ç¾æè‰²', 'è¿è¡£è£™', 'æ°”è´¨', 'ä»£è¡¨', 'è‰²', 'è…°éƒ¨', 'è¤¶çš±', 'é®', 'è‚šå­', 'è‚‰', 'è‚‰', 'ç®€å•', 'æ˜¾', 'æ°”è´¨', 'çˆ¸å¦ˆ', 'å¤¸', 'è£™å­', 'å®‰åˆ©', 'æœˆ', 'ç¬¬ä¸€å¼ ', 'ç…§ç‰‡', 'ç¬”è®°', 'çµæ„Ÿ', 'å¤æ—¥', 'å®è—', 'è£™å­', 'è“¬è“¬è£™', 'å¾®èƒ–', 'ç©¿', 'æ­', 'æ˜¾ç˜¦', 'ç©¿', 'æ­', 'æ˜¾ç˜¦', 'è¿è¡£è£™', 'è¿è¡£è£™', 'æ°”è´¨', 'è¿è¡£è£™']\n",
      "filter_label: ['å¾®èƒ–', 'èƒ¯', 'å®½', 'å¦¹å¦¹', 'æˆ‘é€‰', 'ç®€ç®€å•å•', 'è€çœ‹', 'å‹', 'èº«æ', 'éª¨æ¶', 'å¤æ—¥', 'ä¸¤ç©¿', 'è¿è¡£è£™', 'æ¸©æŸ”', 'éœ²', 'è‚©', 'ä¸éœ²', 'ç¾æè‰²', 'æ°”è´¨', 'è‰²', 'è…°éƒ¨', 'è¤¶çš±', 'é®', 'è‚šå­', 'è‚‰', 'ç®€å•', 'æ˜¾', 'çˆ¸å¦ˆ', 'å¤¸', 'è£™å­', 'å®‰åˆ©', 'æœˆ', 'ç¬¬ä¸€å¼ ', 'ç…§ç‰‡', 'ç¬”è®°', 'çµæ„Ÿ', 'å®è—', 'è“¬è“¬è£™', 'ç©¿', 'æ­', 'æ˜¾ç˜¦']\n",
      "\n",
      "Example 6619:\n",
      "summary_cleaned: ['cm', 'æ–¤', 'ç¾å¼', 'å¤å¤', 'è¿åŠ¨', 'å¥³å­©', 'å®½æ¾', 'ç´§èº«', 'æ··', 'æ­', 'ç¼ºä¸€ä¸å¯', 'å°ä¸ªå­', 'ç©¿', 'æ­', 'ç¾å¼', 'ç©¿', 'æ­', 'è¾£å¦¹', 'ç©¿', 'æ­', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'å†¬å­£', 'ç©¿', 'æ­']\n",
      "filter_label: ['ç¾å¼', 'å¤å¤', 'è¿åŠ¨', 'å¥³å­©', 'å®½æ¾', 'ç´§èº«', 'æ··', 'æ­', 'ç¼ºä¸€ä¸å¯', 'å°ä¸ªå­', 'ç©¿', 'è¾£å¦¹', 'æ—¥å¸¸', 'å†¬å­£']\n",
      "\n",
      "Example 5295:\n",
      "summary_cleaned: ['æ•™å¸ˆ', 'ç©¿', 'æ­', 'æ¢¨å½¢', 'èº«æ', 'è¯·', 'è¿™æ¡', 'ç¥è£¤', 'ç„Š', 'èº«ä¸Š', 'è“', 'è¡¬è¡«', 'ç™½', 'è£¤å­', 'çœŸçš„', 'å¤ª', 'æ¸…çˆ½', 'è€çœ‹', 'æ°§æ°”', 'æ„Ÿ', 'åè¶³', 'æ¢¨å½¢', 'èº«æ', 'æ‰¾åˆ°', 'æœ¬å‘½', 'ç¥è£¤', 'è…°å›´', 'è‡€å›´', 'è…°å›´', 'åˆšåˆš', 'è‡€å›´', 'æ”¾é‡', 'å¾ˆå¤§', 'çœŸçš„', 'è¶…æ˜¾', 'è…¿', 'ç›´', 'ç©¿', 'é¦™', 'æ•™å¸ˆ', 'ç©¿', 'æ­', 'ç©¿', 'æ­', 'æ¢¨å½¢', 'èº«æ', 'è£¤å­', 'ç™½è‰²', 'è£¤å­', 'yyds', 'æ³•å¼', 'å¤å¤', 'ç©¿', 'æ­', 'æ­', 'è£¤å­', 'æ°”è´¨', 'ç©¿', 'æ­è–¯', 'é˜Ÿé•¿', 'è–¯', 'ç®¡å®¶', 'æ½®æµ', 'è–¯', 'æ—¶å°š', 'è–¯']\n",
      "filter_label: ['æ•™å¸ˆ', 'ç©¿', 'æ­', 'æ¢¨å½¢', 'èº«æ', 'è¯·', 'è¿™æ¡', 'ç¥è£¤', 'ç„Š', 'èº«ä¸Š', 'è“', 'è¡¬è¡«', 'ç™½', 'è£¤å­', 'çœŸçš„', 'å¤ª', 'æ¸…çˆ½', 'è€çœ‹', 'æ°§æ°”', 'æ„Ÿ', 'åè¶³', 'æ‰¾åˆ°', 'æœ¬å‘½', 'è…°å›´', 'è‡€å›´', 'åˆšåˆš', 'æ”¾é‡', 'å¾ˆå¤§', 'è¶…æ˜¾', 'è…¿', 'ç›´', 'é¦™', 'ç™½è‰²', 'yyds', 'æ³•å¼', 'å¤å¤', 'æ°”è´¨', 'æ­è–¯', 'é˜Ÿé•¿', 'è–¯', 'ç®¡å®¶', 'æ½®æµ', 'æ—¶å°š']\n",
      "\n",
      "Example 8590:\n",
      "summary_cleaned: ['å­¦ç”Ÿ', 'å…š', 'ç§‹å†¬', 'ç™¾æ­', 'é•¿è£¤', 'jeans', 'æ˜¾é«˜æ˜¾', 'ç˜¦', 'æ˜¾è…¿ç›´', 'ç¥è£¤', 'ä½', 'keycapkeycapkeycapkeycapkeycapkeycap', 'è£¤å­', 'é•¿è£¤', 'ç§‹å†¬', 'é•¿è£¤', 'ç™¾æ­æ˜¾', 'ç˜¦', 'ç¥è£¤', 'ä¸éœ²', 'è…¿', 'ç©¿', 'æ­', 'é«˜ä¸ªå­', 'è£¤å­', 'åŠ ç»’', 'è£¤å­']\n",
      "filter_label: ['å­¦ç”Ÿ', 'å…š', 'ç§‹å†¬', 'ç™¾æ­', 'é•¿è£¤', 'æ˜¾é«˜æ˜¾', 'ç˜¦', 'æ˜¾è…¿ç›´', 'ç¥è£¤', 'ä½', 'è£¤å­', 'ç™¾æ­æ˜¾', 'ä¸éœ²', 'è…¿', 'ç©¿', 'æ­', 'é«˜ä¸ªå­', 'åŠ ç»’']\n",
      "\n",
      "Example 14843:\n",
      "summary_cleaned: ['é»‘è‰²', 'ç³»', 'æ¯æ—¥', 'ç©¿', 'æ­', 'ç©¿', 'æ­']\n",
      "filter_label: ['é»‘è‰²', 'ç³»', 'æ¯æ—¥', 'ç©¿', 'æ­']\n",
      "\n",
      "Example 14141:\n",
      "summary_cleaned: ['å°ä¸ªå­', 'å¥³ç”Ÿ', 'ç©¿', 'è¿™å¥—', 'COOLbutton', 'ä»Šæ—¥', 'å¿«ä¹', 'ä»Šæ—¥', 'å‘èµ·', 'é£', 'ç©¿å«è¡£', 'å¥½å¼€å¿ƒ', 'ç©¿å«è¡£', 'æ­é…', 'ç‰›ä»”è£¤', 'å–œæ¬¢', 'æ·±è‰²', 'ç‰›ä»”è£¤', 'æ„Ÿè§‰', 'æ›´ç™¾æ­åŠ ', 'äº®çœ¼', 'åŒ…åŒ…', 'å¥½çœ‹', 'æ¯æ—¥', 'ç©¿', 'æ­', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'ç©¿', 'æ­', 'å°ä¸ªå­', 'å¥³ç”Ÿ', 'ç©¿', 'æ­', 'ootd', 'æ¯æ—¥', 'ç©¿', 'æ­', 'æ˜¾ç˜¦', 'ç©¿', 'æ­', 'æ°”è´¨', 'ç©¿', 'æ­', 'æ­', 'è£¤å­', 'å¥³ç”Ÿ']\n",
      "filter_label: ['å°ä¸ªå­', 'å¥³ç”Ÿ', 'ç©¿', 'è¿™å¥—', 'COOLbutton', 'ä»Šæ—¥', 'å¿«ä¹', 'å‘èµ·', 'é£', 'ç©¿å«è¡£', 'å¥½å¼€å¿ƒ', 'æ­é…', 'ç‰›ä»”è£¤', 'å–œæ¬¢', 'æ·±è‰²', 'æ„Ÿè§‰', 'æ›´ç™¾æ­åŠ ', 'äº®çœ¼', 'åŒ…åŒ…', 'å¥½çœ‹', 'æ¯æ—¥', 'æ­', 'æ—¥å¸¸', 'ootd', 'æ˜¾ç˜¦', 'æ°”è´¨', 'è£¤å­']\n",
      "\n",
      "Example 11979:\n",
      "summary_cleaned: ['ç”œå¿ƒ', 'è¾£å¦¹', 'æ„Ÿè°¢', 'æ¨è', 'æ ‡é¢˜', 'å“ˆå“ˆå“ˆ', 'è„‘è¢‹', 'ç©ºç©º', 'ä¸€èº«', 'ç²‰ç²‰', 'çˆ±', 'åŠå¸¦', 'ä½è…°è£¤', 'ç”œå¿ƒ', 'è¾£å¦¹', 'yk', 'è¾£å¦¹', 'ç©¿', 'æ­', 'æ—¥å¸¸']\n",
      "filter_label: ['ç”œå¿ƒ', 'è¾£å¦¹', 'æ„Ÿè°¢', 'æ¨è', 'æ ‡é¢˜', 'å“ˆå“ˆå“ˆ', 'è„‘è¢‹', 'ä¸€èº«', 'ç²‰ç²‰', 'çˆ±', 'åŠå¸¦', 'ä½è…°è£¤', 'yk', 'ç©¿', 'æ­', 'æ—¥å¸¸']\n",
      "\n",
      "Example 11515:\n",
      "summary_cleaned: ['carrot', 'è£™å­', 'å–œæ¬¢', 'è‰²', 'å®‰æ’', 'åšå¥½', 'æ ·è¡£', 'æ‹', 'æ­£å¼', 'å›¾', 'å‡†ç¡®', 'å®ç‰©', 'ä¸€å¼ ', 'æ»´', 'p', 'å›¾', 'æ•ˆæœ', 'å½±å“', 'è§‚çœ‹', 'é…è‰²', 'Lolita']\n",
      "filter_label: ['carrot', 'è£™å­', 'å–œæ¬¢', 'è‰²', 'å®‰æ’', 'åšå¥½', 'æ ·è¡£', 'æ‹', 'æ­£å¼', 'å›¾', 'å‡†ç¡®', 'å®ç‰©', 'ä¸€å¼ ', 'æ»´', 'p', 'æ•ˆæœ', 'å½±å“', 'è§‚çœ‹', 'é…è‰²', 'Lolita']\n",
      "\n",
      "Example 5370:\n",
      "summary_cleaned: ['è´µæ°”æ„Ÿ', 'æºå¤´', 'è¡€æ°”æ–¹åˆš', 'è´µæ°”æ„Ÿ', 'è¯´', 'åƒ', 'ç©¿', 'å¤šè´µ', 'èº«ä¸Š', 'é‚£ç§', 'æˆ˜æ–—åŠ›', 'èº«ä¸Š', 'å¥åº·', 'å‹‡æ•¢', 'ç§¯æå‘ä¸Š', 'ç”Ÿæ´»æ€åº¦', 'ç²¾ç¥çŠ¶æ€', 'æ°”è´¨', 'æå‡', 'å¸Œæœ›', 'æ´»åŠ›', 'æ»¡æ»¡', 'ä¹±ä¸ƒå…«ç³Ÿ', 'æŠ¤è‚¤å“', 'æ›´', 'å–œæ¬¢', 'èº«ä½“', 'è¡¥å……', 'å†…æœ', 'äº§å“', 'è„¸ä¸Š', 'æ•£å‘', 'çº¢æ¶¦', 'å…‰æ³½æ„Ÿ', 'å–œæ¬¢', 'ç†¬å¤œ', 'å–é…’', 'é‚£é˜µå­', 'çœŸçš„', 'å…ƒæ°”å¤§ä¼¤', 'è¡¥', 'ä¸€é˜µå­', 'æ°”è¡€', 'çœŸçš„', 'æ„Ÿè§‰', 'æŒº', 'æœ‰ç”¨', 'è„¸ä¸Š', 'çš®è‚¤', 'é€äº®', 'æ°”è¡€', 'æ¶Œä¸Šæ¥', 'é‚£ç§', 'çº¢æ¶¦', 'æ„Ÿ', 'åƒ', 'é˜¿èƒ¶', 'å…¥', 'æ¶²ä½“', 'é˜¿èƒ¶', 'å£æœæ¶²', 'å¬è¯´', 'æ¶²ä½“', 'æ›´å¥½', 'å¸æ”¶', 'ç¦ç«™', 'å®¶', 'å£æœæ¶²', 'ä¸€ç“¶', 'ml', 'é˜¿èƒ¶', 'æˆåˆ†', 'å«', 'ml', 'å‰©ä¸‹', 'å½“å½’', 'å…šå‚', 'æ°”è¡€', 'åŒè¡¥', 'é˜³å®Œ', 'çœŸçš„', 'æ„Ÿå†’', 'é¢‘ç¹', 'ç¦ç«™', 'é˜¿èƒ¶', 'å£æœæ¶²', 'å–', 'æ•´å¹´', 'å»å¹´', 'åæœˆ', 'å–', 'å¹³æ—¶', 'å‡ºå»ç©', 'å°å°çš„', 'ä¸€åª', 'æ”¾åœ¨', 'åŒ…é‡Œ', 'å–', 'æ²¡æƒ³åˆ°', 'æ•ˆæœ', 'ym', 'ä¸è°ƒ', 'å§å¦¹', 'æƒ³åˆ°', 'å¤©å¤©', 'å–é…’', 'ym', 'æ¯æ¬¡', 'ç‰¹åˆ«', 'è§„å¾‹', 'æƒ³', 'æ˜¾å¾—', 'è´µæ°”', 'æ°”è´¨', 'æ°”', 'åœºä¸Š', 'è¾“', 'æ–¹æ³•', 'ç®€å•', 'ç¼ºä»€ä¹ˆ', 'è¡¥', 'ç²¾ç¥çŠ¶æ€', 'å–é…’', 'è°ƒ', 'å¾®é†º', 'é˜¿èƒ¶', 'å£æœæ¶²', 'æ‹ç…§', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'äº«å—', 'ç”Ÿæ´»', 'æ°”è¡€', 'è¡¥', 'å–é…’', 'å¾®é†º', 'æ°›å›´', 'æ„Ÿ', 'è¾£å¦¹']\n",
      "filter_label: ['è´µæ°”æ„Ÿ', 'è¯´', 'åƒ', 'ç©¿', 'å¤šè´µ', 'èº«ä¸Š', 'é‚£ç§', 'æˆ˜æ–—åŠ›', 'å¥åº·', 'å‹‡æ•¢', 'ç§¯æå‘ä¸Š', 'ç”Ÿæ´»æ€åº¦', 'ç²¾ç¥çŠ¶æ€', 'æ°”è´¨', 'æå‡', 'å¸Œæœ›', 'æ´»åŠ›', 'æ»¡æ»¡', 'ä¹±ä¸ƒå…«ç³Ÿ', 'æŠ¤è‚¤å“', 'æ›´', 'å–œæ¬¢', 'èº«ä½“', 'è¡¥å……', 'å†…æœ', 'äº§å“', 'è„¸ä¸Š', 'æ•£å‘', 'çº¢æ¶¦', 'å…‰æ³½æ„Ÿ', 'ç†¬å¤œ', 'å–é…’', 'é‚£é˜µå­', 'çœŸçš„', 'è¡¥', 'ä¸€é˜µå­', 'æ°”è¡€', 'æ„Ÿè§‰', 'æŒº', 'æœ‰ç”¨', 'çš®è‚¤', 'é€äº®', 'æ¶Œä¸Šæ¥', 'æ„Ÿ', 'å…¥', 'æ¶²ä½“', 'å£æœæ¶²', 'å¬è¯´', 'æ›´å¥½', 'å¸æ”¶', 'å®¶', 'ä¸€ç“¶', 'æˆåˆ†', 'å«', 'å‰©ä¸‹', 'åŒè¡¥', 'æ„Ÿå†’', 'é¢‘ç¹', 'å–', 'æ•´å¹´', 'å»å¹´', 'å¹³æ—¶', 'å‡ºå»ç©', 'å°å°çš„', 'ä¸€åª', 'æ”¾åœ¨', 'åŒ…é‡Œ', 'æ²¡æƒ³åˆ°', 'æ•ˆæœ', 'ä¸è°ƒ', 'å§å¦¹', 'æƒ³åˆ°', 'å¤©å¤©', 'æ¯æ¬¡', 'ç‰¹åˆ«', 'æƒ³', 'æ˜¾å¾—', 'è´µæ°”', 'æ°”', 'åœºä¸Š', 'è¾“', 'æ–¹æ³•', 'ç®€å•', 'è°ƒ', 'å¾®é†º', 'æ‹ç…§', 'æ—¥å¸¸', 'æ­', 'äº«å—', 'ç”Ÿæ´»', 'æ°›å›´', 'è¾£å¦¹']\n",
      "\n",
      "Example 2141:\n",
      "summary_cleaned: ['å¤©', 'æˆ‘ä¼š', 'ç©¿', 'é¦™äº‘çº±', 'æ”¹è‰¯', 'æ±‰æœ', 'é¦™äº‘çº±', 'æ”¹è‰¯', 'æ—¥å¸¸', 'æ¬¾', 'æ±‰æœ', 'æ‡’', 'æ‹’ç»', 'ç¹ç', 'ä¸€å¥—', 'å‡ºé—¨', 'é¦™äº‘çº±', 'é¦™äº‘çº±', 'æ–°å›½æ½®', 'ç©¿', 'æ­', 'æ–°', 'ä¸­å¼', 'æ”¹è‰¯', 'æ±‰æœ', 'æ±‰æœ', 'æ±‰æœ', 'ç©¿', 'æ­', 'æ±‰æœ', 'æ—¥å¸¸', 'æ–°', 'ä¸­å¼', 'ç©¿', 'æ­']\n",
      "filter_label: ['å¤©', 'æˆ‘ä¼š', 'ç©¿', 'é¦™äº‘çº±', 'æ”¹è‰¯', 'æ±‰æœ', 'æ—¥å¸¸', 'æ¬¾', 'æ‡’', 'æ‹’ç»', 'ç¹ç', 'ä¸€å¥—', 'å‡ºé—¨', 'æ–°å›½æ½®', 'æ­', 'æ–°', 'ä¸­å¼']\n",
      "\n",
      "Example 1164:\n",
      "summary_cleaned: ['ç»§', 'å±±æœ¬', 'è£¤å', 'æ˜¥å¤', 'ç¥è£¤', 'è¿™è‚¤', 'æ„Ÿ', 'èˆ’æœ', 'ç»', 'å•Šå•Šå•Š', 'çœŸçš„', 'èˆ’æœ', 'æ²¡', 'ç©¿', 'ç‰ˆå‹', 'å®½æ¾', 'é®è‚‰', 'æ˜¾è…¿ç›´', 'æ´—è¿‡', 'ç¼©æ°´', 'èµ·çƒ', 'ä¸æ˜“', 'çš±', 'ç€å®', 'æ— å¯æŒ‘å‰”', 'ç¥è£¤', 'å•Šå•Šå•Š', 'æ„Ÿè§‰', 'å¿…å¤§', 'çˆ†ç¥è£¤', 'è£¤å­', 'è£¤å­', 'ç§è‰', 'æ­', 'è£¤å­', 'æ˜¾ç˜¦', 'ç¥è£¤', 'ç²‰è‰²', 'è£¤å­', 'å°ä¸ªå­', 'è£¤å­', 'æ¢¨å‹', 'èº«æ', 'è£¤å­', 'ç™¾æ­ç¥è£¤', 'å¤å­£', 'è£¤å­', 'OotD', 'æ¯æ—¥', 'ç©¿', 'æ­', 'ootd', 'æ¯æ—¥', 'ç©¿', 'æ­', 'æ˜¥å¤', 'ç©¿', 'æ­', 'èŒåœº', 'é€šå‹¤', 'ç©¿', 'æ­', 'æ—¥å¸¸', 'ç©¿', 'æ­', 'æ˜¾ç˜¦', 'ç©¿', 'æ­', 'è£¤å­', 'ç©¿', 'æ­', 'å¾®èƒ–', 'å¥³å­©', 'ç©¿', 'æ­', 'å¤æ—¥', 'ç©¿', 'æ­', 'ä¼‘é—²', 'ç©¿', 'æ­', 'æ…µæ‡’', 'è£¤å­', 'è£¤å­', 'æ¨è', 'å·¥äºº', 'ç¥è£¤', 'å‚æ„Ÿ', 'è£¤å­', 'æ¢¨å½¢', 'èº«æ', 'æ¡', 'è£¤å­', 'çœŸçš„', 'ç»']\n",
      "filter_label: ['å±±æœ¬', 'è£¤å', 'æ˜¥å¤', 'ç¥è£¤', 'è¿™è‚¤', 'æ„Ÿ', 'èˆ’æœ', 'ç»', 'å•Šå•Šå•Š', 'çœŸçš„', 'æ²¡', 'ç©¿', 'ç‰ˆå‹', 'å®½æ¾', 'é®è‚‰', 'æ˜¾è…¿ç›´', 'æ´—è¿‡', 'ç¼©æ°´', 'èµ·çƒ', 'ä¸æ˜“', 'çš±', 'ç€å®', 'æ— å¯æŒ‘å‰”', 'æ„Ÿè§‰', 'çˆ†ç¥è£¤', 'è£¤å­', 'ç§è‰', 'æ­', 'æ˜¾ç˜¦', 'ç²‰è‰²', 'å°ä¸ªå­', 'æ¢¨å‹', 'èº«æ', 'ç™¾æ­ç¥è£¤', 'å¤å­£', 'OotD', 'æ¯æ—¥', 'ootd', 'èŒåœº', 'é€šå‹¤', 'æ—¥å¸¸', 'å¾®èƒ–', 'å¥³å­©', 'å¤æ—¥', 'ä¼‘é—²', 'æ…µæ‡’', 'æ¨è', 'å·¥äºº', 'å‚æ„Ÿ', 'æ¢¨å½¢', 'æ¡']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: è¯»å–æ–‡ä»¶\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: éšæœºé‡‡æ ·30è¡Œ\n",
    "sampled_df = df.sample(n=30, random_state=42)  # éšæœºé‡‡æ ·30è¡Œ\n",
    "\n",
    "# Step 3: å»é‡å‡½æ•°ï¼Œä¿æŒé¡ºåºä¸å˜\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: æ‰“å°è¾“å‡ºå…¶ä¸­çš„ summary_cleaned åˆ—å’Œ filter_label åˆ—ï¼Œå»æ‰ filter_label ä¸­çš„é‡å¤å•è¯\n",
    "for i, row in sampled_df.iterrows():\n",
    "    summary_cleaned = eval(row['cleaned_post_text'])  # è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "    filter_label = eval(row['filtered_keywords'])  # è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "    filter_label_unique = remove_duplicates(filter_label)  # å»é‡\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"summary_cleaned: {summary_cleaned}\")\n",
    "    print(f\"filter_label: {filter_label_unique}\")\n",
    "    print()  # ç©ºè¡Œç”¨äºç¾è§‚è¾“å‡º"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
