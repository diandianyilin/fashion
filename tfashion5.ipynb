{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load fashion-related data (positive samples)\n",
    "fashion_df = pd.read_csv('/home/disk1/red_disk1/fashion/topics_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword' column which contains fashion-related keywords\n",
    "fashion_samples = fashion_df['keyword group']\n",
    "\n",
    "# Load non-fashion-related data (negative samples)\n",
    "non_fashion_df = pd.read_csv('/home/disk1/red_disk1/fashion/non_fashion_texts.csv')\n",
    "\n",
    "# Assuming the non-fashion data has a column 'word'\n",
    "non_fashion_samples = non_fashion_df['word']\n",
    "\n",
    "# Step 2: Count the number of samples in each dataset\n",
    "num_fashion_samples = fashion_samples.shape[0]\n",
    "num_non_fashion_samples = non_fashion_samples.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of fashion-related samples (positive): {num_fashion_samples}\")\n",
    "print(f\"Number of non-fashion-related samples (negative): {num_non_fashion_samples}\")\n",
    "\n",
    "# Step 3: Check if they are balanced\n",
    "if num_fashion_samples == num_non_fashion_samples:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(f\"The datasets are not balanced. Difference: {abs(num_fashion_samples - num_non_fashion_samples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (356852, 2)\n",
      "Data Format Example:\n",
      "                  word  label\n",
      "0                染指间岛.      0\n",
      "1                中華民國.      0\n",
      "2  以下介紹一種老虎棋，稱為十八子围老虎。      0\n",
      "3                *火雲聖衣      0\n",
      "4              ΛCDM模型.      0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Progress:  18%|█▊        | 2056/11152 [02:52<11:28, 13.21it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Read Positive Samples from 'topics_filtered.csv'\n",
    "# Load the fashion lexicon (positive samples)\n",
    "pos_file = \"/home/disk1/red_disk1/fashion/topics_filtered.csv\"\n",
    "positive_samples_df = pd.read_csv(pos_file)\n",
    "positive_samples = positive_samples_df['keyword group'].dropna().tolist()  # Extract keywords column\n",
    "\n",
    "# Assign label 1 to positive samples\n",
    "positive_samples = pd.DataFrame(positive_samples, columns=['word'])\n",
    "positive_samples['label'] = 1\n",
    "\n",
    "# Step 2: Read Negative Samples from non_fashion_texts\n",
    "def read_negative_samples(directory):\n",
    "    all_lines = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    all_lines.extend([line.strip() for line in file if line.strip() != \"\"])\n",
    "    return all_lines\n",
    "\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "negative_samples_list = read_negative_samples(neg_dir)\n",
    "\n",
    "# Assign label 0 to negative samples\n",
    "negative_samples = pd.DataFrame(negative_samples_list, columns=['word'])\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "# Step 3: Combine Positive and Negative Samples\n",
    "data = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shape and format of the data\n",
    "print(f\"Combined Data Shape: {data.shape}\")\n",
    "print(f\"Data Format Example:\\n{data.head()}\")\n",
    "\n",
    "# Step 4: Load text2vec-large-chinese model\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Embedding Progress\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 5: Get embeddings for the dataset\n",
    "embeddings = get_text2vec_embeddings(data['word'], text2vec_model)\n",
    "\n",
    "# Print the shape and format of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding Example:\\n{embeddings[0]}\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Print the labels format\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels Example:\\n{labels[:5]}\")\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the split data shapes\n",
    "print(f\"Train Embeddings Shape: {train_embeddings.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Embeddings Shape: {val_embeddings.shape}, Val Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print the data loader batch size\n",
    "print(f\"Train Loader Batch Size: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader Batch Size: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your fashion and non-fashion datasets are in CSV format\n",
    "\n",
    "# Load fashion dataset\n",
    "fashion_df = pd.read_csv('/path/to/fashion_data.csv')\n",
    "# Load non-fashion dataset\n",
    "non_fashion_df = pd.read_csv('/path/to/non_fashion_data.csv')\n",
    "\n",
    "# Check the number of samples in each dataset\n",
    "num_fashion = len(fashion_df)\n",
    "num_non_fashion = len(non_fashion_df)\n",
    "\n",
    "# Print the size of both datasets\n",
    "print(f\"Number of fashion samples: {num_fashion}\")\n",
    "print(f\"Number of non-fashion samples: {num_non_fashion}\")\n",
    "\n",
    "# Check if the sizes are balanced\n",
    "if num_fashion == num_non_fashion:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(\"The datasets are not balanced.\")\n",
    "    print(f\"Difference: {abs(num_fashion - num_non_fashion)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 183/183 [00:02<00:00, 81.26it/s]\n",
      "Validation Progress: 100%|██████████| 46/46 [00:00<00:00, 295.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5419, Val Loss: 0.6195, Accuracy: 0.6869\n",
      "Precision: 0.6384, Recall: 0.9419, F1 Score: 0.7610\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 183/183 [00:02<00:00, 83.71it/s]\n",
      "Validation Progress: 100%|██████████| 46/46 [00:00<00:00, 409.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3753, Val Loss: 0.3509, Accuracy: 0.8578\n",
      "Precision: 0.8529, Recall: 0.8837, F1 Score: 0.8680\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 183/183 [00:02<00:00, 90.38it/s]\n",
      "Validation Progress: 100%|██████████| 46/46 [00:00<00:00, 412.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3124, Val Loss: 0.4529, Accuracy: 0.7963\n",
      "Precision: 0.7254, Recall: 0.9897, F1 Score: 0.8372\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 183/183 [00:02<00:00, 90.41it/s]\n",
      "Validation Progress: 100%|██████████| 46/46 [00:00<00:00, 410.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2721, Val Loss: 0.3878, Accuracy: 0.8312\n",
      "Precision: 0.9189, Recall: 0.7468, F1 Score: 0.8239\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 183/183 [00:02<00:00, 90.49it/s]\n",
      "Validation Progress: 100%|██████████| 46/46 [00:00<00:00, 410.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2293, Val Loss: 0.3180, Accuracy: 0.8906\n",
      "Precision: 0.9126, Recall: 0.8773, F1 Score: 0.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 46/46 [00:00<00:00, 413.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: Accuracy: 0.8906, Precision: 0.9126, Recall: 0.8773, F1 Score: 0.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 5: Define ResNet-Based Binary Classification Model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):  # Adjust embedding_dim based on text2vec output (1024)\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        # Use pre-trained ResNet18, but replace the first conv layer to accept text2vec embeddings\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Binary classification\n",
    "        \n",
    "        # Define a simple MLP layer to map embeddings to ResNet input size\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),  # Match the expected ResNet input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),  # Match the expected ResNet input size after projection\n",
    "        )\n",
    "        \n",
    "        # Adding a Conv layer to convert 1D to a 2D feature map\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "\n",
    "        # Adaptive pooling layer to transform the spatial dimensions to (7, 7)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"Input to forward (embedding): {x.shape}\")\n",
    "        x = self.embedding_to_resnet(x)  # Project text2vec embeddings to match ResNet input size\n",
    "        # print(f\"After embedding_to_resnet: {x.shape}\")\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # Reshape to fit Conv2D (batch_size, channels, height, width)\n",
    "        # print(f\"After unsqueeze: {x.shape}\")\n",
    "        x = self.embedding_conv(x)  # Use conv layer to convert 1D feature to 2D feature map\n",
    "        # print(f\"After embedding_conv: {x.shape}\")\n",
    "        x = self.adaptive_pool(x)  # Apply adaptive pooling to resize to (7, 7)\n",
    "        # print(f\"After adaptive_pool: {x.shape}\")\n",
    "        x = self.resnet(x)\n",
    "        # print(f\"Output of ResNet: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct embedding dimension (1024 as per the output shape)\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "\n",
    "# Step 6: Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Progress\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Progress\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Final evaluation\n",
    "val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "print(f\"Final Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save the model\n",
    "model_path = \"/data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /data1/dxw_data/llm/text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "Generating Embeddings: 100%|██████████| 2/2 [00:00<00:00,  5.67it/s]\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Testing Progress: 100%|██████████| 2/2 [00:00<00:00, 322.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /data1/dxw_data/llm/RA/cuhk_xinyu/dataset/test-dataset-output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/data1/dxw_data/llm/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/test-dataset.csv\"\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['word']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the trained model is saved as 'resnet_binary_classifier.pth'\n",
    "model_path = \"/data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/test-dataset-output.csv\"\n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更多测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Path /data1/dxw_data/llm/text2vec-large-chinese not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data1/dxw_data/llm/text2vec-large-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m text2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Function to get embeddings from text2vec-large-chinese\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text2vec_embeddings\u001b[39m(texts, model):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:274\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_name_or_path):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Not a path, load from hub\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name_or_path))\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n",
      "\u001b[0;31mValueError\u001b[0m: Path /data1/dxw_data/llm/text2vec-large-chinese not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/data1/dxw_data/llm/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_notstyle_dataset.csv\"\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['word']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the trained model is saved as 'resnet_binary_classifier.pth'\n",
    "model_path = \"/data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_notstyle_dataset-output.csv\"\n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /data1/dxw_data/llm/text2vec-large-chinese. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 197/197 [00:07<00:00, 26.29it/s]\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Testing Progress: 100%|██████████| 197/197 [00:00<00:00, 463.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_style_dataset-output.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/data1/dxw_data/llm/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_style_dataset.csv\"\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['word']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the trained model is saved as 'resnet_binary_classifier.pth'\n",
    "model_path = \"/data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/dataset/filtered_style_dataset-output.csv\"\n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于聚类embedding的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /data1/dxw_data/llm/text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Processing Rows: 100%|██████████| 12807/12807 [03:59<00:00, 53.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results saved to /data1/dxw_data/llm/redbook_final/script_next/matching_records_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/data1/dxw_data/llm/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 2: Load the trained ResNet binary classifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Load the CSV file containing rake_keywords\n",
    "csv_file = \"/data1/dxw_data/llm/redbook_final/script_next/matching_records.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 4: Process rake_keywords and filter words based on model prediction\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    # Check if rake_keywords is NaN or empty\n",
    "    if pd.isna(row['rake_keywords']) or not row['rake_keywords'].strip():\n",
    "        filtered_keywords.append([])  # Add an empty list for NaN or invalid rows\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Parse the rake_keywords string into a list of words\n",
    "        words = ast.literal_eval(row['rake_keywords'])\n",
    "        \n",
    "        # Get embeddings for the words\n",
    "        embeddings = get_text2vec_embeddings(words, text2vec_model).to(device)\n",
    "        \n",
    "        # Make predictions using the pre-trained ResNet binary classifier\n",
    "        with torch.no_grad():\n",
    "            outputs = model(embeddings)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Filter words where the model predicts label 1\n",
    "        filtered_words = [word for word, label in zip(words, preds) if label == 1]\n",
    "        filtered_keywords.append(filtered_words)\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        # If parsing fails, append an empty list and continue\n",
    "        print(f\"Error parsing rake_keywords in row {idx}: {e}\")\n",
    "        filtered_keywords.append([])\n",
    "\n",
    "# Step 5: Add the new column filter_label and save the DataFrame\n",
    "df['filter_label'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/data1/dxw_data/llm/redbook_final/script_next/matching_records_filtered.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /data1/dxw_data/llm/text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dxw/anaconda3/envs/agent/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Processing Rows: 100%|██████████| 500/500 [07:16<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results saved to /data1/dxw_data/llm/redbook_final/script_next/matching_records_filtered_500.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/data1/dxw_data/llm/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 2: Load the trained ResNet binary classifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/data1/dxw_data/llm/RA/cuhk_xinyu/resnet_binary_classifier.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Load the CSV file containing rake_keywords, limiting to the first 500 rows\n",
    "csv_file = \"/data1/dxw_data/llm/redbook_final/script_next/matching_records.csv\"\n",
    "df = pd.read_csv(csv_file, nrows=500)  # Only load the first 500 rows\n",
    "\n",
    "# Step 4: Ensure that we split rake_keywords correctly into individual words\n",
    "def split_keywords(keywords_string):\n",
    "    # Split by spaces or other separators, depending on the actual format\n",
    "    return keywords_string.split()\n",
    "\n",
    "# Step 5: Process rake_keywords and filter words based on model prediction\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    # Check if rake_keywords is NaN or empty\n",
    "    if pd.isna(row['rake_keywords']) or not row['rake_keywords'].strip():\n",
    "        filtered_keywords.append([])  # Add an empty list for NaN or invalid rows\n",
    "        continue  # Skip NaN or invalid rows\n",
    "    \n",
    "    try:\n",
    "        # Split the rake_keywords string into a list of words\n",
    "        words = split_keywords(row['rake_keywords'])\n",
    "        \n",
    "        # Ensure each word is processed individually\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            # Get embedding for the word\n",
    "            embeddings = get_text2vec_embeddings([word], text2vec_model).to(device)\n",
    "            \n",
    "            # Make predictions using the pre-trained ResNet binary classifier\n",
    "            with torch.no_grad():\n",
    "                outputs = model(embeddings)\n",
    "                pred = torch.argmax(outputs, dim=1).cpu().numpy()[0]\n",
    "            \n",
    "            # If the prediction label is 1, append the word to the filtered list\n",
    "            if pred == 1:\n",
    "                filtered_words.append(word)\n",
    "\n",
    "        filtered_keywords.append(filtered_words)\n",
    "\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        print(f\"Error parsing rake_keywords in row {idx}: {e}\")\n",
    "        filtered_keywords.append([])\n",
    "\n",
    "# Step 6: Add the new column filter_label and save the DataFrame\n",
    "df['filter_label'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/data1/dxw_data/llm/redbook_final/script_next/matching_records_filtered_500.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'matching_records_filtered_500.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: 读取文件\u001b[39;00m\n\u001b[1;32m      4\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching_records_filtered_500.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: 随机采样30行\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sampled_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# 随机采样30行\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'matching_records_filtered_500.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "csv_file = \"/data1/dxw_data/llm/Multimodal-MKT/label/text_predict/dataset/matching_records_filtered_500.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: 随机采样30行\n",
    "sampled_df = df.sample(n=30, random_state=42)  # 随机采样30行\n",
    "\n",
    "# Step 3: 去重函数，保持顺序不变\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: 打印输出其中的 summary_cleaned 列和 filter_label 列，去掉 filter_label 中的重复单词\n",
    "for i, row in sampled_df.iterrows():\n",
    "    summary_cleaned = eval(row['summary_cleaned'])  # 转换为列表\n",
    "    filter_label = eval(row['filter_label'])  # 转换为列表\n",
    "    filter_label_unique = remove_duplicates(filter_label)  # 去重\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"summary_cleaned: {summary_cleaned}\")\n",
    "    print(f\"filter_label: {filter_label_unique}\")\n",
    "    print()  # 空行用于美观输出\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
