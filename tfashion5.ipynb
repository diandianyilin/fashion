{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion-related samples (positive): 52242\n",
      "Number of non-fashion-related samples (negative): 304611\n",
      "The datasets are not balanced. Difference: 252369\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load fashion-related data (positive samples)\n",
    "fashion_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword' column which contains fashion-related keywords\n",
    "fashion_samples = fashion_df['keyword group']\n",
    "\n",
    "# Directory containing non-fashion-related samples\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "\n",
    "# Initialize an empty list to store non-fashion samples\n",
    "non_fashion_samples = []\n",
    "\n",
    "# Recursively iterate through all subdirectories and files in non_fashion_texts directory\n",
    "for root, dirs, files in os.walk(neg_dir):\n",
    "    for file in files:\n",
    "        # Read only text files\n",
    "        if file.startswith('wiki'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Add each line as a sample\n",
    "                lines = f.readlines()\n",
    "                non_fashion_samples.extend([line.strip() for line in lines if line.strip()])\n",
    "\n",
    "# Convert non-fashion samples to a pandas Series for consistency\n",
    "non_fashion_samples = pd.Series(non_fashion_samples)\n",
    "\n",
    "# Step 2: Count the number of samples in each dataset\n",
    "num_fashion_samples = fashion_samples.shape[0]\n",
    "num_non_fashion_samples = non_fashion_samples.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of fashion-related samples (positive): {num_fashion_samples}\")\n",
    "print(f\"Number of non-fashion-related samples (negative): {num_non_fashion_samples}\")\n",
    "\n",
    "# Step 3: Check if they are balanced\n",
    "if num_fashion_samples == num_non_fashion_samples:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(f\"The datasets are not balanced. Difference: {abs(num_fashion_samples - num_non_fashion_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling:\n",
    "- randomly reducing the number of samples from the majority class (non-fashion-related samples) to match the number of samples in the minority class (fashion-related samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downsampled non-fashion-related samples: 52242\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# If the non-fashion dataset is larger, downsample it\n",
    "if num_non_fashion_samples > num_fashion_samples:\n",
    "    # Randomly select `num_fashion_samples` non-fashion samples\n",
    "    downsampled_non_fashion_samples = non_fashion_samples.sample(n=num_fashion_samples, random_state=42)\n",
    "else:\n",
    "    downsampled_non_fashion_samples = non_fashion_samples\n",
    "\n",
    "# Print the number of samples after downsampling\n",
    "print(f\"Number of downsampled non-fashion-related samples: {downsampled_non_fashion_samples.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read Data Files\n",
    "- read positive samples\n",
    "- read negative samples\n",
    "- combine the datasets\n",
    "#### Step 2: Load pre-trained sentence Transformer Model\n",
    "- load text2vec-large-chinese model\n",
    "- generate embeddings for each word\n",
    "#### Step 3: Get embeddings for the dataset\n",
    "- apply the embeddings function\n",
    "- convert labels to tensor\n",
    "#### Step 4: Train-test split\n",
    "- split the data\n",
    "- create PyTorh TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (356855, 2)\n",
      "Data Format Example:\n",
      "                        word  label\n",
      "0                      歷史簡介.      0\n",
      "1       目前运行于该平台下的分布式计算项目如下：      0\n",
      "2                         胸贴      1\n",
      "3  以下是近年巴塞隆拿於聯賽對陣皇家馬德里的對賽成績：      0\n",
      "4                        歷史.      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Progress:  92%|█████████▏| 10294/11152 [16:39<01:20, 10.70it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Read Positive Samples from 'topics_filtered.csv'\n",
    "# Load the fashion lexicon (positive samples)\n",
    "pos_file = \"/home/disk1/red_disk1/fashion/topics_filtered.csv\"\n",
    "positive_samples_df = pd.read_csv(pos_file)\n",
    "positive_samples = positive_samples_df['keyword group'].dropna().tolist()  # Extract keywords column\n",
    "\n",
    "# Assign label 1 to positive samples\n",
    "positive_samples = pd.DataFrame(positive_samples, columns=['word'])\n",
    "positive_samples['label'] = 1\n",
    "\n",
    "# Step 2: Read Negative Samples from non_fashion_texts\n",
    "def read_negative_samples(directory):\n",
    "    all_lines = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    all_lines.extend([line.strip() for line in file if line.strip() != \"\"])\n",
    "    return all_lines\n",
    "\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "negative_samples_list = read_negative_samples(neg_dir)\n",
    "\n",
    "# Assign label 0 to negative samples\n",
    "negative_samples = pd.DataFrame(negative_samples_list, columns=['word'])\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "# Step 3: Combine Positive and Negative Samples\n",
    "data = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shape and format of the data\n",
    "print(f\"Combined Data Shape: {data.shape}\")\n",
    "print(f\"Data Format Example:\\n{data.head()}\")\n",
    "\n",
    "# Step 4: Load text2vec-large-chinese model\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Embedding Progress\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 5: Get embeddings for the dataset\n",
    "embeddings = get_text2vec_embeddings(data['word'], text2vec_model)\n",
    "\n",
    "# Print the shape and format of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding Example:\\n{embeddings[0]}\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Print the labels format\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels Example:\\n{labels[:5]}\")\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the split data shapes\n",
    "print(f\"Train Embeddings Shape: {train_embeddings.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Embeddings Shape: {val_embeddings.shape}, Val Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print the data loader batch size\n",
    "print(f\"Train Loader Batch Size: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader Batch Size: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a ResNet-based binary classification model\n",
    "1. Model Definition: ResNet-Based Binary Classifier\n",
    "2. Training Loop\n",
    "3. Validation Loop\n",
    "4. Training and Evaluation\n",
    "\n",
    "#### Summary\n",
    "- Model Setup: It initializes a ResNet18 model tailored for binary classification using text embeddings as inputs.\n",
    "- Training and Validation: The code implements a standard training loop with backpropagation and uses validation to track performance.\n",
    "- Performance Metrics: Accuracy, precision, recall, and F1 score are calculated to evaluate the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.66it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 345.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0922, Val Loss: 0.0618, Accuracy: 0.9754\n",
      "Precision: 0.9172, Recall: 0.9143, F1 Score: 0.9158\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.64it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 355.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0546, Val Loss: 0.0586, Accuracy: 0.9775\n",
      "Precision: 0.9595, Recall: 0.8829, F1 Score: 0.9196\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:49<00:00, 38.82it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 408.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411, Val Loss: 0.0471, Accuracy: 0.9813\n",
      "Precision: 0.9461, Recall: 0.9245, F1 Score: 0.9352\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:49<00:00, 38.86it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 354.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0321, Val Loss: 0.0481, Accuracy: 0.9826\n",
      "Precision: 0.9419, Recall: 0.9390, F1 Score: 0.9404\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.74it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 400.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0262, Val Loss: 0.0526, Accuracy: 0.9829\n",
      "Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 434.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: Accuracy: 0.9829, Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 7: Define ResNet-Based Binary Classification Model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):  # Adjust embedding_dim based on text2vec output (1024)\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        # Use pre-trained ResNet18, but replace the first conv layer to accept text2vec embeddings\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Binary classification\n",
    "        \n",
    "        # Define a simple MLP layer to map embeddings to ResNet input size\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),  # Match the expected ResNet input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),  # Match the expected ResNet input size after projection\n",
    "        )\n",
    "        \n",
    "        # Adding a Conv layer to convert 1D to a 2D feature map\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "\n",
    "        # Adaptive pooling layer to transform the spatial dimensions to (7, 7)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)  # Project text2vec embeddings to match ResNet input size\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # Reshape to fit Conv2D (batch_size, channels, height, width)\n",
    "        x = self.embedding_conv(x)  # Use conv layer to convert 1D feature to 2D feature map\n",
    "        x = self.adaptive_pool(x)  # Apply adaptive pooling to resize to (7, 7)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct embedding dimension (1024 as per the output shape)\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "\n",
    "# Step 8: Define Training Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Progress\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Progress\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "# Step 9: Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 10: Final Evaluation\n",
    "val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "print(f\"Final Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /home/disk1/red_disk1/fashion/tfashion\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Save the trained model\n",
    "\n",
    "# Save model and tokenizer to the desired directory\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion\"\n",
    "\n",
    "# Make the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, \"tfashion.pth\"))\n",
    "\n",
    "# Optionally, save additional model metadata if needed (e.g., optimizer state, epoch)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(model_save_path, \"tfashion_checkpoint.pth\"))\n",
    "\n",
    "print(f\"Model saved at: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare post data\n",
    "- Combine 'post_title' and 'post_content' into 'post_text'\n",
    "- Clean 'post_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean.csv')\n",
    "# df = pd.read_csv('/home/disk1/red_disk1/poster_9305.csv')\n",
    "\n",
    "# Combine 'post_title' and 'post_content' into 'post_text'\n",
    "df['post_text'] = df['post_title'].fillna('') + ' ' + df['post_content'].fillna('')\n",
    "\n",
    "# Drop 'post_title' and 'post_content' columns\n",
    "df = df.drop(columns=['post_title', 'post_content', 'post_tag'])\n",
    "\n",
    "# Save the updated DataFrame\n",
    "# df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_9305_combined.csv', index=False)\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "# Load stopwords from the provided file\n",
    "with open('/home/disk1/red_disk1/fashion/stopwords_cn.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Function for text cleaning\n",
    "def clean_text(text, stopwords):\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove specific patterns\n",
    "    text = re.sub(r'- 小红书,,', '', text)  # Removing \"- 小红书,,\"\n",
    "    text = re.sub(r'小红书', '', text)  # Explicitly remove \"小红书\"\n",
    "    text = re.sub(r',,\\d{2}-\\d{2},,', '', text)  # Removing patterns like \",,XX-XX,,\"\n",
    "    text = re.sub(r'#', ' ', text)  # Replace '#' with a space\n",
    "    text = re.sub(r'\\s+', '', text)  # This will remove all whitespace characters\n",
    "\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    # Remove stopwords (word-based removal)\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopwords])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function to 'post_text'\n",
    "df['cleaned_post_text'] = df['post_text'].apply(lambda x: clean_text(str(x), stopwords))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('/home/disk1/red_disk1/fashion/post_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional) Test the model tfashion\n",
    "- Step 1: Load the test dataset.\n",
    "- Step 2: Generate embeddings for each word in the test dataset using text2vec-large-chinese.\n",
    "- Step 3: Use a pre-trained ResNet-based classifier model to predict whether each word is related to fashion or not.\n",
    "- Step 4: Save the predictions alongside the original data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/disk1/red_disk1/fashion/text2vec-large-chinese.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/disk1/red_disk1/fashion/text2vec-large-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m text2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Function to get embeddings from text2vec-large-chinese\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text2vec_embeddings\u001b[39m(texts, model):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:306\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    294\u001b[0m         modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    295\u001b[0m             model_name_or_path,\n\u001b[1;32m    296\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    319\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:1454\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[1;32m   1452\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[0;32m-> 1454\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1461\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:56\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     55\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     59\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:87\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:3504\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3500\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3501\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3502\u001b[0m         )\n\u001b[1;32m   3503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3505\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3506\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3507\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3508\u001b[0m         )\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3510\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/disk1/red_disk1/fashion/text2vec-large-chinese."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"  # Update this path if necessary\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['cleaned_post_text']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained tfashion model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained tfashion model from your specified path\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test-output.csv\"  \n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the model\n",
    "1. Tokenization: tokenize the post_text content\n",
    "2. Word Embedding: pass each word (instead of the entire post) through the text2vec-large-chinese model to generate embeddings for each word\n",
    "3. Classification: pass embeddings through the tfashion model to classify whether each word is fashion-related or not\n",
    "4. Filter out any words that are present in the stopwords set before applying the model classification\n",
    "5. Filter out single-character words\n",
    "6. Results: The output will be a classification for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/disk1/red_disk1/fashion/text2vec-large-chinese.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 2: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/disk1/red_disk1/fashion/text2vec-large-chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m text2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 3: Define ResNetBinaryClassifier\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResNetBinaryClassifier\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:306\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    294\u001b[0m         modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    295\u001b[0m             model_name_or_path,\n\u001b[1;32m    296\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    319\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:1454\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[1;32m   1452\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[0;32m-> 1454\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1461\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:56\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     55\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     59\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:87\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:3504\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3500\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3501\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3502\u001b[0m         )\n\u001b[1;32m   3503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   3505\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME, variant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3506\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3507\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3508\u001b[0m         )\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, pretrained_model_name_or_path)):\n\u001b[1;32m   3510\u001b[0m     archive_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /home/disk1/red_disk1/fashion/text2vec-large-chinese."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Step 1: Load the stopwords file\n",
    "stopwords_file = \"/home/disk1/red_disk1/fashion/stopwords_cn.txt\"\n",
    "with open(stopwords_file, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "# Step 2: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese/models--GanymedeNil--text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 3: Define ResNetBinaryClassifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Load the CSV file containing post_text\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/post_cleaned.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 5: Tokenize the post_text content into words and remove stopwords\n",
    "def tokenize_text(text):\n",
    "    words = list(jieba.cut(text))\n",
    "    # Retain English words using regex (keeping words with ASCII characters)\n",
    "    english_words = re.findall(r'[a-zA-Z]+', text)\n",
    "    # Filter out stopwords and single-character Chinese words\n",
    "    filtered_words = [word for word in words if word not in stopwords and len(word) > 1]\n",
    "    return filtered_words + english_words  # Keep both filtered Chinese and English words\n",
    "\n",
    "df['tokenized_text'] = df['cleaned_post_text'].apply(tokenize_text)\n",
    "\n",
    "# Step 6: Get embeddings for each word in the tokenized_text column and classify them\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    words = row['tokenized_text']\n",
    "    \n",
    "    # Get embeddings for the words\n",
    "    embeddings = get_text2vec_embeddings(words, text2vec_model).to(device)\n",
    "    \n",
    "    # Make predictions using the pre-trained ResNet binary classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Filter words where the model predicts label 1 (fashion-related), and keep English words\n",
    "    filtered_words = [word for word, label in zip(words, preds) if label == 1 or re.match(r'[a-zA-Z]+', word)]\n",
    "    filtered_keywords.append(filtered_words)\n",
    "\n",
    "# Step 7: Add the new column filtered_keywords and save the DataFrame\n",
    "df['filtered_keywords2'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 12997:\n",
      "summary_cleaned: ['尊嘟', 'redexclamationmark', '富贵', '小姐姐', '尝试', '温柔', '气质', '姐姐', '富家', '千金', '秋冬', '温柔', '慵懒', '毛衣', '慵懒', '风穿', '高级', '感穿']\n",
      "filter_label: ['富贵', '小姐姐', '尝试', '温柔', '气质', '姐姐', '富家', '千金', '秋冬', '慵懒', '毛衣', '风穿', '高级', '感穿']\n",
      "\n",
      "Example 11585:\n",
      "summary_cleaned: ['温柔', '白月光', '感觉', '试试', '仙气', '裙子', '中式', '民国', '温柔', '连衣裙', '国风', '针织', '中式', '套装']\n",
      "filter_label: ['温柔', '白月光', '感觉', '试试', '仙气', '裙子', '中式', '民国', '连衣裙', '国风', '针织', '套装']\n",
      "\n",
      "Example 11226:\n",
      "summary_cleaned: ['我先', '微胖', '女孩', '微胖']\n",
      "filter_label: ['微胖', '女孩']\n",
      "\n",
      "Example 13552:\n",
      "summary_cleaned: ['redapple', '跟着', '模特', 'keycapkeycap', '复古', '百褶裙', '太好', '身高', 'cm', '体重', '斤斤', '小腿', 'cm', '大腿', 'cm', '腰围', 'cm', '肚围', 'cmredapple', '身材', '我太爱', '这件', '短裙', '几年', '尝试', '百褶裙', '没想到', '感觉', '不错', '哈哈哈', '材质', '材质', '质感', '苹果', '身材', '苹果', '小个子', '短裙', '短裙', '格子裙', '百褶裙', '百褶裙', '百褶裙', '格子', '百褶裙', '复古', '百褶裙', '复古', '格子', '半裙', '秋季', '秋季', '新款', '跟着', '模特', '学穿', '拉德', '苹果', '身材', '小个子', '苹果', '身材', '苹果', '身材', '指南', '苹果', '身材', '显瘦', '苹果', '身材', '微胖', '搭微', '苹果', '身材', '针织', '上衣', '舒服', '针织衫', '搭百搭', '针织', '上衣', '早秋', '必备', '针织衫']\n",
      "filter_label: ['redapple', '跟着', '模特', '复古', '百褶裙', '太好', '身高', '体重', '斤斤', '小腿', '大腿', '腰围', '肚围', '身材', '我太爱', '这件', '短裙', '几年', '尝试', '没想到', '感觉', '不错', '哈哈哈', '材质', '质感', '苹果', '小个子', '格子裙', '格子', '半裙', '秋季', '新款', '学穿', '拉德', '指南', '显瘦', '微胖', '搭微', '针织', '上衣', '舒服', '针织衫', '搭百搭', '早秋', '必备']\n",
      "\n",
      "Example 100:\n",
      "summary_cleaned: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '真的', '好看', '质感', '超棒', '软软', '外面', '搭个', '大衣', '妥妥', '韩剧', '女主', '疯狂', '爱上', '大学生', '大衣', '毛衣', '日常', '气质', '秋冬', '小个子', '秋冬', '毛衣']\n",
      "filter_label: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '好看', '质感', '超棒', '软软', '外面', '搭个', '大衣', '韩剧', '女主', '疯狂', '爱上', '大学生', '日常', '气质', '秋冬', '小个子']\n",
      "\n",
      "Example 9914:\n",
      "summary_cleaned: ['万圣节', '店里', '新品', '万圣节', '每日', '哥特']\n",
      "filter_label: ['万圣节', '店里', '新品', '每日', '哥特']\n",
      "\n",
      "Example 15089:\n",
      "summary_cleaned: ['身材', '巨巨', '巨好', 'doubleexclamationmark', '微胖', '女生', '大胆', '冬天', '真的', '梨形', '身材', '微胖', '女孩', '微胖', '日常', '梨形', '身材', '梨形', '微胖', '梨形', '身材', '梨形', '梨形', '微胖', '秋冬', '冬季', '微胖', '女生', '冬季']\n",
      "filter_label: ['身材', '巨巨', '巨好', '微胖', '女生', '大胆', '冬天', '真的', '梨形', '女孩', '日常', '秋冬', '冬季']\n",
      "\n",
      "Example 2309:\n",
      "summary_cleaned: ['基础', 'Tee', '一衣', '法则', '夏天', '少不了', '基础', 'T恤', '花样', '选择', '三条', '类型', '裤子', '搭配', '裤型', '搭配', '基础', 'tee', '发挥', '可玩性', 'ootd', '每日', '男生', '开春', '基础', '短袖', 'osmosismocuishleESOTERICInexistence', '存世', 'GaforReal']\n",
      "filter_label: ['基础', 'Tee', '一衣', '法则', '夏天', '少不了', 'T恤', '花样', '选择', '类型', '裤子', '搭配', '裤型', 'tee', '发挥', '可玩性', 'ootd', '每日', '男生', '开春', '短袖', 'GaforReal']\n",
      "\n",
      "Example 4101:\n",
      "summary_cleaned: ['只美', '拉德', '包包', 'stuffedflatbread', '复古', '棕色', '百搭', 'mate', '大容量', '棕色', 'mediumdarkskintone', '包包', '撞色', '巧克力', '包包', 'chocolatebar', '羊羔', '包包', 'ewe', '牛仔', '刺绣', '拼接', '棕色', '包包', 'sewingneedle', '哭喊', '中心', '复古', '格纹', '帆布包', 'burrito', '波士顿', '包型', '复古', '腋下', 'meatonbone', '包包', '偏爱', '小众', '包小众', '包包', '大学生', '上课', '包包', '腋下', '帆布包', '拉德', '棕色', '包包', '复古', '包包', '秋冬', '百搭', '包包', '通勤', '包包', '包包', '分享', '宝宝', '辅食', '包包', '重样', '大容量', '包包']\n",
      "filter_label: ['只美', '拉德', '包包', '复古', '棕色', '百搭', 'mate', '大容量', '撞色', '巧克力', 'chocolatebar', '羊羔', '牛仔', '刺绣', '拼接', 'sewingneedle', '中心', '格纹', '帆布包', '包型', '腋下', 'meatonbone', '偏爱', '小众', '包小众', '大学生', '上课', '秋冬', '通勤', '分享', '宝宝', '辅食', '重样']\n",
      "\n",
      "Example 4394:\n",
      "summary_cleaned: ['夏天', '粉色', 'growingheart', '粉色', '中年妇女', 'ootd', '每日', '日常', '每日', '粉色', '粉色', '少女']\n",
      "filter_label: ['夏天', '粉色', 'growingheart', '中年妇女', 'ootd', '每日', '日常', '少女']\n",
      "\n",
      "Example 12496:\n",
      "summary_cleaned: ['回村', '冬季']\n",
      "filter_label: ['冬季']\n",
      "\n",
      "Example 2827:\n",
      "summary_cleaned: ['条纹', '简约', '长袖', 'doubleexclamationmarkMalaysiains', '条纹', '长袖', '短版', '件套', '平价', '好物', '挑战', '星期', '重样', '笔记', '灵感', '马来西亚', '服装店']\n",
      "filter_label: ['条纹', '简约', '长袖', 'doubleexclamationmarkMalaysiains', '短版', '件套', '平价', '好物', '挑战', '重样', '笔记', '灵感', '服装店']\n",
      "\n",
      "Example 15129:\n",
      "summary_cleaned: ['一夜', '爆火', '套路', '六一', '普通人', '博主穿', 'ootd']\n",
      "filter_label: ['一夜', '爆火', '六一', '普通人', '博主穿', 'ootd']\n",
      "\n",
      "Example 9491:\n",
      "summary_cleaned: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '鸭绒', '这件', '羽绒服', '棒球', '款式', '时尚', '保暖', 'smilingfacewithhearts', '厚度', '不算', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '羽绒服', '短款', '羽绒服', '羽绒服', '推荐', '穿着', '还显', '羽绒服']\n",
      "filter_label: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '鸭绒', '这件', '款式', '时尚', '保暖', '厚度', '不算', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '短款', '推荐', '穿着', '还显']\n",
      "\n",
      "Example 4194:\n",
      "summary_cleaned: ['平价', '显瘦', '针织衫', '三十多', 'moneybag', '版型', '好好', '温柔', '氛围', '软乎乎', '不扎', '弹力', '起球', '针织衫', '显瘦', '平价', '好物', '梨形', '身材', '秋冬', '平价', '好物', '学生']\n",
      "filter_label: ['平价', '显瘦', '针织衫', '三十多', '版型', '好好', '温柔', '氛围', '软乎乎', '不扎', '弹力', '起球', '好物', '梨形', '身材', '秋冬', '学生']\n",
      "\n",
      "Example 2920:\n",
      "summary_cleaned: ['宜家', '镜子', '好好看', '记录', '生活', '甜妹', '每日', '软妹', '宜家', '宜家', '拍照', '宜家', '好物']\n",
      "filter_label: ['镜子', '好好看', '记录', '生活', '甜妹', '每日', '软妹', '拍照', '好物']\n",
      "\n",
      "Example 3775:\n",
      "summary_cleaned: ['eplus', '姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', 'doubleexclamationmark', '浪漫生活', '记录', '笔记', '灵感', '服装', '测评', '套装', '平价', '衣服', '测评', '初秋']\n",
      "filter_label: ['姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', '浪漫生活', '记录', '笔记', '灵感', '服装', '平价', '衣服']\n",
      "\n",
      "Example 4534:\n",
      "summary_cleaned: ['几十块', 'lulu', '同款', '外套', '白色', '筒裤', '太显', '简穿', '超会', '企划', 'lulu', '平替', 'ootd', '每日', '显瘦', 'ootd', '白色', '牛仔裤', '拍照', '姿势', '重样', 'lulu', '黑色']\n",
      "filter_label: ['几十块', 'lulu', '同款', '外套', '白色', '筒裤', '太显', '简穿', '超会', '企划', '平替', 'ootd', '每日', '显瘦', '牛仔裤', '拍照', '姿势', '重样', '黑色']\n",
      "\n",
      "Example 10565:\n",
      "summary_cleaned: ['Meetingyouistrulyamiracleofmylif', '六芒星', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '生命', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '手镯', '手工', '不可思议', '事儿', '银手镯', '手工', '银饰', '首饰', 'blingbling', '晒晒', '手上', '手工', '手镯']\n",
      "filter_label: ['Meetingyouistrulyamiracleofmylif', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '生命', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '不可思议', '事儿', '银手镯', '银饰', '首饰', 'blingbling', '晒晒', '手上']\n",
      "\n",
      "Example 7473:\n",
      "summary_cleaned: ['微胖', '妹妹', '我选', '简简单单', '耐看', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '不露', '美杏色', '连衣裙', '气质', '代表', '腰部', '褶皱', '肚子', '简单', '气质', '爸妈', '裙子', '安利', '第一张', '照片', '笔记', '灵感', '夏日', '宝藏', '裙子', '蓬蓬裙', '微胖', '显瘦', '显瘦', '连衣裙', '连衣裙', '气质', '连衣裙']\n",
      "filter_label: ['微胖', '妹妹', '我选', '简简单单', '耐看', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '不露', '美杏色', '气质', '腰部', '褶皱', '肚子', '简单', '爸妈', '裙子', '安利', '第一张', '照片', '笔记', '灵感', '宝藏', '蓬蓬裙', '显瘦']\n",
      "\n",
      "Example 6619:\n",
      "summary_cleaned: ['cm', '美式', '复古', '运动', '女孩', '宽松', '紧身', '缺一不可', '小个子', '美式', '辣妹', '日常', '冬季']\n",
      "filter_label: ['美式', '复古', '运动', '女孩', '宽松', '紧身', '缺一不可', '小个子', '辣妹', '日常', '冬季']\n",
      "\n",
      "Example 5295:\n",
      "summary_cleaned: ['教师', '梨形', '身材', '这条', '神裤', '身上', '衬衫', '裤子', '真的', '清爽', '耐看', '氧气', '十足', '梨形', '身材', '找到', '本命', '神裤', '腰围', '臀围', '腰围', '刚刚', '臀围', '放量', '很大', '真的', '超显', '教师', '梨形', '身材', '裤子', '白色', '裤子', 'yyds', '法式', '复古', '裤子', '气质', '搭薯', '队长', '管家', '潮流', '时尚']\n",
      "filter_label: ['教师', '梨形', '身材', '这条', '神裤', '身上', '衬衫', '裤子', '真的', '清爽', '耐看', '氧气', '十足', '找到', '本命', '腰围', '臀围', '刚刚', '放量', '很大', '超显', '白色', 'yyds', '法式', '复古', '气质', '搭薯', '队长', '管家', '潮流', '时尚']\n",
      "\n",
      "Example 8590:\n",
      "summary_cleaned: ['学生', '秋冬', '百搭', '长裤', 'jeans', '显高显', '显腿直', '神裤', 'keycapkeycapkeycapkeycapkeycapkeycap', '裤子', '长裤', '秋冬', '长裤', '百搭显', '神裤', '不露', '高个子', '裤子', '加绒', '裤子']\n",
      "filter_label: ['学生', '秋冬', '百搭', '长裤', '显高显', '显腿直', '神裤', '裤子', '百搭显', '不露', '高个子', '加绒']\n",
      "\n",
      "Example 14843:\n",
      "summary_cleaned: ['黑色', '每日']\n",
      "filter_label: ['黑色', '每日']\n",
      "\n",
      "Example 14141:\n",
      "summary_cleaned: ['小个子', '女生', '这套', 'COOLbutton', '今日', '快乐', '今日', '发起', '穿卫衣', '好开心', '穿卫衣', '搭配', '牛仔裤', '喜欢', '深色', '牛仔裤', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '日常', '小个子', '女生', 'ootd', '每日', '显瘦', '气质', '裤子', '女生']\n",
      "filter_label: ['小个子', '女生', '这套', 'COOLbutton', '今日', '快乐', '发起', '穿卫衣', '好开心', '搭配', '牛仔裤', '喜欢', '深色', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '日常', 'ootd', '显瘦', '气质', '裤子']\n",
      "\n",
      "Example 11979:\n",
      "summary_cleaned: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '空空', '一身', '粉粉', '吊带', '低腰裤', '甜心', '辣妹', 'yk', '辣妹', '日常']\n",
      "filter_label: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '一身', '粉粉', '吊带', '低腰裤', 'yk', '日常']\n",
      "\n",
      "Example 11515:\n",
      "summary_cleaned: ['carrot', '裙子', '喜欢', '安排', '做好', '样衣', '正式', '准确', '实物', '一张', '效果', '影响', '观看', '配色', 'Lolita']\n",
      "filter_label: ['carrot', '裙子', '喜欢', '安排', '做好', '样衣', '正式', '准确', '实物', '一张', '效果', '影响', '观看', '配色', 'Lolita']\n",
      "\n",
      "Example 5370:\n",
      "summary_cleaned: ['贵气感', '源头', '血气方刚', '贵气感', '多贵', '身上', '那种', '战斗力', '身上', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '喜欢', '熬夜', '喝酒', '那阵子', '真的', '元气大伤', '一阵子', '气血', '真的', '感觉', '有用', '脸上', '皮肤', '透亮', '气血', '涌上来', '那种', '红润', '阿胶', '液体', '阿胶', '口服液', '听说', '液体', '更好', '吸收', '福站', '口服液', '一瓶', 'ml', '阿胶', '成分', 'ml', '剩下', '当归', '党参', '气血', '双补', '阳完', '真的', '感冒', '频繁', '福站', '阿胶', '口服液', '整年', '去年', '十月', '平时', '出去玩', '小小的', '一只', '放在', '包里', '没想到', '效果', 'ym', '不调', '姐妹', '想到', '天天', '喝酒', 'ym', '每次', '特别', '规律', '显得', '贵气', '气质', '场上', '方法', '简单', '缺什么', '精神状态', '喝酒', '微醺', '阿胶', '口服液', '拍照', '日常', '享受', '生活', '气血', '喝酒', '微醺', '氛围', '辣妹']\n",
      "filter_label: ['贵气感', '多贵', '身上', '那种', '战斗力', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '熬夜', '喝酒', '那阵子', '真的', '一阵子', '气血', '感觉', '有用', '皮肤', '透亮', '涌上来', '液体', '口服液', '听说', '更好', '吸收', '一瓶', '成分', '剩下', '双补', '感冒', '频繁', '整年', '去年', '平时', '出去玩', '小小的', '一只', '放在', '包里', '没想到', '效果', '不调', '姐妹', '想到', '天天', '每次', '特别', '显得', '贵气', '场上', '方法', '简单', '微醺', '拍照', '日常', '享受', '生活', '氛围', '辣妹']\n",
      "\n",
      "Example 2141:\n",
      "summary_cleaned: ['我会', '香云纱', '改良', '汉服', '香云纱', '改良', '日常', '汉服', '拒绝', '繁琐', '一套', '出门', '香云纱', '香云纱', '新国潮', '中式', '改良', '汉服', '汉服', '汉服', '汉服', '日常', '中式']\n",
      "filter_label: ['我会', '香云纱', '改良', '汉服', '日常', '拒绝', '繁琐', '一套', '出门', '新国潮', '中式']\n",
      "\n",
      "Example 1164:\n",
      "summary_cleaned: ['山本', '裤后', '春夏', '神裤', '这肤', '舒服', '啊啊啊', '真的', '舒服', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '起球', '不易', '着实', '无可挑剔', '神裤', '啊啊啊', '感觉', '必大', '爆神裤', '裤子', '裤子', '种草', '裤子', '显瘦', '神裤', '粉色', '裤子', '小个子', '裤子', '梨型', '身材', '裤子', '百搭神裤', '夏季', '裤子', 'OotD', '每日', 'ootd', '每日', '春夏', '职场', '通勤', '日常', '显瘦', '裤子', '微胖', '女孩', '夏日', '休闲', '慵懒', '裤子', '裤子', '推荐', '工人', '神裤', '垂感', '裤子', '梨形', '身材', '裤子', '真的']\n",
      "filter_label: ['山本', '裤后', '春夏', '神裤', '这肤', '舒服', '啊啊啊', '真的', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '起球', '不易', '着实', '无可挑剔', '感觉', '爆神裤', '裤子', '种草', '显瘦', '粉色', '小个子', '梨型', '身材', '百搭神裤', '夏季', 'OotD', '每日', 'ootd', '职场', '通勤', '日常', '微胖', '女孩', '夏日', '休闲', '慵懒', '推荐', '工人', '垂感', '梨形']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: 随机采样30行\n",
    "sampled_df = df.sample(n=30, random_state=42)  # 随机采样30行\n",
    "\n",
    "# Step 3: 去重函数，保持顺序不变\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: 打印输出其中的 summary_cleaned 列和 filter_label 列，去掉 filter_label 中的重复单词\n",
    "for i, row in sampled_df.iterrows():\n",
    "    summary_cleaned = eval(row['tokenized_text'])  # 转换为列表\n",
    "    filter_label = eval(row['filtered_keywords2'])  # 转换为列表\n",
    "    filter_label_unique = remove_duplicates(filter_label)  # 去重\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"summary_cleaned: {summary_cleaned}\")\n",
    "    print(f\"filter_label: {filter_label_unique}\")\n",
    "    print()  # 空行用于美观输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 12997:\n",
      "Original post_text: 尊嘟很爱redexclamationmark她们说这是富贵小姐姐穿搭尝试一个新look温柔气质大姐姐富家千金穿搭秋冬温柔慵懒风毛衣慵懒风穿搭高级感穿搭\n",
      "Filtered keywords: ['富贵', '小姐姐', '尝试', '温柔', '气质', '姐姐', '富家', '千金', '秋冬', '慵懒', '毛衣', '风穿', '高级', '感穿']\n",
      "\n",
      "Example 11585:\n",
      "Original post_text: 好爱这种温柔白月光的感觉一定要试试仙气的裙子新中式穿搭民国风温柔连衣裙国风针织新中式套装\n",
      "Filtered keywords: ['温柔', '白月光', '感觉', '试试', '仙气', '裙子', '中式', '民国', '连衣裙', '国风', '针织', '套装']\n",
      "\n",
      "Example 11226:\n",
      "Original post_text: 我先成为我自己微胖女孩微胖穿搭\n",
      "Filtered keywords: ['微胖', '女孩']\n",
      "\n",
      "Example 13552:\n",
      "Original post_text: redapple型跟着模特穿keycapkeycap复古百褶裙太好穿身高cm体重斤斤小腿围cm大腿cm腰围cm肚围cmredapple型身材我太爱这件短裙了前几年都不敢尝试百褶裙没想到今年穿了感觉还不错哈哈哈哈哈应该是材质的问题厚的材质质感好苹果型身材苹果型小个子穿搭短裙穿搭短裙格子裙百褶裙百褶裙这么搭百褶裙穿搭格子百褶裙复古百褶裙复古格子半裙秋季穿搭秋季新款跟着模特学穿搭美拉德穿搭是什么苹果型身材穿搭小个子苹果型身材苹果型身材穿搭指南苹果型身材显瘦穿搭苹果型身材微胖穿搭微苹果型身材针织上衣舒服针织衫这么搭百搭针织上衣早秋必备针织衫\n",
      "Filtered keywords: ['redapple', '跟着', '模特', '复古', '百褶裙', '太好', '身高', '体重', '斤斤', '小腿', '大腿', '腰围', '肚围', '身材', '我太爱', '这件', '短裙', '几年', '尝试', '没想到', '感觉', '不错', '哈哈哈', '材质', '质感', '苹果', '小个子', '格子裙', '格子', '半裙', '秋季', '新款', '学穿', '拉德', '指南', '显瘦', '微胖', '搭微', '针织', '上衣', '舒服', '针织衫', '搭百搭', '早秋', '必备']\n",
      "\n",
      "Example 100:\n",
      "Original post_text: 这件毛衣搭红围巾真的氛围感绝了真的太好看啦质感超棒软软糯糯的外面再搭个大衣妥妥韩剧女主疯狂爱上大学生穿搭大衣穿搭毛衣日常穿搭气质穿搭秋冬穿搭小个子穿搭秋冬毛衣\n",
      "Filtered keywords: ['这件', '毛衣', '搭红', '围巾', '真的', '氛围', '感绝', '好看', '质感', '超棒', '软软', '外面', '搭个', '大衣', '韩剧', '女主', '疯狂', '爱上', '大学生', '日常', '气质', '秋冬', '小个子']\n",
      "\n",
      "Example 9914:\n",
      "Original post_text: 万圣节穿这样还了得穿了店里的新品拍一下照万圣节每日穿搭哥特\n",
      "Filtered keywords: ['万圣节', '店里', '新品', '每日', '哥特']\n",
      "\n",
      "Example 15089:\n",
      "Original post_text: 这种身材巨巨巨好穿doubleexclamationmark微胖女生大胆穿冬天真的很好穿啊胯宽腿粗梨形身材微胖女孩穿搭微胖穿搭日常穿搭梨形身材穿搭梨形微胖梨形身材大粗腿梨形穿搭梨形胯宽腿粗穿搭微胖秋冬冬季穿搭微胖女生冬季穿搭\n",
      "Filtered keywords: ['身材', '巨巨', '巨好', '微胖', '女生', '大胆', '冬天', '真的', '梨形', '女孩', '日常', '秋冬', '冬季']\n",
      "\n",
      "Example 2309:\n",
      "Original post_text: 基础Tee的一衣多穿法则夏天少不了的基础T恤怎么穿才能让他更有花样性所以我选择了三条不同类型的裤子进行搭配不同裤型的搭配也能让基础tee发挥它的可玩性今天穿什么香ootd每日穿搭男生穿搭开春穿搭基础短袖osmosismocuishleESOTERICInexistence存世GaforReal\n",
      "Filtered keywords: ['基础', 'Tee', '一衣', '法则', '夏天', '少不了', 'T恤', '花样', '选择', '类型', '裤子', '搭配', '裤型', 'tee', '发挥', '可玩性', 'ootd', '每日', '男生', '开春', '短袖', 'GaforReal']\n",
      "\n",
      "Example 4101:\n",
      "Original post_text: 只美拉德包包stuffedflatbread𖧧复古棕色百搭mate大容量棕色系mediumdarkskintone滴包包撞色巧克力包包chocolatebar羊羔毛包包ewe牛仔刺绣拼接棕色包包sewingneedle哭喊中心复古格纹帆布包burrito波士顿包型复古腋下包meatonbone包包偏爱小众包小众包包大学生上课包包腋下包帆布包美拉德棕色包包复古包包秋冬百搭包包通勤包包包包分享宝宝辅食包包不重样大容量包包\n",
      "Filtered keywords: ['只美', '拉德', '包包', '复古', '棕色', '百搭', 'mate', '大容量', '撞色', '巧克力', 'chocolatebar', '羊羔', '牛仔', '刺绣', '拼接', 'sewingneedle', '中心', '格纹', '帆布包', '包型', '腋下', 'meatonbone', '偏爱', '小众', '包小众', '大学生', '上课', '秋冬', '通勤', '分享', '宝宝', '辅食', '重样']\n",
      "\n",
      "Example 4394:\n",
      "Original post_text: 攒了一个夏天的粉色growingheart我是一个爱穿粉色系的中年妇女ootd每日穿搭日常穿搭每日穿搭粉色系粉色少女心\n",
      "Filtered keywords: ['夏天', '粉色', 'growingheart', '中年妇女', 'ootd', '每日', '日常', '少女']\n",
      "\n",
      "Example 12496:\n",
      "Original post_text: 回村啦冬季穿搭\n",
      "Filtered keywords: ['冬季']\n",
      "\n",
      "Example 2827:\n",
      "Original post_text: 条纹简约款长袖doubleexclamationmarkMalaysiains条纹长袖短版件套我的平价好物今天穿什么香挑战一个星期穿搭不重样笔记灵感马来西亚服装店吧生\n",
      "Filtered keywords: ['条纹', '简约', '长袖', 'doubleexclamationmarkMalaysiains', '短版', '件套', '平价', '好物', '挑战', '重样', '笔记', '灵感', '服装店']\n",
      "\n",
      "Example 15129:\n",
      "Original post_text: 她一夜爆火居然靠的是这个穿搭套路六一不是猫穿搭普通人穿搭博主穿搭ootd\n",
      "Filtered keywords: ['一夜', '爆火', '六一', '普通人', '博主穿', 'ootd']\n",
      "\n",
      "Example 9491:\n",
      "Original post_text: rmoneybag拿下最近很火的羽绒服doubleexclamationmarkg白鸭绒这件羽绒服是棒球服的款式时尚又保暖smilingfacewithhearts厚度不算很厚版型很好看很韩系衣长不是很长个人感觉比较适合小个子姐妹cleanfit简单出门羽绒服短款羽绒服羽绒服推荐穿着还显瘦的羽绒服\n",
      "Filtered keywords: ['rmoneybag', '拿下', '很火', '羽绒服', 'doubleexclamationmarkg', '鸭绒', '这件', '款式', '时尚', '保暖', '厚度', '不算', '版型', '好看', '韩系', '衣长', '很长', '个人感觉', '适合', '小个子', '姐妹', 'cleanfit', '简单', '出门', '短款', '推荐', '穿着', '还显']\n",
      "\n",
      "Example 4194:\n",
      "Original post_text: 平价显瘦针织衫三十多moneybag版型巨好好温柔好有氛围感谁懂软乎乎的不扎弹力大也没起球啥的可以冲针织衫内搭显瘦穿搭平价好物梨形身材今天穿什么香秋冬穿搭我的平价好物学生党\n",
      "Filtered keywords: ['平价', '显瘦', '针织衫', '三十多', '版型', '好好', '温柔', '氛围', '软乎乎', '不扎', '弹力', '起球', '好物', '梨形', '身材', '秋冬', '学生']\n",
      "\n",
      "Example 2920:\n",
      "Original post_text: 宜家的镜子好好看记录生活甜妹每日穿搭软妹穿搭宜家宜家拍照宜家好物\n",
      "Filtered keywords: ['镜子', '好好看', '记录', '生活', '甜妹', '每日', '软妹', '拍照', '好物']\n",
      "\n",
      "Example 3775:\n",
      "Original post_text: eplus姐妹看过的初秋格裙套装到货测评doubleexclamationmark浪漫生活的记录者笔记灵感服装测评套装平价衣服测评初秋穿搭\n",
      "Filtered keywords: ['姐妹', '看过', '初秋', '格裙', '套装', '到货', '测评', '浪漫生活', '记录', '笔记', '灵感', '服装', '平价', '衣服']\n",
      "\n",
      "Example 4534:\n",
      "Original post_text: 几十块lulu同款外套plus白色直筒裤太显瘦啦极简穿搭超会穿企划lulu平替ootd每日穿搭显瘦穿搭我的ootd白色牛仔裤拍照姿势不重样lulu黑色\n",
      "Filtered keywords: ['几十块', 'lulu', '同款', '外套', '白色', '筒裤', '太显', '简穿', '超会', '企划', '平替', 'ootd', '每日', '显瘦', '牛仔裤', '拍照', '姿势', '重样', '黑色']\n",
      "\n",
      "Example 10565:\n",
      "Original post_text: Meetingyouistrulyamiracleofmylif六芒星镶嵌手镯上刻的是Meetingyouistrulyamiracleofmylife遇见你真的是我生命中的奇迹我的手工日常宝藏饰品大公开配饰分享手镯手工是件不可思议的事儿纯银手镯手工银饰首饰就要blingbling晒晒手上戴什么手工手镯\n",
      "Filtered keywords: ['Meetingyouistrulyamiracleofmylif', '镶嵌', '手镯', '上刻', 'Meetingyouistrulyamiracleofmylife', '遇见', '真的', '生命', '奇迹', '手工', '日常', '宝藏', '饰品', '公开', '配饰', '分享', '不可思议', '事儿', '银手镯', '银饰', '首饰', 'blingbling', '晒晒', '手上']\n",
      "\n",
      "Example 7473:\n",
      "Original post_text: 微胖胯宽妹妹可以相信我选的简简单单耐看H型身材大骨架夏日两穿连衣裙好温柔呀露肩和不露肩都好美杏色连衣裙简直就是气质的代表色腰部褶皱也可以遮肚子的肉肉简单又显气质连爸妈都夸的小裙子一定要安利给你们月第一张照片笔记灵感夏日宝藏小裙子蓬蓬裙微胖穿搭显瘦穿搭显瘦连衣裙连衣裙气质连衣裙\n",
      "Filtered keywords: ['微胖', '妹妹', '我选', '简简单单', '耐看', '身材', '骨架', '夏日', '两穿', '连衣裙', '温柔', '不露', '美杏色', '气质', '腰部', '褶皱', '肚子', '简单', '爸妈', '裙子', '安利', '第一张', '照片', '笔记', '灵感', '宝藏', '蓬蓬裙', '显瘦']\n",
      "\n",
      "Example 6619:\n",
      "Original post_text: cm斤今天是美式复古运动女孩宽松和紧身混搭我缺一不可小个子穿搭美式穿搭辣妹穿搭日常穿搭冬季穿搭\n",
      "Filtered keywords: ['美式', '复古', '运动', '女孩', '宽松', '紧身', '缺一不可', '小个子', '辣妹', '日常', '冬季']\n",
      "\n",
      "Example 5295:\n",
      "Original post_text: 教师穿搭梨形身材请把这条神裤焊在身上蓝衬衫白裤子真的太清爽耐看啦氧气感十足梨形身材找到本命神裤啦我腰围臀围腰围刚刚好臀围放量很大真的超显腿直今天穿什么香教师穿搭穿搭梨形身材裤子白色裤子yyds法式复古穿搭好搭的裤子气质穿搭薯队长薯管家潮流薯时尚薯\n",
      "Filtered keywords: ['教师', '梨形', '身材', '这条', '神裤', '身上', '衬衫', '裤子', '真的', '清爽', '耐看', '氧气', '十足', '找到', '本命', '腰围', '臀围', '刚刚', '放量', '很大', '超显', '白色', 'yyds', '法式', '复古', '气质', '搭薯', '队长', '管家', '潮流', '时尚']\n",
      "\n",
      "Example 8590:\n",
      "Original post_text: 学生党秋冬百搭长裤jeans显高显瘦显腿直的神裤低至几keycapkeycapkeycapkeycapkeycapkeycap裤子长裤秋冬长裤百搭显瘦神裤不露腿穿搭高个子裤子加绒裤子\n",
      "Filtered keywords: ['学生', '秋冬', '百搭', '长裤', '显高显', '显腿直', '神裤', '裤子', '百搭显', '不露', '高个子', '加绒']\n",
      "\n",
      "Example 14843:\n",
      "Original post_text: 今天黑色系每日穿搭穿搭\n",
      "Filtered keywords: ['黑色', '每日']\n",
      "\n",
      "Example 14141:\n",
      "Original post_text: 小个子女生今天穿这套COOLbutton今日快乐今日发起风啦可以穿卫衣啦好开心今天穿卫衣搭配牛仔裤喜欢深色牛仔裤感觉更百搭加一个亮眼包包好看每日穿搭日常穿搭穿搭小个子女生穿搭ootd每日穿搭显瘦穿搭气质穿搭好搭的裤子女生\n",
      "Filtered keywords: ['小个子', '女生', '这套', 'COOLbutton', '今日', '快乐', '发起', '穿卫衣', '好开心', '搭配', '牛仔裤', '喜欢', '深色', '感觉', '更百搭加', '亮眼', '包包', '好看', '每日', '日常', 'ootd', '显瘦', '气质', '裤子']\n",
      "\n",
      "Example 11979:\n",
      "Original post_text: 今天是甜心辣妹感谢推荐的标题哈哈哈我的脑袋空空今天也是一身粉粉又是我爱的吊带和低腰裤哈哈甜心辣妹yk辣妹穿搭我的日常\n",
      "Filtered keywords: ['甜心', '辣妹', '感谢', '推荐', '标题', '哈哈哈', '脑袋', '一身', '粉粉', '吊带', '低腰裤', 'yk', '日常']\n",
      "\n",
      "Example 11515:\n",
      "Original post_text: 多carrot的小裙子大家看一下喜欢什么色我们去安排做好样衣再去拍正式图准确实物是最后一张滴其余的都是为p图效果但是不影响观看配色Lolita\n",
      "Filtered keywords: ['carrot', '裙子', '喜欢', '安排', '做好', '样衣', '正式', '准确', '实物', '一张', '效果', '影响', '观看', '配色', 'Lolita']\n",
      "\n",
      "Example 5370:\n",
      "Original post_text: 贵气感的源头是血气方刚贵气感并不说是吃的穿的有多贵是能明显的从一个人身上看到那种战斗力在身上健康勇敢积极向上的生活态度可见一个人的精神状态在气质提升上个多么重要希望大家每天都能活力满满比起乱七八糟的护肤品我更喜欢给身体补充一些内服的产品这样脸上会从内而外的散发着红润光泽感我喜欢熬夜经常出去喝酒那阵子真的元气大伤补了一阵子气血真的能感觉还挺有用的脸上皮肤透亮气血涌上来的那种红润感之前一直吃阿胶后来又入了液体阿胶口服液听说液体的更好吸收而且福站家的口服液一瓶ml里面阿胶成分就含ml剩下还有当归党参等可以气血双补自从阳完以后真的感冒频繁福站阿胶口服液喝了马上一整年了去年十月一左右开始喝的平时出去玩小小的一只放在包里也很方便每天坚持喝没想到效果这么好尤其是ym不调的姐妹谁能想到我一个几乎天天喝酒的人ym每次都特别规律要想显得贵气气质和气场上不能输其实方法很简单缺什么补什么尤其把每天的精神状态保持好才是最重要的喝酒内调微醺阿胶口服液拍照日常穿搭享受生活气血不足怎么补喝酒要微醺氛围感辣妹\n",
      "Filtered keywords: ['贵气感', '多贵', '身上', '那种', '战斗力', '健康', '勇敢', '积极向上', '生活态度', '精神状态', '气质', '提升', '希望', '活力', '满满', '乱七八糟', '护肤品', '喜欢', '身体', '补充', '内服', '产品', '脸上', '散发', '红润', '光泽感', '熬夜', '喝酒', '那阵子', '真的', '一阵子', '气血', '感觉', '有用', '皮肤', '透亮', '涌上来', '液体', '口服液', '听说', '更好', '吸收', '一瓶', '成分', '剩下', '双补', '感冒', '频繁', '整年', '去年', '平时', '出去玩', '小小的', '一只', '放在', '包里', '没想到', '效果', '不调', '姐妹', '想到', '天天', '每次', '特别', '显得', '贵气', '场上', '方法', '简单', '微醺', '拍照', '日常', '享受', '生活', '氛围', '辣妹']\n",
      "\n",
      "Example 2141:\n",
      "Original post_text: 我的天这是我会穿的香云纱改良汉服继续用香云纱改良日常款汉服因为我懒所以拒绝繁琐一套即出门香云纱香云纱新国潮穿搭新中式改良汉服汉服汉服穿搭汉服日常新中式穿搭\n",
      "Filtered keywords: ['我会', '香云纱', '改良', '汉服', '日常', '拒绝', '繁琐', '一套', '出门', '新国潮', '中式']\n",
      "\n",
      "Example 1164:\n",
      "Original post_text: 继山本裤后又一春夏神裤这肤感舒服绝了啊啊啊真的舒服的跟没穿一样版型宽松嘎嘎遮肉显腿直洗过了也没有缩水也不起球还不易皱着实是无可挑剔的神裤了啊啊啊啊啊感觉今年必大爆神裤裤子裤子种草好搭的裤子显瘦神裤粉色裤子小个子裤子梨型身材裤子百搭神裤夏季裤子OotD每日穿搭ootd每日穿搭春夏穿搭职场通勤穿搭日常穿搭显瘦穿搭裤子穿搭微胖女孩穿搭夏日穿搭休闲穿搭慵懒裤子裤子推荐打工人神裤垂感裤子梨形身材这条裤子真的绝了\n",
      "Filtered keywords: ['山本', '裤后', '春夏', '神裤', '这肤', '舒服', '啊啊啊', '真的', '版型', '宽松', '遮肉', '显腿直', '洗过', '缩水', '起球', '不易', '着实', '无可挑剔', '感觉', '爆神裤', '裤子', '种草', '显瘦', '粉色', '小个子', '梨型', '身材', '百搭神裤', '夏季', 'OotD', '每日', 'ootd', '职场', '通勤', '日常', '微胖', '女孩', '夏日', '休闲', '慵懒', '推荐', '工人', '垂感', '梨形']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: Sample 30 rows randomly\n",
    "sampled_df = df.sample(n=30, random_state=42)\n",
    "\n",
    "# Step 3: Function to remove duplicates while maintaining order\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: Iterate over sampled rows and process the content\n",
    "for i, row in sampled_df.iterrows():\n",
    "    try:\n",
    "        # Keep the original cleaned_post_text as is for display\n",
    "        summary_cleaned_original = row['cleaned_post_text']\n",
    "        \n",
    "        # Attempt to safely convert filtered_keywords to list, if possible\n",
    "        if isinstance(row['filtered_keywords2'], str):\n",
    "            try:\n",
    "                filter_label = ast.literal_eval(row['filtered_keywords'])  # Safely convert to list\n",
    "            except (ValueError, SyntaxError):\n",
    "                # Treat it as a simple string if not a valid list\n",
    "                filter_label = row['filtered_keywords2'].split()\n",
    "        else:\n",
    "            filter_label = row['filtered_keywords2']\n",
    "    \n",
    "    except (ValueError, SyntaxError):\n",
    "        # Log problematic rows\n",
    "        print(f\"Error parsing row {i}, skipping this row.\")\n",
    "        continue\n",
    "    \n",
    "    # Remove duplicates from filtered_keywords\n",
    "    filter_label_unique = remove_duplicates(filter_label)\n",
    "    \n",
    "    # Output the original text and processed keywords\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Original post_text: {summary_cleaned_original}\")\n",
    "    print(f\"Filtered keywords: {filter_label_unique}\")\n",
    "    print()  # Empty line for better output formatting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
