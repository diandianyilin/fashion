{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fashion-related samples (positive): 52242\n",
      "Number of non-fashion-related samples (negative): 304611\n",
      "The datasets are not balanced. Difference: 252369\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load fashion-related data (positive samples)\n",
    "fashion_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword' column which contains fashion-related keywords\n",
    "fashion_samples = fashion_df['keyword group']\n",
    "\n",
    "# Directory containing non-fashion-related samples\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "\n",
    "# Initialize an empty list to store non-fashion samples\n",
    "non_fashion_samples = []\n",
    "\n",
    "# Recursively iterate through all subdirectories and files in non_fashion_texts directory\n",
    "for root, dirs, files in os.walk(neg_dir):\n",
    "    for file in files:\n",
    "        # Read only text files\n",
    "        if file.startswith('wiki'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                # Add each line as a sample\n",
    "                lines = f.readlines()\n",
    "                non_fashion_samples.extend([line.strip() for line in lines if line.strip()])\n",
    "\n",
    "# Convert non-fashion samples to a pandas Series for consistency\n",
    "non_fashion_samples = pd.Series(non_fashion_samples)\n",
    "\n",
    "# Step 2: Count the number of samples in each dataset\n",
    "num_fashion_samples = fashion_samples.shape[0]\n",
    "num_non_fashion_samples = non_fashion_samples.shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of fashion-related samples (positive): {num_fashion_samples}\")\n",
    "print(f\"Number of non-fashion-related samples (negative): {num_non_fashion_samples}\")\n",
    "\n",
    "# Step 3: Check if they are balanced\n",
    "if num_fashion_samples == num_non_fashion_samples:\n",
    "    print(\"The datasets are balanced.\")\n",
    "else:\n",
    "    print(f\"The datasets are not balanced. Difference: {abs(num_fashion_samples - num_non_fashion_samples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling:\n",
    "- randomly reducing the number of samples from the majority class (non-fashion-related samples) to match the number of samples in the minority class (fashion-related samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of downsampled non-fashion-related samples: 52242\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# If the non-fashion dataset is larger, downsample it\n",
    "if num_non_fashion_samples > num_fashion_samples:\n",
    "    # Randomly select `num_fashion_samples` non-fashion samples\n",
    "    downsampled_non_fashion_samples = non_fashion_samples.sample(n=num_fashion_samples, random_state=42)\n",
    "else:\n",
    "    downsampled_non_fashion_samples = non_fashion_samples\n",
    "\n",
    "# Print the number of samples after downsampling\n",
    "print(f\"Number of downsampled non-fashion-related samples: {downsampled_non_fashion_samples.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read Data Files\n",
    "- read positive samples\n",
    "- read negative samples\n",
    "- combine the datasets\n",
    "#### Step 2: Load pre-trained sentence Transformer Model\n",
    "- load text2vec-large-chinese model\n",
    "- generate embeddings for each word\n",
    "#### Step 3: Get embeddings for the dataset\n",
    "- apply the embeddings function\n",
    "- convert labels to tensor\n",
    "#### Step 4: Train-test split\n",
    "- split the data\n",
    "- create PyTorh TensorDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data Shape: (356852, 2)\n",
      "Data Format Example:\n",
      "      word  label\n",
      "0     市政府.      0\n",
      "1   地理与气候.      0\n",
      "2  綠 (消歧義)      0\n",
      "3      邯山区      0\n",
      "4     江戸幕府      0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding Progress: 100%|██████████| 11152/11152 [15:32<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings Shape: torch.Size([356852, 1024])\n",
      "Embedding Example:\n",
      "tensor([ 0.9775, -0.6769, -0.7114,  ..., -0.5798,  1.1212, -2.3075],\n",
      "       device='cuda:0')\n",
      "Labels Shape: torch.Size([356852])\n",
      "Labels Example:\n",
      "tensor([0, 0, 0, 0, 0])\n",
      "Train Embeddings Shape: torch.Size([285481, 1024]), Train Labels Shape: torch.Size([285481])\n",
      "Val Embeddings Shape: torch.Size([71371, 1024]), Val Labels Shape: torch.Size([71371])\n",
      "Train Loader Batch Size: 8922 batches\n",
      "Val Loader Batch Size: 2231 batches\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Read Positive Samples from 'topics_filtered.csv'\n",
    "# Load the fashion lexicon (positive samples)\n",
    "pos_file = \"/home/disk1/red_disk1/fashion/topics_filtered.csv\"\n",
    "positive_samples_df = pd.read_csv(pos_file)\n",
    "positive_samples = positive_samples_df['keyword group'].dropna().tolist()  # Extract keywords column\n",
    "\n",
    "# Assign label 1 to positive samples\n",
    "positive_samples = pd.DataFrame(positive_samples, columns=['word'])\n",
    "positive_samples['label'] = 1\n",
    "\n",
    "# Step 2: Read Negative Samples from non_fashion_texts\n",
    "def read_negative_samples(directory):\n",
    "    all_lines = []\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder_path = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for filename in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    all_lines.extend([line.strip() for line in file if line.strip() != \"\"])\n",
    "    return all_lines\n",
    "\n",
    "neg_dir = \"/home/disk1/red_disk1/Multimodal_MKT/non_fashion_texts\"\n",
    "negative_samples_list = read_negative_samples(neg_dir)\n",
    "\n",
    "# Assign label 0 to negative samples\n",
    "negative_samples = pd.DataFrame(negative_samples_list, columns=['word'])\n",
    "negative_samples['label'] = 0\n",
    "\n",
    "# Step 3: Combine Positive and Negative Samples\n",
    "data = pd.concat([positive_samples, negative_samples], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Print the shape and format of the data\n",
    "print(f\"Combined Data Shape: {data.shape}\")\n",
    "print(f\"Data Format Example:\\n{data.head()}\")\n",
    "\n",
    "# Step 4: Load text2vec-large-chinese model\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Embedding Progress\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 5: Get embeddings for the dataset\n",
    "embeddings = get_text2vec_embeddings(data['word'], text2vec_model)\n",
    "\n",
    "# Print the shape and format of the embeddings\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "print(f\"Embedding Example:\\n{embeddings[0]}\")\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels = torch.tensor(data['label'].values)\n",
    "\n",
    "# Print the labels format\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels Example:\\n{labels[:5]}\")\n",
    "\n",
    "# Step 6: Train-Test Split\n",
    "train_embeddings, val_embeddings, train_labels, val_labels = train_test_split(\n",
    "    embeddings, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print the split data shapes\n",
    "print(f\"Train Embeddings Shape: {train_embeddings.shape}, Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Embeddings Shape: {val_embeddings.shape}, Val Labels Shape: {val_labels.shape}\")\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = TensorDataset(val_embeddings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Print the data loader batch size\n",
    "print(f\"Train Loader Batch Size: {len(train_loader)} batches\")\n",
    "print(f\"Val Loader Batch Size: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a RedNet-based binary classification model\n",
    "1. Model Definition: ResNet-Based Binary Classifier\n",
    "2. Training Loop\n",
    "3. Validation Loop\n",
    "4. Training and Evaluation\n",
    "\n",
    "#### Summary\n",
    "- Model Setup: It initializes a ResNet18 model tailored for binary classification using text embeddings as inputs.\n",
    "- Training and Validation: The code implements a standard training loop with backpropagation and uses validation to track performance.\n",
    "- Performance Metrics: Accuracy, precision, recall, and F1 score are calculated to evaluate the binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/diandian/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.66it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 345.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0922, Val Loss: 0.0618, Accuracy: 0.9754\n",
      "Precision: 0.9172, Recall: 0.9143, F1 Score: 0.9158\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.64it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 355.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0546, Val Loss: 0.0586, Accuracy: 0.9775\n",
      "Precision: 0.9595, Recall: 0.8829, F1 Score: 0.9196\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:49<00:00, 38.82it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 408.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0411, Val Loss: 0.0471, Accuracy: 0.9813\n",
      "Precision: 0.9461, Recall: 0.9245, F1 Score: 0.9352\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:49<00:00, 38.86it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:06<00:00, 354.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0321, Val Loss: 0.0481, Accuracy: 0.9826\n",
      "Precision: 0.9419, Recall: 0.9390, F1 Score: 0.9404\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 8922/8922 [03:50<00:00, 38.74it/s]\n",
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 400.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0262, Val Loss: 0.0526, Accuracy: 0.9829\n",
      "Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 2231/2231 [00:05<00:00, 434.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results: Accuracy: 0.9829, Precision: 0.9455, Recall: 0.9367, F1 Score: 0.9411\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import resnet18\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Step 7: Define ResNet-Based Binary Classification Model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):  # Adjust embedding_dim based on text2vec output (1024)\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        # Use pre-trained ResNet18, but replace the first conv layer to accept text2vec embeddings\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)  # Binary classification\n",
    "        \n",
    "        # Define a simple MLP layer to map embeddings to ResNet input size\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),  # Match the expected ResNet input size\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),  # Match the expected ResNet input size after projection\n",
    "        )\n",
    "        \n",
    "        # Adding a Conv layer to convert 1D to a 2D feature map\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "\n",
    "        # Adaptive pooling layer to transform the spatial dimensions to (7, 7)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)  # Project text2vec embeddings to match ResNet input size\n",
    "        x = x.unsqueeze(1).unsqueeze(2)  # Reshape to fit Conv2D (batch_size, channels, height, width)\n",
    "        x = self.embedding_conv(x)  # Use conv layer to convert 1D feature to 2D feature map\n",
    "        x = self.adaptive_pool(x)  # Apply adaptive pooling to resize to (7, 7)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model with the correct embedding dimension (1024 as per the output shape)\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "\n",
    "# Step 8: Define Training Parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Progress\"):\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation Progress\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1\n",
    "\n",
    "# Step 9: Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Step 10: Final Evaluation\n",
    "val_loss, accuracy, precision, recall, f1 = evaluate(model, val_loader, criterion, device)\n",
    "print(f\"Final Results: Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at: /home/disk1/red_disk1/fashion/tfashion\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Save the trained model\n",
    "\n",
    "# Save model and tokenizer to the desired directory\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion\"\n",
    "\n",
    "# Make the directory if it doesn't exist\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Save the entire model\n",
    "torch.save(model.state_dict(), os.path.join(model_save_path, \"tfashion.pth\"))\n",
    "\n",
    "# Optionally, save additional model metadata if needed (e.g., optimizer state, epoch)\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}\n",
    "torch.save(checkpoint, os.path.join(model_save_path, \"tfashion_checkpoint.pth\"))\n",
    "\n",
    "print(f\"Model saved at: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model tfashion\n",
    "- Step 1: Load the test dataset.\n",
    "- Step 2: Generate embeddings for each word in the test dataset using text2vec-large-chinese.\n",
    "- Step 3: Use a pre-trained ResNet-based classifier model to predict whether each word is related to fashion or not.\n",
    "- Step 4: Save the predictions alongside the original data into a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n",
      "Generating Embeddings: 100%|██████████| 489/489 [02:26<00:00,  3.35it/s]\n",
      "/tmp/ipykernel_267914/1813903382.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
      "Testing Progress: 100%|██████████| 489/489 [00:01<00:00, 388.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /home/disk1/red_disk1/fashion/test-output.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "# Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Function to get embeddings from text2vec-large-chinese\n",
    "def get_text2vec_embeddings(texts, model):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), 32), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i+32].tolist()\n",
    "        batch_embeddings = model.encode(batch_texts, convert_to_tensor=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "# Step 1: Load the test dataset\n",
    "test_file = \"/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean_combined.csv\"  # Update this path if necessary\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Get the words column from the test dataset\n",
    "test_words = test_data['post_text']\n",
    "\n",
    "# Step 2: Get embeddings for the test words\n",
    "test_embeddings = get_text2vec_embeddings(test_words, text2vec_model)\n",
    "\n",
    "# Step 3: Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(test_embeddings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Step 4: Load the trained tfashion model\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained tfashion model from your specified path\n",
    "model_save_path = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 5: Perform inference on the test dataset\n",
    "output_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing Progress\"):\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        output_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Step 6: Save the results\n",
    "# Add the output_labels as a new column to the original test_data DataFrame\n",
    "test_data['output_label'] = output_labels\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test-output.csv\"  \n",
    "test_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "1. Tokenization: tokenize the post_text content\n",
    "\n",
    "2. Word Embedding: pass each word (instead of the entire post) through the text2vec-large-chinese model to generate embeddings for each word\n",
    "\n",
    "3. Classification: pass embeddings through the tfashion model to classify whether each word is fashion-related or not\n",
    "\n",
    "4. Results: The output will be a classification for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/disk1/red_disk1/fashion/text2vec-large-chinese. Creating a new one with mean pooling.\n",
      "/tmp/ipykernel_267914/2145679474.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_file, map_location=device))\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.702 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Processing Rows:  73%|███████▎  | 11432/15622 [09:40<05:52, 11.89it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "\n",
    "# Step 1: Load the pre-trained SentenceTransformer model (text2vec-large-chinese)\n",
    "model_path = \"/home/disk1/red_disk1/fashion/text2vec-large-chinese\"\n",
    "text2vec_model = SentenceTransformer(model_path)\n",
    "\n",
    "# Step 2: Load the trained ResNet binary classifier\n",
    "class ResNetBinaryClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=1024):\n",
    "        super(ResNetBinaryClassifier, self).__init__()\n",
    "        self.resnet = resnet18(pretrained=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "        self.embedding_to_resnet = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        )\n",
    "        self.embedding_conv = nn.Conv2d(1, 3, kernel_size=(1, 1))\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_to_resnet(x)\n",
    "        x = x.unsqueeze(1).unsqueeze(2)\n",
    "        x = self.embedding_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model_file = \"/home/disk1/red_disk1/fashion/tfashion/tfashion.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetBinaryClassifier(embedding_dim=1024)\n",
    "model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Load the CSV file containing post_text\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/poster_test_fashion_nlpclean_combined.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 4: Tokenize the post_text content into words\n",
    "def tokenize_text(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "df['tokenized_text'] = df['post_text'].apply(tokenize_text)\n",
    "\n",
    "# Step 5: Get embeddings for each word in the tokenized_text column and classify them\n",
    "def get_text2vec_embeddings(words, model):\n",
    "    embeddings = model.encode(words, convert_to_tensor=True)\n",
    "    return embeddings\n",
    "\n",
    "filtered_keywords = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Rows\"):\n",
    "    words = row['tokenized_text']\n",
    "    \n",
    "    # Get embeddings for the words\n",
    "    embeddings = get_text2vec_embeddings(words, text2vec_model).to(device)\n",
    "    \n",
    "    # Make predictions using the pre-trained ResNet binary classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = model(embeddings)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Filter words where the model predicts label 1 (fashion-related)\n",
    "    filtered_words = [word for word, label in zip(words, preds) if label == 1]\n",
    "    filtered_keywords.append(filtered_words)\n",
    "\n",
    "# Step 6: Add the new column filtered_keywords and save the DataFrame\n",
    "df['filtered_keywords'] = filtered_keywords\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Filtered results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'matching_records_filtered_500.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 1: 读取文件\u001b[39;00m\n\u001b[1;32m      4\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatching_records_filtered_500.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: 随机采样30行\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sampled_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# 随机采样30行\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/agent/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'matching_records_filtered_500.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: 读取文件\n",
    "csv_file = \"/home/disk1/red_disk1/fashion/test_filtered.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Step 2: 随机采样30行\n",
    "sampled_df = df.sample(n=30, random_state=42)  # 随机采样30行\n",
    "\n",
    "# Step 3: 去重函数，保持顺序不变\n",
    "def remove_duplicates(word_list):\n",
    "    seen = set()\n",
    "    return [x for x in word_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "# Step 4: 打印输出其中的 summary_cleaned 列和 filter_label 列，去掉 filter_label 中的重复单词\n",
    "for i, row in sampled_df.iterrows():\n",
    "    summary_cleaned = eval(row['post_text'])  # 转换为列表\n",
    "    filter_label = eval(row['filtered_keywords'])  # 转换为列表\n",
    "    filter_label_unique = remove_duplicates(filter_label)  # 去重\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"post_text: {summary_cleaned}\")\n",
    "    print(f\"filtered_keywords: {filter_label_unique}\")\n",
    "    print()  # 空行用于美观输出\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
