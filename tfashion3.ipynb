{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUll Pipeline: Fine-Tuning BERT, Classifying Fashion-Related Content, and Applying Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Fine-Tune BERT on Fashion Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Step 1: Load Fashion Corpus\n",
    "fashion_keywords_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics0611_filtered.csv')\n",
    "\n",
    "# Extract the 'keyword group' column as a list of fashion-related keywords\n",
    "# Convert all values to strings and drop NaN values\n",
    "fashion_keywords = fashion_keywords_df['keyword group'].dropna().astype(str).tolist()\n",
    "\n",
    "# Custom Dataset for Fashion Corpus\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(1, dtype=torch.long)  # Label = 1 for fashion-related\n",
    "        }\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=2)\n",
    "\n",
    "# Set max length for the keywords\n",
    "max_len = 32  # Adjust as per your dataset\n",
    "fashion_dataset = FashionDataset(fashion_keywords, tokenizer, max_len)\n",
    "train_dataloader = DataLoader(fashion_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Step 2: Define Training Arguments and Fine-Tune the Model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=fashion_dataset,\n",
    "    eval_dataset=fashion_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('/home/disk1/red_disk1/Multimodal_MKT/fashion_bert_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Classify Fashion-Related Sentences in cleaned_post_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_test_fashion_nlpclean.csv')\n",
    "# df = pd.read_csv('/home/disk1/red_disk1/poster_9305.csv')\n",
    "\n",
    "# Combine 'post_title' and 'post_content' into 'post_text'\n",
    "df['post_text'] = df['post_title'].fillna('') + ' ' + df['post_content'].fillna('')\n",
    "\n",
    "# Drop 'post_title' and 'post_content' columns\n",
    "df = df.drop(columns=['post_title', 'post_content', 'post_tag'])\n",
    "\n",
    "# Save the updated DataFrame\n",
    "# df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_9305_combined.csv', index=False)\n",
    "df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/poster_test_fashion_nlpclean_combined.csv', index=False)\n",
    "\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "# Load stopwords from the provided file\n",
    "with open('/home/disk1/red_disk1/Multimodal_MKT/stopwords_cn.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Function for text cleaning\n",
    "def clean_text(text, stopwords):\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # Remove specific patterns\n",
    "    text = re.sub(r'- 小红书,,', '', text)  # Removing \"- 小红书,,\"\n",
    "    text = re.sub(r'小红书', '', text)  # Explicitly remove \"小红书\"\n",
    "    text = re.sub(r',,\\d{2}-\\d{2},,', '', text)  # Removing patterns like \",,XX-XX,,\"\n",
    "    text = re.sub(r'#', ' ', text)  # Replace '#' with a space\n",
    "    \n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    \n",
    "    # Remove stopwords (word-based removal)\n",
    "    cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopwords])\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning function to 'post_text'\n",
    "df['cleaned_post_text'] = df['post_text'].apply(lambda x: clean_text(str(x), stopwords))\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/post_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned BERT model for fashion classification\n",
    "model = BertForSequenceClassification.from_pretrained('/home/disk1/red_disk1/Multimodal_MKT/fashion_bert_model')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to classify if a sentence/phrase is fashion-related\n",
    "def classify_fashion_sentence(sentence, model, tokenizer):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_label == 1  # Return True if the sentence is fashion-related\n",
    "\n",
    "# Function to filter fashion-related sentences in a post\n",
    "def filter_fashion_sentences(text, model, tokenizer):\n",
    "    sentences = text.split('，')  # Split by comma or another delimiter suitable for sentences\n",
    "    fashion_sentences = [sentence for sentence in sentences if classify_fashion_sentence(sentence, model, tokenizer)]\n",
    "    return ' '.join(fashion_sentences)\n",
    "\n",
    "# Load the cleaned data\n",
    "df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/post_cleaned.csv')\n",
    "\n",
    "# Apply the sentence filtering function to each row of 'cleaned_post_text'\n",
    "df['fashion_text'] = df['cleaned_post_text'].apply(lambda x: filter_fashion_sentences(x, model, tokenizer))\n",
    "\n",
    "# Save the DataFrame with filtered fashion-related sentences\n",
    "df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/post_filtered_bert.csv', index=False)\n",
    "\n",
    "# Display the count of fashion-related posts\n",
    "num_fashion_related = df['fashion_text'].apply(lambda x: len(x.strip()) > 0).sum()\n",
    "num_non_fashion_related = len(df) - num_fashion_related\n",
    "print(f\"Number of fashion-related posts: {num_fashion_related}\")\n",
    "print(f\"Number of non-fashion-related posts: {num_non_fashion_related}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Extract Fashion-Related Keywords Using RAKE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake_nltk\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Initialize RAKE keyword extractor\n",
    "r = Rake()  # You can also pass a stopwords list here, e.g., Rake(stopwords)\n",
    "\n",
    "# Function to extract fashion-related keywords using RAKE\n",
    "def extract_keywords(text):\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return r.get_ranked_phrases()\n",
    "\n",
    "# Apply RAKE on the fashion_text column to extract keywords\n",
    "df['fashion_keywords'] = df['fashion_text'].apply(lambda x: extract_keywords(x))\n",
    "\n",
    "# Save the final DataFrame with fashion-related keywords\n",
    "df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/post_filtered_rake.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Filter Out Non-Fashion Keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fashion lexicon from 'topics0611_filtered.csv'\n",
    "fashion_lexicon_df = pd.read_csv('/home/disk1/red_disk1/Multimodal_MKT/topics0611_filtered.csv')\n",
    "fashion_keywords = set(fashion_lexicon_df['keyword group'].dropna().astype(str))\n",
    "\n",
    "# Function to keep only fashion-related keywords\n",
    "def filter_fashion_keywords(keywords):\n",
    "    return [keyword for keyword in keywords if keyword in fashion_keywords]\n",
    "\n",
    "# Apply the fashion keyword filter\n",
    "df['filtered_fashion_keywords'] = df['fashion_keywords'].apply(lambda x: filter_fashion_keywords(x))\n",
    "\n",
    "# Save the final output with only fashion-related keywords\n",
    "df.to_csv('/home/disk1/red_disk1/Multimodal_MKT/post_filtered.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
