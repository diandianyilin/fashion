# tfashion: A Fashion-Related Text Classification Model

[tfashion5.ipynb](https://github.com/diandianyilin/fashion/blob/October/tfashion5.ipynb)  
The `tfashion` model is a **binary text classifier** designed to identify **fashion-related words** in Chinese social media posts. It leverages a **pre-trained SentenceTransformer model** (`text2vec-large-chinese`) to generate word embeddings and a **ResNet-based binary classifier** for classification.

## Overview

Fashion trends and keywords play a crucial role in influencing consumer behavior, especially in social media contexts. The `tfashion` model was developed to address the need for identifying **fashion-related terms** within unstructured text. By combining a **SentenceTransformer** for embedding generation and a **ResNet-based classifier** for word-level predictions, this model can effectively filter out non-fashion-related content, leaving only relevant keywords.

## Key Features
- **Pre-trained SentenceTransformer embeddings**: Uses `text2vec-large-chinese` to convert text into embeddings.
- **ResNet-based binary classifier**: Built on top of the `ResNet18` architecture to classify each word as either fashion-related or not.
- **Stopword and single-character removal**: Enhances classification accuracy by removing irrelevant words.
- **Token-level classification**: Operates at the word level rather than classifying entire sentences or posts.


## Dataset Preparation

The dataset contains two categories:
- **Fashion-related samples (positive)**: Extracted from labeled social media posts.[topics_filtered.csv](https://github.com/diandianyilin/fashion/blob/main/topics_filtered.csv)
  - remove single Chinese characters and single English letters
  - remove certian noninformative fashion words: e.g., 'ç©¿æ­', 'OOTD', 'ootd', 'Ootd', 'æ—¥å¸¸', 'æ¯æ—¥', 'å®‰åˆ©', 'ç§è‰', 'å–œæ¬¢', 'çœŸçš„'
- **Non-fashion-related samples (negative)**: Extracted from general Chinese Wikipedia text dumps.[non_fashion_texts](https://github.com/diandianyilin/fashion/tree/main/non_fashion_texts)

### Imbalance Handling
- The initial dataset was imbalanced:
  - **Fashion-related samples**: 43,127
  - **Non-fashion-related samples**: 73,084
- To address this imbalance, the non-fashion-related samples were **downsampled** to match the number of fashion-related samples, resulting in **balanced datasets**:
  - **Downsampled Non-fashion-related samples**: 43,127

## Model Training

### Step 1: Data Preparation
1. **Read positive and negative samples** from their respective files.
2. **Combine the datasets** into a single DataFrame.
3. **Downsample the non-fashion-related samples** to balance the dataset.

### Step 2: Word Embedding Generation
1. Use the pre-trained **`text2vec-large-chinese`** model to generate **1024-dimensional embeddings** for each word in the dataset.
2. Convert the word embeddings and labels into **PyTorch tensors**.

### Step 3: Train-Test Split
1. The dataset is split into **80% training** and **20% validation** sets.
2. Data is loaded into **PyTorch DataLoaders** for efficient batch processing.

### Step 4: Model Definition
1. **ResNet18-based binary classifier**: A ResNet architecture is adapted for binary classification by modifying the fully connected layers to predict one of two classes (fashion-related or not).
2. The model is trained using the **Adam optimizer** and **cross-entropy loss**.

### Step 5: Training and Validation
1. The model is trained for 5 epochs, with validation after each epoch.
2. Performance is measured using **accuracy**, **precision**, **recall**, and **F1 score**.

### Training Results:
- **Training and Validation Results**:

| **Epoch** | **Train Loss** | **Val Loss** | **Accuracy** | **Precision** | **Recall** | **F1 Score** |
|:---------:|:--------------:|:------------:|:------------:|:-------------:|:----------:|:------------:|
| 1         | 0.0910         | 0.0593       | 0.9760       | 0.9084        | 0.9309     | 0.9195       |
| 2         | 0.0553         | 0.0553       | 0.9770       | 0.9614        | 0.8789     | 0.9183       |
| 3         | 0.0417         | 0.0489       | 0.9801       | 0.9152        | 0.9530     | 0.9337       |
| 4         | 0.0325         | 0.0467       | 0.9827       | 0.9369        | 0.9462     | 0.9415       |
| 5         | 0.0262         | 0.0464       | 0.9827       | 0.9568        | 0.9244     | 0.9403       |
| **Final** |                |              | 0.9827       | 0.9568        | 0.9244     | 0.9403       |


The training and validation results for each epoch are presented, with performance metrics such as precision, recall, and F1 score showing consistent improvements across the epochs.


## Model Inference: Classifying Fashion-related Text

### Step 1: Preprocessing
1. **Tokenize** the `post_text` content into individual words using **Jieba**.
2. Remove **stopwords** from the tokens using a custom stopwords list [stopwords_cn.txt](https://github.com/diandianyilin/fashion/blob/main/stopwords_cn.txt).
3. Remove any **single-character tokens**.

### Step 2: Word Embedding and Classification
1. Pass each word through the **SentenceTransformer** model to generate embeddings.
2. Classify each word using the **ResNet-based binary classifier** (`tfashion`).
3. Retain words that are classified as fashion-related.

### Example Output:

| **Example** | **Original post_text** | **Filtered Keywords** |
|:-----------:|:-----------------------|:----------------------|
| 12997       | å°Šå˜Ÿå¾ˆçˆ±redexclamationmarkå¥¹ä»¬è¯´è¿™æ˜¯å¯Œè´µå°å§å§ç©¿æ­å°è¯•ä¸€ä¸ªæ–°lookæ¸©æŸ”æ°”è´¨å¤§å§å§å¯Œå®¶åƒé‡‘ç©¿æ­ç§‹å†¬æ¸©æŸ”æ…µæ‡’é£æ¯›è¡£æ…µæ‡’é£ç©¿æ­é«˜çº§æ„Ÿç©¿æ­ | ['å¯Œè´µ', 'å°å§å§', 'å°è¯•', 'æ¸©æŸ”', 'æ°”è´¨', 'å§å§', 'å¯Œå®¶', 'åƒé‡‘', 'ç§‹å†¬', 'æ…µæ‡’', 'æ¯›è¡£', 'é£ç©¿', 'é«˜çº§', 'æ„Ÿç©¿'] |
| 11585       | å¥½çˆ±è¿™ç§æ¸©æŸ”ç™½æœˆå…‰çš„æ„Ÿè§‰ä¸€å®šè¦è¯•è¯•ä»™æ°”çš„è£™å­æ–°ä¸­å¼ç©¿æ­æ°‘å›½é£æ¸©æŸ”è¿è¡£è£™å›½é£é’ˆç»‡æ–°ä¸­å¼å¥—è£… | ['æ¸©æŸ”', 'ç™½æœˆå…‰', 'æ„Ÿè§‰', 'è¯•è¯•', 'ä»™æ°”', 'è£™å­', 'ä¸­å¼', 'æ°‘å›½', 'è¿è¡£è£™', 'å›½é£', 'é’ˆç»‡', 'å¥—è£…'] |
| 11226       | æˆ‘å…ˆæˆä¸ºæˆ‘è‡ªå·±å¾®èƒ–å¥³å­©å¾®èƒ–ç©¿æ­ | ['å¾®èƒ–', 'å¥³å­©'] |
| 13552       | redappleå‹è·Ÿç€æ¨¡ç‰¹ç©¿keycapkeycapå¤å¤ç™¾è¤¶è£™å¤ªå¥½ç©¿èº«é«˜cmä½“é‡æ–¤æ–¤å°è…¿å›´cmå¤§è…¿cmè…°å›´cmè‚šå›´cmredappleå‹èº«ææˆ‘å¤ªçˆ±è¿™ä»¶çŸ­è£™äº†å‰å‡ å¹´éƒ½ä¸æ•¢å°è¯•ç™¾è¤¶è£™æ²¡æƒ³åˆ°ä»Šå¹´ç©¿äº†æ„Ÿè§‰è¿˜ä¸é”™å“ˆå“ˆå“ˆå“ˆå“ˆåº”è¯¥æ˜¯æè´¨çš„é—®é¢˜åšçš„æè´¨è´¨æ„Ÿå¥½è‹¹æœå‹èº«æè‹¹æœå‹å°ä¸ªå­ç©¿æ­çŸ­è£™ç©¿æ­çŸ­è£™æ ¼å­è£™ç™¾è¤¶è£™ç™¾è¤¶è£™è¿™ä¹ˆæ­ç™¾è¤¶è£™ç©¿æ­æ ¼å­ç™¾è¤¶è£™å¤å¤ç™¾è¤¶è£™å¤å¤æ ¼å­åŠè£™ç§‹å­£ç©¿æ­ç§‹å­£æ–°æ¬¾è·Ÿç€æ¨¡ç‰¹å­¦ç©¿æ­ç¾æ‹‰å¾·ç©¿æ­æ˜¯ä»€ä¹ˆè‹¹æœå‹èº«æç©¿æ­å°ä¸ªå­è‹¹æœå‹èº«æè‹¹æœå‹èº«æç©¿æ­æŒ‡å—è‹¹æœå‹èº«ææ˜¾ç˜¦ç©¿æ­è‹¹æœå‹èº«æå¾®èƒ–ç©¿æ­å¾®è‹¹æœå‹èº«æé’ˆç»‡ä¸Šè¡£èˆ’æœé’ˆç»‡è¡«è¿™ä¹ˆæ­ç™¾æ­é’ˆç»‡ä¸Šè¡£æ—©ç§‹å¿…å¤‡é’ˆç»‡è¡« | ['redapple', 'è·Ÿç€', 'æ¨¡ç‰¹', 'å¤å¤', 'ç™¾è¤¶è£™', 'å¤ªå¥½', 'èº«é«˜', 'ä½“é‡', 'æ–¤æ–¤', 'å°è…¿', 'å¤§è…¿', 'è…°å›´', 'è‚šå›´', 'èº«æ', 'æˆ‘å¤ªçˆ±', 'è¿™ä»¶', 'çŸ­è£™', 'å‡ å¹´', 'å°è¯•', 'æ²¡æƒ³åˆ°', 'æ„Ÿè§‰', 'ä¸é”™', 'å“ˆå“ˆå“ˆ', 'æè´¨', 'è´¨æ„Ÿ', 'è‹¹æœ', 'å°ä¸ªå­', 'æ ¼å­è£™', 'æ ¼å­', 'åŠè£™', 'ç§‹å­£', 'æ–°æ¬¾', 'å­¦ç©¿', 'æ‹‰å¾·', 'æŒ‡å—', 'æ˜¾ç˜¦', 'å¾®èƒ–', 'æ­å¾®', 'é’ˆç»‡', 'ä¸Šè¡£', 'èˆ’æœ', 'é’ˆç»‡è¡«', 'æ­ç™¾æ­', 'æ—©ç§‹', 'å¿…å¤‡'] |
| 100         | è¿™ä»¶æ¯›è¡£æ­çº¢å›´å·¾çœŸçš„æ°›å›´æ„Ÿç»äº†çœŸçš„å¤ªå¥½çœ‹å•¦è´¨æ„Ÿè¶…æ£’è½¯è½¯ç³¯ç³¯çš„å¤–é¢å†æ­ä¸ªå¤§è¡£å¦¥å¦¥éŸ©å‰§å¥³ä¸»ç–¯ç‹‚çˆ±ä¸Šå¤§å­¦ç”Ÿç©¿æ­å¤§è¡£ç©¿æ­æ¯›è¡£æ—¥å¸¸ç©¿æ­æ°”è´¨ç©¿æ­ç§‹å†¬ç©¿æ­å°ä¸ªå­ç©¿æ­ç§‹å†¬æ¯›è¡£ | ['è¿™ä»¶', 'æ¯›è¡£', 'æ­çº¢', 'å›´å·¾', 'çœŸçš„', 'æ°›å›´', 'æ„Ÿç»', 'å¥½çœ‹', 'è´¨æ„Ÿ', 'è¶…æ£’', 'è½¯è½¯', 'å¤–é¢', 'æ­ä¸ª', 'å¤§è¡£', 'éŸ©å‰§', 'å¥³ä¸»', 'ç–¯ç‹‚', 'çˆ±ä¸Š', 'å¤§å­¦ç”Ÿ', 'æ—¥å¸¸', 'æ°”è´¨', 'ç§‹å†¬', 'å°ä¸ªå­'] |
| 9914        | ä¸‡åœ£èŠ‚ç©¿è¿™æ ·è¿˜äº†å¾—ç©¿äº†åº—é‡Œçš„æ–°å“æ‹ä¸€ä¸‹ç…§ä¸‡åœ£èŠ‚æ¯æ—¥ç©¿æ­å“¥ç‰¹ | ['ä¸‡åœ£èŠ‚', 'åº—é‡Œ', 'æ–°å“', 'æ¯æ—¥', 'å“¥ç‰¹'] |
| 15089       | è¿™ç§èº«æå·¨å·¨å·¨å¥½ç©¿doubleexclamationmarkå¾®èƒ–å¥³ç”Ÿå¤§èƒ†ç©¿å†¬å¤©çœŸçš„å¾ˆå¥½ç©¿å•Šèƒ¯å®½è…¿ç²—æ¢¨å½¢èº«æå¾®èƒ–å¥³å­©ç©¿æ­å¾®èƒ–ç©¿æ­æ—¥å¸¸ç©¿æ­æ¢¨å½¢èº«æç©¿æ­æ¢¨å½¢å¾®èƒ–æ¢¨å½¢èº«æå¤§ç²—è…¿æ¢¨å½¢ç©¿æ­æ¢¨å½¢èƒ¯å®½è…¿ç²—ç©¿æ­å¾®èƒ–ç§‹å†¬å†¬å­£ç©¿æ­å¾®èƒ–å¥³ç”Ÿå†¬å­£ç©¿æ­ | ['èº«æ', 'å·¨å·¨', 'å·¨å¥½', 'å¾®èƒ–', 'å¥³ç”Ÿ', 'å¤§èƒ†', 'å†¬å¤©', 'çœŸçš„', 'æ¢¨å½¢', 'å¥³å­©', 'æ—¥å¸¸', 'ç§‹å†¬', 'å†¬å­£'] |
| 2309        | åŸºç¡€Teeçš„ä¸€è¡£å¤šç©¿æ³•åˆ™å¤å¤©å°‘ä¸äº†çš„åŸºç¡€Tæ¤æ€ä¹ˆç©¿æ‰èƒ½è®©ä»–æ›´æœ‰èŠ±æ ·æ€§æ‰€ä»¥æˆ‘é€‰æ‹©äº†ä¸‰æ¡ä¸åŒç±»å‹çš„è£¤å­è¿›è¡Œæ­é…ä¸åŒè£¤å‹çš„æ­é…ä¹Ÿèƒ½è®©åŸºç¡€teeå‘æŒ¥å®ƒçš„å¯ç©æ€§ä»Šå¤©ç©¿ä»€ä¹ˆé¦™ootdæ¯æ—¥ç©¿æ­ç”·ç”Ÿç©¿æ­å¼€æ˜¥ç©¿æ­åŸºç¡€çŸ­è¢–osmosismocuishleESOTERICInexistenceå­˜ä¸–GaforReal | ['åŸºç¡€', 'Tee', 'ä¸€è¡£', 'æ³•åˆ™', 'å¤å¤©', 'å°‘ä¸äº†', 'Tæ¤', 'èŠ±æ ·', 'é€‰æ‹©', 'ç±»å‹', 'è£¤å­', 'æ­é…', 'è£¤å‹', 'tee', 'å‘æŒ¥', 'å¯ç©æ€§', 'ootd', 'æ¯æ—¥', 'ç”·ç”Ÿ', 'å¼€æ˜¥', 'çŸ­è¢–', 'GaforReal'] |
| 4101        | åªç¾æ‹‰å¾·åŒ…åŒ…stuffedflatbreadğ–§§å¤å¤æ£•è‰²ç™¾æ­mateå¤§å®¹é‡æ£•è‰²ç³»mediumdarkskintoneæ»´åŒ…åŒ…æ’è‰²å·§å…‹åŠ›åŒ…åŒ…chocolatebarç¾Šç¾”æ¯›åŒ…åŒ…eweç‰›ä»”åˆºç»£æ‹¼æ¥æ£•è‰²åŒ…åŒ…sewingneedleå“­å–Šä¸­å¿ƒå¤å¤æ ¼çº¹å¸†å¸ƒåŒ…burritoæ³¢å£«é¡¿åŒ…å‹å¤å¤è…‹ä¸‹åŒ…meatonboneåŒ…åŒ…åçˆ±å°ä¼—åŒ…å°ä¼—åŒ…åŒ…å¤§å­¦ç”Ÿä¸Šè¯¾åŒ…åŒ…è…‹ä¸‹åŒ…å¸†å¸ƒåŒ…ç¾æ‹‰å¾·æ£•è‰²åŒ…åŒ…å¤å¤åŒ…åŒ…ç§‹å†¬ç™¾æ­åŒ…åŒ…é€šå‹¤åŒ…åŒ…åŒ…åŒ…åˆ†äº«å®å®è¾…é£ŸåŒ…åŒ…ä¸é‡æ ·å¤§å®¹é‡åŒ…åŒ… | ['åªç¾', 'æ‹‰å¾·', 'åŒ…åŒ…', 'å¤å¤', 'æ£•è‰²', 'ç™¾æ­', 'mate', 'å¤§å®¹é‡', 'æ’è‰²', 'å·§å…‹åŠ›', 'chocolatebar', 'ç¾Šç¾”', 'ç‰›ä»”', 'åˆºç»£', 'æ‹¼æ¥', 'sewingneedle', 'ä¸­å¿ƒ', 'æ ¼çº¹', 'å¸†å¸ƒåŒ…', 'åŒ…å‹', 'è…‹ä¸‹', 'meatonbone', 'åçˆ±', 'å°ä¼—', 'åŒ…å°ä¼—', 'å¤§å­¦ç”Ÿ', 'ä¸Šè¯¾', 'ç§‹å†¬', 'é€šå‹¤', 'åˆ†äº«', 'å®å®', 'è¾…é£Ÿ', 'é‡æ ·'] |
| 4394        | æ”’äº†ä¸€ä¸ªå¤å¤©çš„ç²‰è‰²growingheartæˆ‘æ˜¯ä¸€ä¸ªçˆ±ç©¿ç²‰è‰²ç³»çš„ä¸­å¹´å¦‡å¥³ootdæ¯æ—¥ç©¿æ­æ—¥å¸¸ç©¿æ­æ¯æ—¥ç©¿æ­ç²‰è‰²ç³»ç²‰è‰²å°‘å¥³å¿ƒ | ['å¤å¤©', 'ç²‰è‰²', 'growingheart', 'ä¸­å¹´å¦‡å¥³', 'ootd', 'æ¯æ—¥', 'æ—¥å¸¸', 'å°‘å¥³'] |


For more examples, see [tfashion5.ipynb](https://github.com/diandianyilin/fashion/blob/October/tfashion5.ipynb)  
